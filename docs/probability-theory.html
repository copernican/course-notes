<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Master’s Companion</title>
  <meta name="description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="A Master’s Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Master’s Companion" />
  
  <meta name="twitter:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

<meta name="author" content="Sean Wilson">


<meta name="date" content="2018-06-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="analysis.html">
<link rel="next" href="linear-algebra.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\newcommand{\coloneqq}{\mathrel{\mathop:}\mathrel{\mkern-1.2mu}=}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Master's Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="part"><span><b>I Background material</b></span></li>
<li class="chapter" data-level="2" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>2</b> Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="analysis.html"><a href="analysis.html#upper-bounds-and-suprema"><i class="fa fa-check"></i><b>2.1</b> Upper bounds and suprema</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>3</b> Probability theory</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-theory.html"><a href="probability-theory.html#background-material"><i class="fa fa-check"></i><b>3.1</b> Background material</a></li>
<li class="chapter" data-level="3.2" data-path="probability-theory.html"><a href="probability-theory.html#transformations-and-expectations"><i class="fa fa-check"></i><b>3.2</b> Transformations and expectations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-theory.html"><a href="probability-theory.html#distributions-of-functions-of-a-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Distributions of functions of a random variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-theory.html"><a href="probability-theory.html#expected-values"><i class="fa fa-check"></i><b>3.2.2</b> Expected values</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-theory.html"><a href="probability-theory.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>3.2.3</b> Moments and moment generating functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-theory.html"><a href="probability-theory.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.3</b> Multiple random variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="probability-theory.html"><a href="probability-theory.html#conditional-distributions-and-independence"><i class="fa fa-check"></i><b>3.3.1</b> Conditional distributions and independence</a></li>
<li class="chapter" data-level="3.3.2" data-path="probability-theory.html"><a href="probability-theory.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.3.2</b> Covariance and correlation</a></li>
<li class="chapter" data-level="3.3.3" data-path="probability-theory.html"><a href="probability-theory.html#multivariate-distributions"><i class="fa fa-check"></i><b>3.3.3</b> Multivariate distributions</a></li>
<li class="chapter" data-level="3.3.4" data-path="probability-theory.html"><a href="probability-theory.html#inequalities"><i class="fa fa-check"></i><b>3.3.4</b> Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-theory.html"><a href="probability-theory.html#properties-of-a-random-sample"><i class="fa fa-check"></i><b>3.4</b> Properties of a random sample</a><ul>
<li class="chapter" data-level="3.4.1" data-path="probability-theory.html"><a href="probability-theory.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i><b>3.4.1</b> Sums of random variables from a random sample</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability-theory.html"><a href="probability-theory.html#sampling-from-the-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Sampling from the normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="probability-theory.html"><a href="probability-theory.html#order-statistics"><i class="fa fa-check"></i><b>3.4.3</b> Order statistics</a></li>
<li class="chapter" data-level="3.4.4" data-path="probability-theory.html"><a href="probability-theory.html#convergence-concepts"><i class="fa fa-check"></i><b>3.4.4</b> Convergence concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Linear algebra</a></li>
<li class="part"><span><b>II Mathematical statistics</b></span></li>
<li class="chapter" data-level="5" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html"><i class="fa fa-check"></i><b>5</b> Common families of distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#defn-exp-family"><i class="fa fa-check"></i><b>5.1</b> Exponential families</a><ul>
<li class="chapter" data-level="5.1.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#natural-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Natural parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-and-scale-families"><i class="fa fa-check"></i><b>5.2</b> Location and scale families</a><ul>
<li class="chapter" data-level="5.2.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-families"><i class="fa fa-check"></i><b>5.2.1</b> Location families</a></li>
<li class="chapter" data-level="5.2.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#scale-families"><i class="fa fa-check"></i><b>5.2.2</b> Scale families</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="pagerank.html"><a href="pagerank.html"><i class="fa fa-check"></i><b>6</b> PageRank</a><ul>
<li class="chapter" data-level="6.1" data-path="pagerank.html"><a href="pagerank.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="pagerank.html"><a href="pagerank.html#computing-eigenpairs"><i class="fa fa-check"></i><b>6.2</b> Computing eigenpairs</a></li>
<li class="chapter" data-level="6.3" data-path="pagerank.html"><a href="pagerank.html#algorithm"><i class="fa fa-check"></i><b>6.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.4" data-path="pagerank.html"><a href="pagerank.html#considerations"><i class="fa fa-check"></i><b>6.4</b> Considerations</a><ul>
<li class="chapter" data-level="6.4.1" data-path="pagerank.html"><a href="pagerank.html#connection-to-markov-chains"><i class="fa fa-check"></i><b>6.4.1</b> Connection to Markov chains</a></li>
<li class="chapter" data-level="6.4.2" data-path="pagerank.html"><a href="pagerank.html#calculating-the-dominant-eigenvalue"><i class="fa fa-check"></i><b>6.4.2</b> Calculating the dominant eigenvalue</a></li>
<li class="chapter" data-level="6.4.3" data-path="pagerank.html"><a href="pagerank.html#computational-complexity"><i class="fa fa-check"></i><b>6.4.3</b> Computational complexity</a></li>
<li class="chapter" data-level="6.4.4" data-path="pagerank.html"><a href="pagerank.html#convergence"><i class="fa fa-check"></i><b>6.4.4</b> Convergence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Master’s Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-theory" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Probability theory</h1>
<div id="background-material" class="section level2">
<h2><span class="header-section-number">3.1</span> Background material</h2>

<div class="theorem">
<p><span id="thm:prob-partitions" class="theorem"><strong>Theorem 3.1  </strong></span>If <span class="math inline">\(P\)</span> is a probability function, then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P\left(A\right)=\sum_{i=1}^{\infty}P\left(A\cap C_{i}\right)\)</span> for any partition <span class="math inline">\(C_{1},C_{2},\ldots\)</span>;</li>
<li><span class="math inline">\(P\left(\cup_{i=1}^{\infty}A_{i}\right)\leq\sum_{i=1}^{\infty}P\left(A_{i}\right)\)</span> for any sets <span class="math inline">\(A_{1},A_{2},\ldots\)</span>.</li>
</ol>
</div>


<div class="theorem">
<span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 3.2  (Bayes’ Rule)  </strong></span>Let <span class="math inline">\(A_{1},A_{2},\ldots\)</span> be a partition of the sample space <span class="math inline">\(\Omega\)</span>, and let <span class="math inline">\(B\)</span> be any set. Then, for each <span class="math inline">\(i=1,2,\ldots\)</span>, <span class="math display">\[
P\left(A_{i}|B\right)=\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B|A_{j}\right)P\left(A_{j}\right)}.
\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> From the definition of conditional probability, we have
<span class="math display">\[\begin{align*}
P\left(A_{i}|B\right) &amp; =\frac{P\left(A_{i}\cap B\right)}{P\left(B\right)} \\
 &amp; =\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{P\left(B\right)}\tag{conditional probability} \\
 &amp; =\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B\cap A_{j}\right)}\tag{the $A_{j}$ partition $\Omega$} \\
 &amp; =\frac{P\left(B|A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B|A_{j}\right)P\left(A_{j}\right)}.\tag{conditional probability}
\end{align*}\]</span>
</div>


<div class="definition">
<span id="def:unnamed-chunk-8" class="definition"><strong>Definition 3.1  </strong></span>The <em>cumulative distribution function</em> or <em>cdf</em> of a random variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(F_{X}\left(x\right)\)</span>, is defined by <span class="math inline">\(F_{X}\left(x\right)=P_{X}\left(X\leq x\right)\)</span>, for all <span class="math inline">\(x\)</span>.
</div>


<div class="theorem">
<p><span id="thm:properties-of-cdf" class="theorem"><strong>Theorem 3.3  </strong></span>The function <span class="math inline">\(F\left(x\right)\)</span> is a cdf if and only if the following three conditions hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\lim_{x\rightarrow-\infty}F\left(x\right)=0\)</span> and <span class="math inline">\(\lim_{x\rightarrow\infty}F\left(x\right)=1\)</span>.</li>
<li><span class="math inline">\(F\left(x\right)\)</span> is a nondecreasing function of <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(F\left(x\right)\)</span> is right-continuous; that is, for every number <span class="math inline">\(x_{0}\)</span>, <span class="math inline">\(\lim_{x\downarrow x_{0}}F\left(x\right)=F\left(x_{0}\right)\)</span>.</li>
</ol>
(This is Theorem 1.5.3 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>

</div>
<div id="transformations-and-expectations" class="section level2">
<h2><span class="header-section-number">3.2</span> Transformations and expectations</h2>
<div id="distributions-of-functions-of-a-random-variable" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Distributions of functions of a random variable</h3>
<p>When transformations are made, it is important to keep track of the sample spaces of the random variables; otherwise, much confusion can arise. When the transformation is from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y=g\left(X\right)\)</span>, it is most convenient to use</p>
<span class="math display" id="eq:trans-rv-sample-space">\[\begin{equation}
\mathcal{X}=\left\{ x:f_{X}\left(x\right)&gt;0\right\} \quad\text{and}\quad\mathcal{Y}=\left\{ y:y=g\left(x\right)\text{ for some }x\in\mathcal{X}\right\}.
\tag{3.1}
\end{equation}\]</span>

<div class="theorem">
<p><span id="thm:cdf-of-function-of-rv" class="theorem"><strong>Theorem 3.4  </strong></span>Let <span class="math inline">\(X\)</span> have cdf <span class="math inline">\(F_{X}\left(x\right)\)</span>, let <span class="math inline">\(Y=g\left(X\right)\)</span>, and let <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> be defined as in Equation <a href="probability-theory.html#eq:trans-rv-sample-space">(3.1)</a>.</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(g\)</span> is an increasing function on <span class="math inline">\(\mathcal{X}\)</span>, <span class="math inline">\(F_{Y}\left(y\right)=F_{X}\left(g^{-1}\left(y\right)\right)\)</span> for <span class="math inline">\(y\in\mathcal{Y}\)</span>.</li>
<li>If <span class="math inline">\(g\)</span> is a decreasing function on <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(X\)</span> is a continuous random variable, <span class="math inline">\(F_{Y}\left(y\right)=1-F_{X}\left(g^{-1}\left(y\right)\right)\)</span> for <span class="math inline">\(y\in\mathcal{Y}\)</span>.</li>
</ol>
(This is Theorem 2.1.3 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> <span class="math inline">\(g\)</span> is a monotone function, i.e., it maps each <span class="math inline">\(x\)</span> to a single <span class="math inline">\(y\)</span>, and each <span class="math inline">\(y\)</span> comes from at most one <span class="math inline">\(x\)</span>. In this case that <span class="math inline">\(g\)</span> is increasing, we have</p>
<span class="math display">\[\begin{align*}
\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} &amp; =\left\{ x\in\mathcal{X}:g^{-1}\left(g\left(x\right)\right)\leq g^{-1}\left(y\right)\right\} \\
    &amp; =\left\{ x\in\mathcal{X}:x\leq g^{-1}\left(y\right)\right\},
\end{align*}\]</span>
<p>and the cdf of <span class="math inline">\(Y\)</span> is</p>
<span class="math display">\[\begin{align*}
F_{Y}\left(y\right) &amp; =P\left(\left\{ Y\leq y\right\} \right) \\
    &amp; =P\left(\left\{ g\left(X\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:x\leq g^{-1}\left(y\right)\right\} \right) \\
    &amp; =\int_{-\infty}^{g^{-1}\left(y\right)}f_{X}\left(x\right)\dif x \\
    &amp; =F_{X}\left(g^{-1}\left(y\right)\right).
\end{align*}\]</span>
<p>In the case that <span class="math inline">\(g\)</span> is decreasing, we have</p>
<span class="math display">\[\begin{align*}
\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} &amp; =\left\{ x\in\mathcal{X}:g^{-1}\left(g\left(x\right)\right)\geq g^{-1}\left(y\right)\right\} \\
    &amp; =\left\{ x\in\mathcal{X}:x\geq g^{-1}\left(y\right)\right\},
\end{align*}\]</span>
<p>and the cdf of <span class="math inline">\(Y\)</span> is</p>
<span class="math display">\[\begin{align*}
F_{Y}\left(y\right) &amp; =P\left(\left\{ Y\leq y\right\} \right) \\
    &amp; =P\left(\left\{ g\left(X\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:x\geq g^{-1}\left(y\right)\right\} \right) \\
    &amp; =\int_{g^{-1}\left(y\right)}^{\infty}f_{X}\left(x\right)\dif x \\
    &amp; =1-F_{X}\left(g^{-1}\left(y\right)\right).\tag{continuity of $X$}
\end{align*}\]</span>
</div>


<div class="theorem">
<p><span id="thm:pdf-of-function-of-rv" class="theorem"><strong>Theorem 3.5  </strong></span>Let <span class="math inline">\(X\)</span> have pdf <span class="math inline">\(f_{X}\left(x\right)\)</span> and let <span class="math inline">\(Y=g\left(X\right)\)</span>, where <span class="math inline">\(g\)</span> is a monotone function. Let <span class="math inline">\(\mathcal{X}=\left\{ x:f_{X}\left(x\right)&gt;0\right\}\)</span> and let <span class="math inline">\(\mathcal{Y}=\left\{ y:y=g\left(x\right),x\in\mathcal{X}\right\}\)</span>. Suppose that <span class="math inline">\(f_{X}\left(x\right)\)</span> is continuous on <span class="math inline">\(\mathcal{X}\)</span> and that <span class="math inline">\(g^{-1}\left(y\right)\)</span> has a continuous derivative on <span class="math inline">\(\mathcal{Y}\)</span>. Then the pdf of <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[
f_{Y}\left(y\right)=
  \begin{cases}
    f_{X}\left(g^{-1}\left(y\right)\right)\left|\dfrac{\dif}{\dif y}g^{-1}\left(y\right)\right|, &amp; y\in\mathcal{Y}\\
    0, &amp; \text{otherwise}
  \end{cases}.
\]</span></p>
(This is Theorem 2.1.5 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From Theorem <a href="probability-theory.html#thm:cdf-of-function-of-rv">3.4</a> and applying the chain rule, we have</p>
<p><span class="math display">\[
f_{Y}\left(y\right)=
  \dfrac{\dif}{\dif y}F_{Y}\left(y\right)=
  f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)
\]</span></p>
<p>in the case that g is increasing and</p>
<p><span class="math display">\[
f_{Y}\left(y\right)=
  \dfrac{\dif}{\dif y}F_{Y}\left(y\right)=
  0-f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)=
  -f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)
\]</span></p>
in the case that <span class="math inline">\(g\)</span> is decreasing, which can be expressed concisely as in the theorem.
</div>

<p>We will look at <span class="math inline">\(F_{X}^{-1}\)</span>, the inverse of the cdf <span class="math inline">\(F_{X}\)</span>. If <span class="math inline">\(F_{X}\)</span> is strictly increasing, then <span class="math inline">\(F_{X}^{-1}\)</span> is well defined by</p>
<span class="math display" id="eq:inverse-cdf-increasing">\[\begin{equation}
F_{X}^{-1}\left(y\right)=x\implies F_{X}\left(x\right)=y.
\tag{3.2}
\end{equation}\]</span>
<p>However, if <span class="math inline">\(F_{X}\)</span> is constant on some interval, then <span class="math inline">\(F_{X}^{-1}\)</span> is not well defined by Equation <a href="probability-theory.html#eq:inverse-cdf-increasing">(3.2)</a>. Any <span class="math inline">\(x\)</span> satisfying <span class="math inline">\(x_{1}\leq x\leq x_{2}\)</span> satisfies <span class="math inline">\(F_{X}\left(x\right)=y\)</span>. This problem is avoided by defining <span class="math inline">\(F_{X}^{-1}\left(y\right)\)</span> for <span class="math inline">\(0&lt;y&lt;1\)</span> by</p>
<p><span class="math display">\[
F_{X}^{-1}\left(y\right)=\inf\left\{ x:F_{X}\left(x\right)\geq y\right\},
\]</span> a definition that agrees with Equation <a href="probability-theory.html#eq:inverse-cdf-increasing">(3.2)</a> when <span class="math inline">\(F_{X}\)</span> is nonconstant and provides an <span class="math inline">\(F_{X}^{-1}\)</span> that is single-valued even when <span class="math inline">\(F_{X}\)</span> is not strictly increasing. Using this definition, for some interval <span class="math inline">\(\left(x_{1},x_{2}\right)\)</span> on which <span class="math inline">\(F_{X}\)</span> is constant, we have <span class="math inline">\(F_{X}^{-1}\left(y\right)=x_{1}\)</span>. At the endpoints of the range of <span class="math inline">\(y\)</span>, <span class="math inline">\(F_{X}^{-1}\left(y\right)\)</span> can also be defined. <span class="math inline">\(F_{X}^{-1}\left(1\right)=\infty\)</span> if <span class="math inline">\(F_{X}\left(x\right)&lt;1\)</span> for all <span class="math inline">\(x\)</span> and, for any <span class="math inline">\(F_{X}\)</span>, <span class="math inline">\(F_{X}^{-1}\left(0\right)=-\infty\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-12" class="theorem"><strong>Theorem 3.6  (Probability integral transformation)  </strong></span>Let <span class="math inline">\(X\)</span> have continuous cdf <span class="math inline">\(F_{X}\left(x\right)\)</span> and define the random variable <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y=F_{X}\left(X\right)\)</span>. Then <span class="math inline">\(Y\)</span> is uniformly distributed on <span class="math inline">\(\left(0,1\right)\)</span>, that is, <span class="math inline">\(P\left(\left\{ Y\leq y\right\} \right)=y\)</span>, <span class="math inline">\(0&lt;y&lt;1\)</span>.</p>
(This is Theorem 2.1.10 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> For <span class="math inline">\(Y=F_{X}\left(X\right)\)</span> we have, for <span class="math inline">\(0&lt;y&lt;1\)</span>,</p>
<span class="math display">\[\begin{align*}
P\left(\left\{ Y\leq y\right\} \right) &amp; =P\left(\left\{ F_{X}\left(X\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ F_{X}^{-1}\left[F_{X}\left(X\right)\right]\leq F_{X}^{-1}\left(y\right)\right\} \right)\tag{$F_{X}^{-1}$ is increasing} \\
    &amp; =P\left(\left\{ X\leq F_{X}^{-1}\left(y\right)\right\} \right)\tag{see paragraph below} \\
    &amp; =F_{X}\left(F_{X}^{-1}\left(y\right)\right)\tag{definition of $F_{X}$} \\
    &amp; =y.\tag{continuity of $F_{X}$}
\end{align*}\]</span>
<p>At the endpoints we have <span class="math inline">\(P\left(\left\{ Y\leq y\right\} \right)=1\)</span> for <span class="math inline">\(y\geq 1\)</span> and <span class="math inline">\(P\left(\left\{ Y\leq y\right\} \right)=0\)</span> for <span class="math inline">\(y\leq 0\)</span>, showing that <span class="math inline">\(Y\)</span> has a uniform distribution.</p>
<p>The reasoning behind the equality</p>
<p><span class="math display">\[
P\left(\left\{ F_{X}^{-1}\left(F_{X}\left(X\right)\right)\leq F_{X}^{-1}\left(y\right)\right\} \right)=P\left(\left\{ X\leq F_{X}^{-1}\left(y\right)\right\} \right)
\]</span></p>
is somewhat subtle and deserves additional attention. If <span class="math inline">\(F_{X}\)</span> is strictly increasing, then it is true that <span class="math inline">\(F_{X}^{-1}\left(F_{X}\left(x\right)\right)=x\)</span>. However, if <span class="math inline">\(F_{X}\)</span> is flat, it may be that <span class="math inline">\(F_{X}^{-1}\left(F_{X}\left(x\right)\right)\neq x\)</span>. Suppose <span class="math inline">\(F_{X}\)</span> contains an interval <span class="math inline">\(\left(x_{1},x_{2}\right)\)</span> on which <span class="math inline">\(F_{X}\)</span> is constant, and let <span class="math inline">\(x\in\left[x_{1},x_{2}\right]\)</span>. Then <span class="math inline">\(F_{X}^{-1}\left(F_{X}\left(x\right)\right)=x_{1}\)</span> for any <span class="math inline">\(x\)</span> in this interval. Even in this case, though, the probability equality holds, since <span class="math inline">\(P\left(\left\{ X\leq x\right\} \right)=P\left(\left\{ X\leq x_{1}\right\} \right)\)</span> for any <span class="math inline">\(x\in\left[x_{1},x_{2}\right]\)</span>. The flat cdf denotes a region of <span class="math inline">\(0\)</span> probability <span class="math inline">\((P\left(\left\{ x_{1}&lt;X\leq x\right\} \right)=F_{X}\left(x\right)-F_{X}\left(x_{1}\right)=0)\)</span>.
</div>

</div>
<div id="expected-values" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Expected values</h3>

<div class="theorem">
<p><span id="thm:properties-of-expectation" class="theorem"><strong>Theorem 3.7  </strong></span>Let <span class="math inline">\(X\)</span> be a random variable and let <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> be constants. Then for any functions <span class="math inline">\(g_{1}\left(x\right)\)</span> and <span class="math inline">\(g_{2}\left(x\right)\)</span> whose expectations exist,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\E\left[ag_{1}\left(X\right)+bg_{2}\left(X\right)+c\right]=a\E\left[g_{1}\left(X\right)\right]+b\E\left[g_{2}\left(X\right)\right]+c\)</span>.</li>
<li>If <span class="math inline">\(g_{1}\left(x\right)\geq 0\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(\E\left[g_{1}\left(X\right)\right]\geq 0\)</span>.</li>
<li>If <span class="math inline">\(g_{1}\left(x\right)\geq g_{2}\left(x\right)\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(\E\left[g_{1}\left(X\right)\right]\geq\E\left[g_{2}\left(X\right)\right]\)</span>.</li>
<li>If <span class="math inline">\(a\leq g_{1}\left(x\right)\leq b\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(a\leq\E\left[g_{1}\left(X\right)\right]\leq b\)</span>.</li>
</ol>
(This is Theorem 2.2.5 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>

</div>
<div id="moments-and-moment-generating-functions" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Moments and moment generating functions</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-15" class="definition"><strong>Definition 3.2  </strong></span>For each integer <span class="math inline">\(n\)</span>, the <span class="math inline">\(n\text{th}\)</span> <em>moment</em> of <span class="math inline">\(X\)</span> or <span class="math inline">\((F_{X}\left(x\right))\)</span>, <span class="math inline">\(\mu&#39;_{n}\)</span>, is</p>
<p><span class="math display">\[
\mu&#39;_{n}=\E\left[X^{n}\right].
\]</span></p>
<p>The <span class="math inline">\(n\text{th}\)</span> <em>central moment</em> of <span class="math inline">\(X\)</span>, <span class="math inline">\(\mu_{n}\)</span>, is</p>
<p><span class="math display">\[
\mu_{n}=\E\left[\left(X-\mu\right)^{n}\right],
\]</span></p>
where <span class="math inline">\(\mu=\mu&#39;_{1}=\E\left[X\right]\)</span>.
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-16" class="definition"><strong>Definition 3.3  </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with cdf <span class="math inline">\(F_{X}\)</span>. The <em>moment generating function</em> (mgf) of <span class="math inline">\(X\)</span> (or <span class="math inline">\(F_{X}\)</span>), denoted by <span class="math inline">\(M_{X}\left(t\right)\)</span>, is</p>
<p><span class="math display">\[
M_{X}\left(t\right)=\E\left[\mathrm{e}^{tX}\right],
\]</span></p>
provided that the expectation exists for <span class="math inline">\(t\)</span> in some neighborhood of <span class="math inline">\(0\)</span>. That is, there is an <span class="math inline">\(h&gt;0\)</span> such that, for all <span class="math inline">\(t\)</span> in <span class="math inline">\(-h&lt;t&lt;h\)</span>, <span class="math inline">\(\E\left[\mathrm{e}^{tX}\right]\)</span> exists. If the expectation does not exist in a neighborhood of <span class="math inline">\(0\)</span>, we say that the moment generating function does not exist.
</div>

<p>More explicitly, we can write the mgf of <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[
M_{X}\left(t\right)=\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x
\]</span> if <span class="math inline">\(X\)</span> is continuous, or</p>
<p><span class="math display">\[
M_{X}\left(t\right)=\sum_{x}\mathrm{e}^{tx}P\left(\left\{ X=x\right\} \right)
\]</span></p>
<p>if <span class="math inline">\(X\)</span> is discrete.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-17" class="theorem"><strong>Theorem 3.8  </strong></span>If <span class="math inline">\(X\)</span> has mgf <span class="math inline">\(M_{X}\left(t\right)\)</span>, then</p>
<p><span class="math display">\[
\E\left[X^{n}\right]=M_{X}^{\left(n\right)}\left(0\right),
\]</span></p>
<p>where we define</p>
<p><span class="math display">\[
M_{X}^{\left(n\right)}\left(0\right)=\frac{\dif^{n}}{\dif t^{n}}M_{X}\left(t\right)\Bigr\vert_{t=0}.
\]</span></p>
<p>That is, the <span class="math inline">\(n\text{th}\)</span> moment is equal to the <span class="math inline">\(n\text{th}\)</span> derivative of <span class="math inline">\(M_{X}\left(t\right)\)</span> evaluated at <span class="math inline">\(t=0\)</span>.</p>
(This is Theorem 2.3.7 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Assuming that we can differentiate under the integral sign, we have</p>
<span class="math display">\[\begin{align*}
\frac{\dif}{\dif t}M_{X}\left(t\right) &amp; =\frac{\dif}{\dif t}\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x \\
  &amp; =\int_{-\infty}^{\infty}\left(\frac{\dif}{\dif t}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x \\
  &amp; =\int_{-\infty}^{\infty}\left(x\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x \\
  &amp; =\E\left[X\mathrm{e}^{tX}\right].
\end{align*}\]</span>
<p>Thus,</p>
<p><span class="math display">\[
\frac{\dif}{\dif t}M_{X}\left(t\right)\Bigr\vert_{t=0}=\E\left[X\mathrm{e}^{tX}\right]\Big\vert_{t=0}=\E\left[X\mathrm{e}^{0}\right]=\E\left[X\right].
\]</span></p>
<p>Noting that</p>
<p><span class="math display">\[
\frac{\dif^{n}}{\dif t^{n}}\mathrm{e}^{tx}=\frac{\dif^{n-1}}{\dif t^{n-1}}\left[\frac{\dif}{\dif t}\mathrm{e}^{tx}\right]=\frac{\dif^{n-2}}{\dif t^{n-2}}\left[\frac{\dif}{\dif t}x\mathrm{e}^{tx}\right]=\frac{\dif^{n-2}}{\dif t^{n-2}}x^{2}\mathrm{e}^{tx}=\frac{\dif}{\dif t}x^{n-1}\mathrm{e}^{tx}=x^{n}\mathrm{e}^{tx},
\]</span></p>
<p>we can establish that</p>
<span class="math display">\[\begin{align*}
\frac{\dif^{n}}{\dif t^{n}}M_{X}\left(t\right)\Bigr\vert_{t=0} &amp; =\frac{\dif^{n}}{\dif t^{n}}\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x\Big\vert_{t=0} \\
    &amp; =\int_{-\infty}^{\infty}\left(\frac{\dif^{n}}{\dif t^{n}}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\Big\vert_{t=0} \\
    &amp; =\int_{-\infty}^{\infty}\left(x^{n}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\Big\vert_{t=0} \\
    &amp; =\E\left[X^{n}\mathrm{e}^{tX}\right]\Big\vert_{t=0} \\
    &amp; =\E\left[X^{n}\right].
\end{align*}\]</span>
</div>


<div class="lemma">
<p><span id="lem:limit-sequence-exponential" class="lemma"><strong>Lemma 3.1  </strong></span>Let <span class="math inline">\(a_{1},a_{2},\ldots\)</span> be a sequence of numbers converging to <span class="math inline">\(a\)</span>, that is, <span class="math inline">\(\lim_{n\rightarrow\infty}a_{n}=a\)</span>. Then</p>
<p><span class="math display">\[
\lim_{n\rightarrow\infty}\left(1+\frac{a_{n}}{n}\right)^{n}=\mathrm{e}^{a}.
\]</span></p>
(This is Lemma 2.3.14 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="theorem">
<p><span id="thm:mgf-affine-transform" class="theorem"><strong>Theorem 3.9  </strong></span>For any constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the mgf of the random variable <span class="math inline">\(aX+b\)</span> is given by</p>
<p><span class="math display">\[
M_{aX+b}\left(t\right)=\mathrm{e}^{bt}M_{X}\left(at\right).
\]</span></p>
(This is Theorem 2.3.15 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> By definition,</p>
<p><span class="math display">\[
M_{aX+b}\left(t\right)=
  \E\left[\mathrm{e}^{\left(aX+b\right)t}\right]=
  \E\left[\mathrm{e}^{\left(aX\right)t}\mathrm{e}^{bt}\right]=
  \mathrm{e}^{bt}\E\left[\mathrm{e}^{\left(at\right)X}\right]=
  \mathrm{e}^{bt}M_{X}\left(at\right),
\]</span></p>
proving the theorem.
</div>

</div>
</div>
<div id="multiple-random-variables" class="section level2">
<h2><span class="header-section-number">3.3</span> Multiple random variables</h2>
<div id="conditional-distributions-and-independence" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Conditional distributions and independence</h3>

<div class="definition">
<span id="def:unnamed-chunk-20" class="definition"><strong>Definition 3.4  </strong></span>Let <span class="math inline">\(\left(X,Y\right)\)</span> be a bivariate random vector with joint pdf or pmf <span class="math inline">\(f\left(x,y\right)\)</span> and marginal pdfs or pmfs <span class="math inline">\(f_{X}\left(x\right)\)</span> and <span class="math inline">\(f_{Y}\left(y\right)\)</span>. Then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are called <em>independent random variables</em> if, for every <span class="math inline">\(x\in\mathbb{R}\)</span> and <span class="math inline">\(y\in\mathbb{R}\)</span>, <span class="math inline">\(f\left(x,y\right)=f_{X}\left(x\right)f_{Y}\left(y\right)\)</span>.
</div>


<div class="theorem">
<p><span id="thm:expected-value-of-two-ind-rvs" class="theorem"><strong>Theorem 3.10  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables.</p>
<ol style="list-style-type: decimal">
<li>For any <span class="math inline">\(\mathcal{A}\subset\mathbb{R}\)</span> and <span class="math inline">\(\mathcal{B}\subset\mathbb{R}\)</span>, <span class="math inline">\(P\left(\left\{ X\in \mathcal{A}\right\} \cap\left\{ Y\in \mathcal{B}\right\} \right)=P\left(\left\{ X\in \mathcal{A}\right\} \right)P\left(\left\{ Y\in \mathcal{B}\right\} \right)\)</span>; that is, the events <span class="math inline">\(\left\{ X\in \mathcal{A}\right\}\)</span> and <span class="math inline">\(\left\{ Y\in \mathcal{B}\right\}\)</span> are independent events.</li>
<li>Let <span class="math inline">\(g\left(x\right)\)</span> be a function only of <span class="math inline">\(x\)</span> and <span class="math inline">\(h\left(y\right)\)</span> be a function only of <span class="math inline">\(y\)</span>. Then</li>
</ol>
<p><span class="math display">\[
\E\left[g\left(X\right)h\left(Y\right)\right]=\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right].
\]</span></p>
(This is Theorem 4.2.10 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> For continuous random variables, part (2) is proved by noting that</p>
<span class="math display">\[\begin{align*}
\E\left[g\left(X\right)h\left(Y\right)\right] &amp; =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x\right)h\left(y\right)f\left(x,y\right)\dif x\dif y \\
    &amp; =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x\right)h\left(y\right)f_{X}\left(x\right)f_{Y}\left(y\right)\dif x\dif y\tag{independence} \\
    &amp; =\int_{-\infty}^{\infty}h\left(y\right)f_{Y}\left(y\right)\int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)\dif x\dif y \\
    &amp; =\left(\int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)\dif x\right)\left(\int_{-\infty}^{\infty}h\left(y\right)f_{Y}\left(y\right)\dif y\right) \\
    &amp; =\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right].
\end{align*}\]</span>
<p>The result for discrete random variables is proved by replacing integrals by sums. Part (1) can be proved by series of steps similar to those above or by the following argument. Let <span class="math inline">\(g\left(x\right)\)</span> be the indicator function of the set <span class="math inline">\(\mathcal{A}\)</span>. Let <span class="math inline">\(h\left(y\right)\)</span> be the indicator function of the set <span class="math inline">\(\mathcal{B}\)</span>. Note that <span class="math inline">\(g\left(x\right)h\left(y\right)\)</span> is the indicator function of the set <span class="math inline">\(\mathcal{C}\subset\mathbb{R}^{2}\)</span> defined by <span class="math inline">\(\mathcal{C}=\left\{ \left(x,y\right):x\in \mathcal{A},y\in \mathcal{B}\right\}\)</span>. Thus using the expectation equality just proved, we have</p>
<span class="math display">\[\begin{align*}
P\left(\left\{ X\in A\right\} \cap\left\{ Y\in B\right\} \right) &amp; =P\left(\left\{ \left(X,Y\right)\in C\right\} \right) \\
    &amp; =\E\left[g\left(X\right)h\left(Y\right)\right] \\
    &amp; =\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right] \\
    &amp; =P\left(\left\{ X\in A\right\} \right)P\left(\left\{ Y\in B\right\} \right).
\end{align*}\]</span>
</div>


<div class="theorem">
<p><span id="thm:mgf-of-sum-of-two-ind-rvs" class="theorem"><strong>Theorem 3.11  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables with moment generating functions <span class="math inline">\(M_{X}\left(t\right)\)</span> and <span class="math inline">\(M_{Y}\left(t\right)\)</span>. Then the moment generating function of the random variable <span class="math inline">\(Z=X+Y\)</span> is given by <span class="math inline">\(M_{Z}\left(t\right)=M_{X}\left(t\right)M_{Y}\left(t\right)\)</span>.</p>
(This is Theorem 4.2.12 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Using the definition of the mgf and Theorem <a href="probability-theory.html#thm:expected-value-of-two-ind-rvs">3.10</a>, we have</p>
<span class="math display">\[
M_{Z}\left(t\right)=
  \E\left[\mathrm{e}^{tZ}\right]=
  \E\left[\mathrm{e}^{t\left(X+Y\right)}\right]=
  \E\left[\mathrm{e}^{tX}\mathrm{e}^{tY}\right]=
  \E\left[\mathrm{e}^{tX}\right]\E\left[\mathrm{e}^{tY}\right]=
  M_{X}\left(t\right)M_{Y}\left(t\right).
\]</span>
</div>

</div>
<div id="covariance-and-correlation" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Covariance and correlation</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-23" class="definition"><strong>Definition 3.5  </strong></span>The <em>covariance</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the number defined by</p>
<span class="math display">\[
\Cov\left(X,Y\right)=\E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right].
\]</span>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-24" class="definition"><strong>Definition 3.6  </strong></span>The <em>correlation</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the number defined by</p>
<p><span class="math display">\[
\rho_{XY}=\frac{\Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}.
\]</span></p>
The value <span class="math inline">\(\rho_{XY}\)</span> is also called the <em>correlation coefficient</em>.
</div>


<div class="theorem">
<p><span id="thm:cov-of-ind-rvs" class="theorem"><strong>Theorem 3.12  </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math inline">\(\Cov\left(X,Y\right)=0\)</span> and <span class="math inline">\(\rho_{XY}=0\)</span>.</p>
(This is Theorem 4.5.5 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we have <span class="math inline">\(\E\left[XY\right]=\E\left[X\right]\E\left[Y\right]\)</span>. Thus</p>
<span class="math display">\[\begin{align*}
\E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right] &amp; =\E\left[XY-\mu_{Y}X-\mu_{X}Y+\mu_{X}\mu_{Y}\right] \\
    &amp; =\E\left[XY\right]-\mu_{Y}\E\left[X\right]-\mu_{X}\E\left[Y\right]+\mu_{X}\mu_{Y} \\
    &amp; =\E\left[X\right]\E\left[Y\right]-\E\left[Y\right]\E\left[X\right]-\E\left[X\right]\E\left[Y\right]+\E\left[X\right]\E\left[Y\right] \\
    &amp; =0
\end{align*}\]</span>
<p>and</p>
<span class="math display">\[
\rho_{XY}=\frac{\Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}=\frac{0}{\sigma_{X}\sigma_{Y}}=0.
\]</span>
</div>


<div class="theorem">
<p><span id="thm:variance-of-ind-rvs" class="theorem"><strong>Theorem 3.13  </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are any two random variables and <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are any two constants, then</p>
<p><span class="math display">\[
\Var\left(aX+bY\right)=a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right)+2ab\Cov\left(X,Y\right).
\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then</p>
<p><span class="math display">\[
\Var\left(aX+bY\right)=a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right).
\]</span></p>
(This is Theorem 4.5.6 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We have</p>
<span class="math display">\[\begin{align*}
\Var\left(aX+bY\right) &amp; =\E\left[\left(\left(aX+bY\right)-\E\left[aX+bY\right]\right)^{2}\right] \\
    &amp; =\E\left[\left(aX+bY-a\E\left[X\right]-b\E\left[Y\right]\right)^{2}\right] \\
    &amp; =\E\left[\left(aX+bY-a\mu_{X}-b\mu_{Y}\right)^{2}\right] \\
    &amp; =\E\left[\left(a\left(X-\mu_{X}\right)+b\left(Y-\mu_{Y}\right)\right)^{2}\right] \\
    &amp; =\E\left[\left(a\left(X-\mu_{X}\right)\right)^{2}-2ab\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)+\left(b\left(Y-\mu_{Y}\right)\right)^{2}\right] \\
    &amp; =\E\left[a^{2}\left(X-\mu_{X}\right)^{2}\right]-\E\left[2ab\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]+\E\left[b^{2}\left(Y-\mu_{Y}\right)^{2}\right] \\
    &amp; =a^{2}\E\left[\left(X-\mu_{X}\right)^{2}\right]-2ab\E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]+b^{2}\E\left[\left(Y-\mu_{Y}\right)^{2}\right] \\
    &amp; =a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right)-2ab\Cov\left(X,Y\right).
\end{align*}\]</span>
If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, it follows from Theorem <a href="probability-theory.html#thm:cov-of-ind-rvs">3.12</a> that <span class="math inline">\(\Cov\left(X,Y\right)=0\)</span> and the second equality is immediate from the first.
</div>


<div class="theorem">
<p><span id="thm:correlation-values" class="theorem"><strong>Theorem 3.14  </strong></span>For any random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(-1\leq\rho_{XY}\leq 1\)</span>.</li>
<li><span class="math inline">\(\left|\rho_{XY}\right|=1\)</span> if and only if there exist numbers <span class="math inline">\(a\neq 0\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(P\left(\left\{ Y=aX+b\right\} \right)=1\)</span>. If <span class="math inline">\(\rho_{XY}=1\)</span>, then <span class="math inline">\(a&gt;0\)</span>, and if <span class="math inline">\(\rho_{XY}=-1\)</span>, then <span class="math inline">\(a&lt;0\)</span>.</li>
</ol>
(This is Theorem 4.5.7 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>

</div>
<div id="multivariate-distributions" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Multivariate distributions</h3>

<div class="theorem">
<p><span id="thm:mgf-of-sum-of-multiple-ind-rvs" class="theorem"><strong>Theorem 3.15  </strong></span>Let <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> be mutually independent random variables with mgfs <span class="math inline">\(M_{X_{1}}\left(t\right),\ldots M_{X_{n}}\left(t\right)\)</span>. Let <span class="math inline">\(Z=X_{1}+\cdots+X_{n}\)</span>. Then the mgf of <span class="math inline">\(Z\)</span> is</p>
<p><span class="math display">\[
M_{Z}\left(t\right)=M_{X_{1}}\left(t\right)\cdot\cdots\cdot M_{X_{n}}\left(t\right).
\]</span></p>
<p>In particular, if <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> all have the same distribution with mgf <span class="math inline">\(M_{X}\left(t\right)\)</span>, then <span class="math inline">\(M_{Z}\left(t\right)=\left(M_{X}\left(t\right)\right)^{n}\)</span>.</p>
(This is Theorem 4.6.7 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>, which is a generalization of Theorem <a href="probability-theory.html#thm:mgf-of-sum-of-two-ind-rvs">3.11</a>).
</div>

</div>
<div id="inequalities" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Inequalities</h3>

<div class="lemma">
<p><span id="lem:youngs-inequality" class="lemma"><strong>Lemma 3.2  (Young’s Inequality)  </strong></span>Let <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> be any positive numbers, and let <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> be any positive numbers (necessarily greater than 1) satisfying</p>
<p><span class="math display">\[
\frac{1}{p}+\frac{1}{q}=1.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\frac{1}{p}a^{p}+\frac{1}{q}b^{q}\geq ab
\]</span></p>
<p>with equality if and only if <span class="math inline">\(a^{p}=b^{q}\)</span>.</p>
(This is Lemma 4.7.1 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there).
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Fix <span class="math inline">\(b\)</span>, and consider the function</p>
<p><span class="math display">\[
g\left(a\right)=\frac{1}{p}a^{p}+\frac{1}{q}b^{q}-ab.
\]</span></p>
<p>To minimize <span class="math inline">\(g\left(a\right)\)</span>, differentiate and set equal to <span class="math inline">\(0\)</span>:</p>
<p><span class="math display">\[
\frac{\dif}{\dif a}g\left(a\right)=0\implies a^{p-1}-b=0\implies b=a^{p-1}.
\]</span></p>
<p>We will evaluate the second derivative of <span class="math inline">\(g\)</span> with respect to <span class="math inline">\(a\)</span> at</p>
<p><span class="math display">\[
b=a^{p-1}\implies b^{1/\left(p-1\right)}=\left(a^{p-1}\right)^{1/\left(p-1\right)}\implies a=b^{1/\left(p-1\right)}
\]</span></p>
<p>to verify that this is a minimum.</p>
<span class="math display">\[\begin{align*}
\frac{\dif^{2}}{\dif a^{2}}\left[g\left(a\right)\right|_{a=b^{1/\left(p-1\right)}} &amp; =\frac{\dif}{\dif a}\left[a^{p-1}-b\right|_{a=b^{1/\left(p-1\right)}} \\
    &amp; =\left[\left(p-1\right)a^{p-2}\right|_{a=b^{1/\left(p-1\right)}} \\
    &amp; =\left(p-1\right)\left(b^{1/\left(p-1\right)}\right)^{p-2} \\
    &amp; =\left(p-1\right)b^{\left(p-2\right)/\left(p-1\right)} \\
    &amp; =\left(p-1\right)b^{\left(p-1-1\right)/\left(p-1\right)} \\
    &amp; =\left(p-1\right)b^{\left[\left(p-1\right)/\left(p-1\right)\right]-1/\left(p-1\right)} \\
    &amp; =\left(p-1\right)b^{1-1/\left(p-1\right)}
\end{align*}\]</span>
<p>We have <span class="math inline">\(p&gt;1\)</span> and <span class="math inline">\(b&gt;0\)</span>, so that <span class="math inline">\(p-1&gt;0\)</span> and <span class="math inline">\(b^{1-1/\left(p-1\right)}&gt;0\)</span>. It follows that <span class="math inline">\(b=a^{p-1}\)</span> is a minimum. We have</p>
<p><span class="math display">\[
\frac{1}{p}+\frac{1}{q}=1\implies\frac{1}{q}=1-\frac{1}{p}=\frac{p-1}{p}\implies q\left(p-1\right)=p,
\]</span></p>
<p>so that the value of <span class="math inline">\(g\left(a\right)\)</span> at the minimum is</p>
<p><span class="math display">\[
\frac{1}{p}a^{p}+\frac{1}{q}\left(a^{p-1}\right)^{q}-aa^{p-1}=\frac{1}{p}a^{p}+\frac{1}{q}a^{p}-a^{p}=a^{p}\left(\frac{1}{p}+\frac{1}{q}-1\right)=a^{p}\left(1-1\right)=0.
\]</span></p>
<p>Hence the minimum is <span class="math inline">\(0\)</span> and the inequality is established. The domain of <span class="math inline">\(g\)</span> is <span class="math inline">\(\left\{ a:0&lt;a&lt;\infty\right\}\)</span> and we have <span class="math inline">\(p&gt;1\)</span>, so that for some fixed <span class="math inline">\(b\)</span>, <span class="math inline">\(g&#39;\left(a\right)=a^{p-1}-b\)</span> is increasing in <span class="math inline">\(a\)</span>. Thus, the minimum we found is unique, so that equality holds only if <span class="math inline">\(a^{p-1}=b\)</span>, which is equivalent to</p>
<span class="math display">\[
a^{p-1}=b\implies a^{p/q}=b\implies\left(a^{p/q}\right)^{q}=b^{q}\implies a^{p}=b^{q}.
\]</span>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-29" class="theorem"><strong>Theorem 3.16  (Hölder’s Inequality)  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be any two random variables, and let <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> satisfy Lemma <a href="probability-theory.html#lem:youngs-inequality">3.2</a>. Then</p>
<p><span class="math display">\[
\left|\E\left[XY\right]\right|\leq
\E\left[\left|XY\right|\right]\leq
\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}.
\]</span></p>
(This is Theorem 4.7.2 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The first inequality follows from <span class="math inline">\(-\left|XY\right|\leq XY\leq\left|XY\right|\)</span> and Theorem <a href="probability-theory.html#thm:properties-of-expectation">3.7</a>. To prove the second inequality, define</p>
<p><span class="math display">\[
a=\frac{\left|X\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}}\quad\text{and}\quad b=\frac{\left|Y\right|}{\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}.
\]</span></p>
<p>Applying Lemma <a href="probability-theory.html#lem:youngs-inequality">3.2</a>, we get</p>
<span class="math display">\[\begin{align*}
\frac{1}{p}a^{p}+\frac{1}{q}b^{q}   &amp; \geq ab \\
\implies\frac{1}{p}\left(\frac{\left|X\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}}\right)^{p}+\frac{1}{q}\left(\frac{\left|Y\right|}{\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\right)^{q} &amp; \geq\frac{\left|X\right|\left|Y\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}} \\
\implies\frac{1}{p}\frac{\left|X\right|^{p}}{\E\left[\left|X\right|^{p}\right]}+\frac{1}{q}\frac{\left|Y\right|^{q}}{\E\left[\left|Y\right|^{q}\right]} &amp; \geq\frac{\left|XY\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}.
\end{align*}\]</span>
<p>Taking the expectation of both sides gives</p>
<span class="math display">\[\begin{align*}
\E\left[\frac{1}{p}\frac{\left|X\right|^{p}}{\E\left[\left|X\right|^{p}\right]}+\frac{1}{q}\frac{\left|Y\right|^{q}}{\E\left[\left|Y\right|^{q}\right]}\right]  &amp; \geq\E\left[\frac{\left|XY\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\right] \\
\implies\frac{1}{p\E\left[\left|X\right|^{p}\right]}\E\left[\left|X\right|^{p}\right]+\frac{1}{q\E\left[\left|Y\right|^{q}\right]}\E\left[\left|Y\right|^{q}\right] &amp; \geq\E\left[\frac{\left|XY\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\right] \\
\implies\frac{1}{p}+\frac{1}{q} &amp; \geq\frac{1}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\E\left[\left|XY\right|\right] \\
\implies1   &amp; \geq\frac{1}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\E\left[\left|XY\right|\right] \\
\implies\E\left[\left|XY\right|\right] &amp; \leq\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}.
\end{align*}\]</span>
</div>

<p>Perhaps the most famous special case of Hölder’s Inequality is that for which <span class="math inline">\(p=q=2\)</span>.</p>

<div class="theorem">
<p><span id="thm:cauchy-schwarz-inequality" class="theorem"><strong>Theorem 3.17  (Cauchy-Schwarz Inequality)  </strong></span>For any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[
\left|\E\left[XY\right]\right|\leq\E\left[\left|XY\right|\right]\leq\left(\E\left[\left|X\right|^{2}\right]\right)^{1/2}\left(\E\left[\left|Y\right|^{2}\right]\right)^{1/2}.
\]</span></p>
(This is Theorem 4.7.3 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>

</div>
</div>
<div id="properties-of-a-random-sample" class="section level2">
<h2><span class="header-section-number">3.4</span> Properties of a random sample</h2>
<div id="sums-of-random-variables-from-a-random-sample" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Sums of random variables from a random sample</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-31" class="definition"><strong>Definition 3.7  </strong></span>The <em>sample variance</em> is the statistic defined by</p>
<p><span class="math display">\[
S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}.
\]</span></p>
The sample <em>standard deviation</em> is the statistic defined by <span class="math inline">\(S=\sqrt{S^{2}}\)</span>.
</div>


<div class="theorem">
<p><span id="thm:computing-sums-rand-samples" class="theorem"><strong>Theorem 3.18  </strong></span>Let <span class="math inline">\(x_{1},\ldots,x_{n}\)</span> be any numbers and <span class="math inline">\(\bar{x}=\left(x_{1}+\cdots+x_{n}\right)/n\)</span>. Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\min_{a}\sum_{i=1}^{n}\left(x_{i}-a\right)^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\)</span>,</li>
<li><span class="math inline">\(\left(n-1\right)s^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2}\)</span>.</li>
</ol>
(This is Theorem 5.2.4 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> To prove part (1), add and subtract <span class="math inline">\(\bar{x}\)</span> to get</p>
<span class="math display">\[\begin{align*}
\sum_{i=1}^{n}\left(x_{i}-a\right)^{2}  &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}+\bar{x}-a\right)^{2} \\
 &amp; =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)\right]\left[\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)\right] \\
 &amp; =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)^{2}+\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)+\left(\bar{x}-a\right)\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)^{2}\right] \\
    &amp; =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)^{2}+2\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)+\left(\bar{x}-a\right)^{2}\right] \\
    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left[2\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)\right]+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2} \\
    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\sum_{i=1}^{n}\left(x_{i}\bar{x}-ax_{i}-\bar{x}^{2}+a\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2} \\
    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}\sum_{i=1}^{n}x_{i}-a\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}\bar{x}^{2}+\sum_{i=1}^{n}a\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2} \\
    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}\left(n\bar{x}\right)-a\left(n\bar{x}\right)-n\bar{x}^{2}+na\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2} \\
    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(n\bar{x}^{2}-na\bar{x}-n\bar{x}^{2}+na\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2} \\
    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(0\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2} \\
    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}.
\end{align*}\]</span>
<p>It is now clear that the right-hand side is minimized at <span class="math inline">\(a=\bar{x}\)</span>. To prove part (2), take <span class="math inline">\(a=0\)</span> in the above, i.e.,</p>
<span class="math display">\[\begin{align*}
\sum_{i=1}^{n}\left(x_{i}-a\right)^{2}  &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2} \\
\implies\sum_{i=1}^{n}\left(x_{i}-0\right)^{2}  &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-0\right)^{2} \\
\implies\sum_{i=1}^{n}x_{i}^{2} &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\bar{x}^{2} \\
\implies\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}\bar{x}^{2}   &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2} \\
\implies\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2}    &amp; =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}.
\end{align*}\]</span>
<p>The sample variance is defined as</p>
<p><span class="math display">\[
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\implies\left(n-1\right)s^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2},
\]</span></p>
so the final equality of part (2) holds.
</div>

</div>
<div id="sampling-from-the-normal-distribution" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Sampling from the normal distribution</h3>

<div class="theorem">
<p><span id="thm:rand-sample-from-normal-dist" class="theorem"><strong>Theorem 3.19  </strong></span>Let <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> be a random sample from a <span class="math inline">\(\mathcal{N}\left(\mu,\sigma^{2}\right)\)</span> distribution, and let <span class="math inline">\(\bar{X}=\left(1/n\right)\sum_{i=1}^{n}X_{i}\)</span> and <span class="math inline">\(S^{2}=\left[1/\left(n-1\right)\right]\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\)</span>. Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^{2}\)</span> are independent random variables,</li>
<li><span class="math inline">\(\bar{X}\)</span> has a <span class="math inline">\(\mathcal{N}\left(\mu,\sigma^{2}/n\right)\)</span> distribution,</li>
<li><span class="math inline">\(\left(n-1\right)S^{2}/\sigma^{2}\)</span> has a chi-squared distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</li>
</ol>
(This is Theorem 5.3.1 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.; the following proof is given there.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>


<div class="definition">
<p><span id="def:t-distribution" class="definition"><strong>Definition 3.8  </strong></span>Let <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> be a random sample from a <span class="math inline">\(\mathcal{N}\left(\mu,\sigma^{2}\right)\)</span> distribution. The quantity <span class="math inline">\(\left(\bar{X}-\mu\right)/\left(S/\sqrt{n}\right)\)</span> has <em>Student’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</em> Equivalently, a random variable <span class="math inline">\(T\)</span> has Student’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(p\)</span> degrees of freedom, and we write <span class="math inline">\(T\sim t_{p}\)</span> if it has pdf</p>
<span class="math display">\[
f_{T}\left(t\right)=\frac{\Gamma\left(\frac{p+1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)}\frac{1}{\left(p\pi\right)^{1/2}}\frac{1}{\left(1+t^{2}/p\right)^{\left(p+1\right)/2}},\quad-\infty&lt;t&lt;\infty.
\]</span>
</div>

</div>
<div id="order-statistics" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Order statistics</h3>
<p>The order statistics of a random sample <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> are the sample values placed in ascending order. They are denoted by <span class="math inline">\(X_{\left(1\right)},\ldots,X_{\left(n\right)}\)</span>. The order statistics are random variables that satisfy <span class="math inline">\(X_{\left(1\right)}\leq\ldots\leq X_{\left(n\right)}\)</span>, and in particular, <span class="math inline">\(X_{\left(1\right)}=\underset{1\leq i\leq n}{\min}X_{i}\)</span> and <span class="math inline">\(X_{\left(n\right)}=\underset{1\leq i\leq n}{\max}X_{i}\)</span>.</p>

<div class="theorem">
<p><span id="thm:order-stat-discrete" class="theorem"><strong>Theorem 3.20  </strong></span>Let <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> be a random sample from a discrete distribution with pmf <span class="math inline">\(f_{X}\left(x_{i}\right)=p_{i}\)</span>, where <span class="math inline">\(x_{1}&lt;x_{2}&lt;\cdots\)</span> are the possible values of <span class="math inline">\(X\)</span> in ascending order. Define</p>
<span class="math display">\[\begin{align*}
P_{0}   &amp; =0 \\
P_{1}   &amp; =p_{1} \\
P_{2}   &amp; =p_{1}+p_{2} \\
    &amp; \vdots \\
P_{i}   &amp; =p_{1}+p_{2}+\cdots+p_{i} \\
    &amp; \vdots
\end{align*}\]</span>
<p>Let <span class="math inline">\(X_{\left(1\right)},\ldots,X_{\left(n\right)}\)</span> denote the order statistics from the sample. Then</p>
<p><span class="math display">\[
P\left(\left\{ X_{\left(j\right)}\leq x_{i}\right\} \right)=\sum_{k=j}^{n}\binom{n}{k}P_{i}^{k}\left(1-P_{i}\right)^{n-k}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P\left(\left\{ X_{\left(j\right)}=x_{i}\right\} \right)=\sum_{k=j}^{n}\binom{n}{k}\left[P_{i}^{k}\left(1-P_{i}\right)^{n-k}-P_{i-1}^{k}\left(1-P_{i-1}\right)^{n-k}\right].
\]</span></p>
(This is Theorem 5.4.3 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>


<div class="theorem">
<p><span id="thm:order-stat-continuous" class="theorem"><strong>Theorem 3.21  </strong></span>Let <span class="math inline">\(X_{\left(1\right)},\ldots,X_{\left(n\right)}\)</span> denote the order statistics of a random sample, <span class="math inline">\(X_{1},\ldots,X_{n}\)</span>, from a continuous population with cdf <span class="math inline">\(F_{X}\left(x\right)\)</span> and pdf <span class="math inline">\(f_{X}\left(x\right)\)</span>. Then the pdf of <span class="math inline">\(X_{\left(j\right)}\)</span> is</p>
<p><span class="math display">\[
f_{X_{\left(j\right)}}\left(x\right)=\frac{n!}{\left(j-1\right)!\left(n-j\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{j-1}\left[1-F_{X}\left(x\right)\right]^{n-j}.
\]</span></p>
(This is Theorem 5.4.4 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>


<div class="theorem">
<p><span id="thm:order-stat-joint-pdf" class="theorem"><strong>Theorem 3.22  </strong></span>Let <span class="math inline">\(X_{\left(1\right)},\ldots,X_{\left(n\right)}\)</span> denote the order statistics of a random sample, <span class="math inline">\(X_{1},\ldots,X_{n}\)</span>, from a continuous population with cdf <span class="math inline">\(F_{X}\left(x\right)\)</span> and pdf <span class="math inline">\(f_{X}\left(x\right)\)</span>. Then the joint pdf of <span class="math inline">\(X_{\left(i\right)} and X_{\left(j\right)}\)</span>, <span class="math inline">\(1\leq i&lt;j\leq n\)</span>, is</p>
<span class="math display">\[\begin{align*}
f_{X_{\left(i\right)},X_{\left(j\right)}}\left(u,v\right)   &amp; =\frac{n!}{\left(i-1\right)!\left(j-1-i\right)!\left(n-j\right)!}f_{X}\left(u\right)f_{X}\left(v\right) \\
 &amp;\quad\times\left[F_{X}\left(u\right)\right]^{i-1}\left[F_{X}\left(v\right)-F_{X}\left(u\right)\right]^{j-1-i}\left[1-F_{X}\left(v\right)\right]^{n-j}
\end{align*}\]</span>
<p>for <span class="math inline">\(-\infty&lt;u&lt;v&lt;\infty\)</span>.</p>
(This is Theorem 5.4.6 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>

</div>
<div id="convergence-concepts" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Convergence concepts</h3>

<div class="theorem">
<p><span id="thm:strong-lln" class="theorem"><strong>Theorem 3.23  (Strong Law of Large Numbers)  </strong></span>Let <span class="math inline">\(X_{1},X_{2},\ldots\)</span> be iid random variables with <span class="math inline">\(\E\left[X_{i}\right]=\mu\)</span> and <span class="math inline">\(\Var\left(X_{i}\right)=\sigma^{2}&lt;\infty\)</span>, and define <span class="math inline">\(\bar{X}_{n}=\left(1/n\right)\sum_{i=1}^{n}X_{i}\)</span>. Then, for every <span class="math inline">\(\epsilon&gt;0\)</span>,</p>
<p><span class="math display">\[
P\left(\lim_{n\rightarrow\infty}\left|\bar{X}_{n}-\mu\right|&lt;\epsilon\right)=1;
\]</span></p>
that is, <span class="math inline">\(\bar{X}_{n}\)</span> converges almost surely to <span class="math inline">\(\mu\)</span>.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>


<div class="theorem">
<p><span id="thm:central-limit" class="theorem"><strong>Theorem 3.24  (Central Limit Theorem)  </strong></span>Let <span class="math inline">\(X_{1},X_{2},\ldots\)</span> be a sequence of iid random variables whose mgfs exist in a neighborhood of <span class="math inline">\(0\)</span> (that is, <span class="math inline">\(M_{X_{i}}\left(t\right)\)</span> exists for <span class="math inline">\(\left|t\right|&lt;h\)</span>, for some positive <span class="math inline">\(h\)</span>). Let <span class="math inline">\(\E\left[X_{i}\right]=\mu\)</span> and <span class="math inline">\(\Var\left(X_{i}\right)=\sigma^{2}&gt;0\)</span>. (Both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^{2}\)</span> are finite since the mgf exists.) Define <span class="math inline">\(\bar{X}_{n}=\left(1/n\right)\sum_{i=1}^{n}X_{i}\)</span>. Let <span class="math inline">\(G_{n}\left(x\right)\)</span> denote the cdf of <span class="math inline">\(\sqrt{n}\left(\bar{X}-\mu\right)/\sigma\)</span>. Then, for any <span class="math inline">\(x\in\mathbb{R}\)</span>,</p>
<p><span class="math display">\[
\lim_{n\rightarrow\infty}G_{n}\left(x\right)=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-y^{2}/2}\dif y;
\]</span></p>
<p>that is, <span class="math inline">\(\sqrt{n}\left(\bar{X}_{n}-\mu\right)/\sigma\)</span> has a limiting standard normal distribution.</p>
(This is Theorem 5.5.14 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(Z\sim\mathcal{N}\left(0,1\right)\)</span>, so that the mgf of <span class="math inline">\(Z\)</span> given by <span class="math inline">\(M_{Z}\left(t\right)=\mathrm{e}^{0\cdot t+\left(1\cdot t^{2}\right)/2}=\mathrm{e}^{t^{2}/2}\)</span>. We will show that, for <span class="math inline">\(\left|t\right|&lt;h\)</span>, the mgf of <span class="math inline">\(\sqrt{n}\left(\bar{X}_{n}-\mu\right)/\sigma\)</span> converges to <span class="math inline">\(\mathrm{e}^{t^{2}/2}\)</span>.</p>
<p>Define <span class="math inline">\(Y_{i}=\left(X_{i}-\mu\right)/\sigma\)</span>, and let <span class="math inline">\(M_{Y}\left(t\right)\)</span> denote the common mgf of the <span class="math inline">\(Y_{i}\text{&#39;s}\)</span>, which exists for <span class="math inline">\(\left|t\right|&lt;\sigma h\)</span> and is given by Theorem <a href="probability-theory.html#thm:mgf-affine-transform">3.9</a>. We have</p>
<p><span class="math display">\[
\frac{X_{i}-\mu}{\sigma}=Y_{i}\implies X_{i}-\mu=\sigma Y_{i}\implies X_{i}=\sigma Y_{i}+\mu,
\]</span></p>
<p>so that</p>
<span class="math display">\[\begin{align*}
\frac{\sqrt{n}\left(\bar{X}_{n}-\mu\right)}{\sigma} &amp; =\frac{\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu\right)}{\sigma} \\
    &amp; =\frac{\sqrt{n}}{\sigma}\left[\frac{1}{n}\sum_{i=1}^{n}\left(\sigma Y_{i}+\mu\right)-\mu\right] \\
    &amp; =\frac{\sqrt{n}}{\sigma}\left[\frac{1}{n}\left(\sigma\sum_{i=1}^{n}Y_{i}+n\mu\right)-\mu\right] \\
    &amp; =\frac{\sqrt{n}}{\sigma}\left[\frac{\sigma}{n}\sum_{i=1}^{n}Y_{i}+\mu-\mu\right] \\
    &amp; =\frac{\sqrt{n}}{\sigma}\left(\frac{\sigma}{n}\sum_{i=1}^{n}Y_{i}\right) \\
    &amp; =\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i}.
\end{align*}\]</span>
<p>Then, from the properties of mgfs (see Theorems <a href="probability-theory.html#thm:mgf-affine-transform">3.9</a> and <a href="probability-theory.html#thm:mgf-of-sum-of-multiple-ind-rvs">3.15</a>), we have</p>
<p><span class="math display">\[
M_{\sqrt{n}\left(\bar{X}_{n}-\mu\right)/\sigma}\left(t\right)=M_{\sum_{i=1}^{n}Y_{i}/\sqrt{n}}\left(t\right)=M_{\sum_{i=1}^{n}Y_{i}}\left(\frac{t}{\sqrt{n}}\right)=\left(M_{Y}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}.
\]</span></p>
<p>We now expand <span class="math inline">\(M_{Y}\left(t/\sqrt{n}\right)\)</span> in a Taylor series (power series) around 0. We have</p>
<p><span class="math display">\[
M_{Y}\left(\frac{t}{\sqrt{n}}\right)=\sum_{k=0}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{\left(t/\sqrt{n}\right)^{k}}{k!},
\]</span></p>
<p>where <span class="math inline">\(M_{Y}^{\left(k\right)}\left(0\right)=\left(\dif^{k}/\dif t^{k}\right)M_{Y}\left(t\right)\vert_{t=0}\)</span>. Since the mgfs exist for <span class="math inline">\(\left|t\right|&lt;h\)</span>, the power series expansion is valid if <span class="math inline">\(t&lt;\sqrt{n}\sigma h\)</span>.</p>
<p>We have</p>
<span class="math display">\[\begin{align*}
M_{Y}^{\left(0\right)} &amp; =\E\left[Y^{0}\right]=\E\left[1\right]=1, \\
M_{Y}^{\left(1\right)} &amp; =\E\left[Y^{1}\right]=\E\left[\frac{X-\mu}{\sigma}\right]=\frac{1}{\sigma}\left(\E\left[X\right]-\E\left[\mu\right]\right)=\frac{1}{\sigma}\left(\mu-\mu\right)=0,
\end{align*}\]</span>
<p>and, noting that</p>
<p><span class="math display">\[
\Var\left(X\right)=\E\left[X^{2}\right]-\left(\E\left[X\right]\right)^{2}\implies\sigma^{2}=\E\left[X^{2}\right]-\mu^{2}\implies\E\left[X^{2}\right]=\mu^{2}+\sigma^{2},
\]</span></p>
<p>we have</p>
<span class="math display">\[\begin{align*}
M_{Y}^{\left(2\right)} &amp; =\E\left[Y^{2}\right] \\
    &amp; =\E\left[\left(\frac{X-\mu}{\sigma}\right)^{2}\right] \\
    &amp; =\frac{1}{\sigma^{2}}\E\left[X^{2}-2\mu X+\mu^{2}\right] \\
    &amp; =\frac{1}{\sigma^{2}}\left(\E\left[X^{2}\right]-2\mu\E\left[X\right]+\E\left[\mu^{2}\right]\right) \\
    &amp; =\frac{1}{\sigma^{2}}\left(\mu^{2}+\sigma^{2}-2\mu^{2}+\mu^{2}\right) \\
    &amp; =\frac{1}{\sigma^{2}}\left(\sigma^{2}\right) \\
    &amp; =1.
\end{align*}\]</span>
<p>(By construction, the mean and variance of Y are 0 and 1, respectively.) Then, we have</p>
<span class="math display">\[\begin{align*}
M_{Y}\left(\frac{t}{\sqrt{n}}\right)    &amp; =\sum_{k=0}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\left(\frac{t/\sqrt{n}}{k!}\right)^{k} \\
    &amp; =1\frac{\left(t\sqrt{n}\right)^{0}}{0!}+0\frac{\left(t\sqrt{n}\right)}{1!}+1\frac{\left(t\sqrt{n}\right)^{2}}{2!}+\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{\left(t/\sqrt{n}\right)^{k}}{k!} \\
    &amp; =T_{2}\left(\frac{t}{\sqrt{n}}\right)+R_{2}\left(\frac{t}{\sqrt{n}}\right),
\end{align*}\]</span>
<p>where</p>
<p><span class="math display">\[
T_{2}\left(\frac{t}{\sqrt{n}}\right)=1+\frac{\left(t/\sqrt{n}\right)^{2}}{2!}\quad\text{and}\quad R_{2}\left(\frac{t}{\sqrt{n}}\right)=\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{\left(t/\sqrt{n}\right)^{k}}{k!}.
\]</span></p>
<p>We have <span class="math inline">\(n&gt;0\)</span>, so for fixed <span class="math inline">\(t\neq 0\)</span>, the quantity <span class="math inline">\(t/\sqrt{n}\rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow\infty\)</span>. Then, noting that <span class="math inline">\(M_{Y}^{\left(2\right)}\left(0\right)\)</span> exists, it follows from Theorem <a href="probability-theory.html#thm:taylor">3.25</a> that</p>
<p><span class="math display">\[
\lim_{t/\sqrt{n}\rightarrow0}\frac{M_{Y}\left(t/\sqrt{n}\right)-T_{2}\left(t/\sqrt{n}\right)}{\left(t/\sqrt{n}-0\right)^{2}}=0\implies\lim_{n\rightarrow\infty}\frac{R_{2}\left(t/\sqrt{n}\right)}{\left(t/\sqrt{n}\right)^{2}}=0.
\]</span></p>
<p>Since <span class="math inline">\(t\)</span> is fixed, we also have</p>
<p><span class="math display">\[
\lim_{n\rightarrow\infty}\frac{R_{2}\left(t/\sqrt{n}\right)}{\left(1/\sqrt{n}\right)^{2}}=\lim_{n\rightarrow\infty}nR_{2}\left(\frac{t}{\sqrt{n}}\right)=0,
\]</span></p>
<p>and this is also true at <span class="math inline">\(t=0\)</span> since</p>
<p><span class="math display">\[
R_{2}\left(\frac{0}{\sqrt{n}}\right)=R_{2}\left(0\right)=\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{0^{k}}{k!}=\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\cdot0=0.
\]</span></p>
<p>Thus, for any fixed <span class="math inline">\(t\)</span>, we can write</p>
<span class="math display">\[\begin{align*}
\lim_{n\rightarrow\infty}\left(M_{Y}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}   &amp; =\lim_{n\rightarrow\infty}\left[1+\frac{\left(t/\sqrt{n}\right)^{2}}{2}+R_{2}\left(\frac{t}{\sqrt{n}}\right)\right]^{n} \\
    &amp; =\lim_{n\rightarrow\infty}\left[1+\frac{t^{2}}{2n}+R_{2}\left(\frac{t}{\sqrt{n}}\right)\right]^{n} \\
    &amp; =\lim_{n\rightarrow\infty}\left[1+\frac{1}{n}\left(\frac{t^{2}}{2}+nR_{2}\left(\frac{t}{\sqrt{n}}\right)\right)\right]^{n}.
\end{align*}\]</span>
<p>Setting <span class="math inline">\(a_{n}=\left(t^{2}/2\right)+nR_{2}\left(t/\sqrt{n}\right)\)</span>, we have</p>
<p><span class="math display">\[
\lim_{n\rightarrow\infty}a_{n}=\lim_{n\rightarrow\infty}\left[\frac{t^{2}}{2}+nR_{2}\left(\frac{t}{\sqrt{n}}\right)\right]=\lim_{n\rightarrow\infty}\frac{t^{2}}{2}+\lim_{n\rightarrow\infty}nR_{2}\left(\frac{t}{\sqrt{n}}\right)=\frac{t^{2}}{2}+0=\frac{t^{2}}{2},
\]</span></p>
<p>i.e., the sequence <span class="math inline">\(a_{1},a_{2},\ldots\)</span> converges to <span class="math inline">\(a=t^{2}/2\)</span> as <span class="math inline">\(n\rightarrow\infty\)</span>. It follows from Lemma <a href="probability-theory.html#lem:limit-sequence-exponential">3.1</a> that</p>
<p><span class="math display">\[
\lim_{n\rightarrow\infty}\left(M_{Y}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}=\lim_{n\rightarrow\infty}\left[1+\frac{1}{n}\left(\frac{t^{2}}{2}+nR_{2}\left(\frac{t}{\sqrt{n}}\right)\right)\right]^{n}=\lim_{n\rightarrow\infty}\left[1+\frac{a_{n}}{n}\right]^{n}=\mathrm{e}^{a}=\mathrm{e}^{t^{2}/2}.
\]</span></p>
Since <span class="math inline">\(\mathrm{e}^{t^{2}/2}\)</span> is the mgf of the <span class="math inline">\(\mathcal{N}\left(0,1\right)\)</span> distribution, the theorem is proved.
</div>


<div class="definition">
<p><span id="def:taylor-polynomial" class="definition"><strong>Definition 3.9  </strong></span>If a function <span class="math inline">\(g\left(x\right)\)</span> has derivatives of order <span class="math inline">\(r\)</span>, that is, <span class="math inline">\(g^{\left(r\right)}\left(x\right)=\frac{\dif^{r}}{\dif x^{r}}g\left(x\right)\)</span> exists, then for any constant <span class="math inline">\(a\)</span>, the <em>Taylor polynomial of order <span class="math inline">\(r\)</span> about <span class="math inline">\(a\)</span></em> is</p>
<span class="math display">\[
T_{r}\left(x\right)=\sum_{i=0}^{r}\frac{g^{\left(i\right)}\left(a\right)}{i!}\left(x-a\right)^{i}.
\]</span>
</div>


<div class="theorem">
<p><span id="thm:taylor" class="theorem"><strong>Theorem 3.25  </strong></span>If</p>
<p><span class="math display">\[
g^{\left(r\right)}\left(a\right)=\frac{\dif^{r}}{\dif x^{r}}g\left(x\right)\Bigr\vert_{x=a}
\]</span></p>
<p>exists, then</p>
<p><span class="math display">\[
\lim_{x\rightarrow a}\frac{g\left(x\right)-T_{r}\left(x\right)}{\left(x-a\right)^{r}}=0.
\]</span></p>
(This is Theorem 5.5.21 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>).
</div>


</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-casella2002statistical">
<p>Casella, G., and R.L. Berger. 2002. <em>Statistical Inference</em>. Duxbury Advanced Series in Statistics and Decision Sciences. Thomson Learning. <a href="https://books.google.com/books?id=0x\_vAAAAMAAJ" class="uri">https://books.google.com/books?id=0x\_vAAAAMAAJ</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-algebra.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["course-notes.pdf", "course-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
