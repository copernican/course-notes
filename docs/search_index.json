[
["index.html", "A Master’s Companion Chapter 1 Introduction 1.1 Prerequisites", " A Master’s Companion Sean Wilson 2018-10-17 Chapter 1 Introduction This is a compilation of course notes, worked examples, and other supporting materials for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program. The courses currently covered include MATH-502 Deterministic Mathematical Models MATH-503 Mathematical Statistics MATH-504 Numerical Methods (in progress) MATH-640 Bayesian Statistics (in progress) Additional planned courses include MATH-611 Stochastic Simulation MATH-623 Sparse Representations and Random Sampling MATH-658 Survey Sampling 1.1 Prerequisites We assume knowledge of multivariable calculus and linear algebra, as well as a first course in probability. We introduce, but do not dwell on, a number of results from analysis and probability that are required for the material that follows. "],
["analysis.html", "Chapter 2 Analysis 2.1 Upper bounds and suprema", " Chapter 2 Analysis 2.1 Upper bounds and suprema This section is drawn from Lay (2005). Theorem 2.1 Let \\(x,y\\in\\mathbb{R}\\) such that \\(x\\leq y+\\epsilon\\) for every \\(\\epsilon&gt;0\\). Then \\(x\\leq y\\). Definition 2.1 Let \\(\\mathcal{S}\\) be a subset of \\(\\mathbb{R}\\). If there exists a real number \\(m\\) such that \\(m\\geq s\\) for all \\(s\\in \\mathcal{S}\\), then \\(m\\) is called an upper bound for \\(\\mathcal{S}\\), and we say that \\(\\mathcal{S}\\) is bounded above. If \\(m\\leq s\\) for all \\(s\\in \\mathcal{S}\\), then \\(m\\) is a lower bound for \\(\\mathcal{S}\\) and \\(\\mathcal{S}\\) is bounded below. The set \\(\\mathcal{S}\\) is said to be bounded if it is bounded above and bounded below. If an upper bound \\(m\\) for \\(\\mathcal{S}\\) is a member of \\(\\mathcal{S}\\), then \\(m\\) is called the maximum (or largest element) of \\(\\mathcal{S}\\), and we write \\(m=\\max \\mathcal{S}\\). Similarly, if a lower bound of \\(\\mathcal{S}\\) is a member of \\(\\mathcal{S}\\), then it is called the minimum (or least element) of \\(\\mathcal{S}\\), denoted by \\(\\min \\mathcal{S}\\). A set may have upper or lower bounds, or it may have neither. If \\(m\\) is an upper bound for \\(\\mathcal{S}\\), then any number greater than \\(m\\) is also an upper bound. While a set may have many upper and lower bounds, if it has a maximum or a minimum, then those values are unique. Thus we speak of an upper bound and the maximum. Definition 2.2 Let \\(\\mathcal{S}\\) be a nonempty subset of \\(\\mathbb{R}\\). If \\(\\mathcal{S}\\) is bounded above, then the least upper bound of \\(\\mathcal{S}\\) is called its supremum and is denoted by \\(\\sup \\mathcal{S}\\). Thus \\(m=\\sup \\mathcal{S}\\) iff \\(m\\geq s\\), for all \\(s\\in \\mathcal{S}\\), and if \\(m&#39;&lt;m\\), then there exists \\(s&#39;\\in \\mathcal{S}\\) such that \\(s&#39;&gt;m&#39;\\). If \\(\\mathcal{S}\\) is bounded below, then the greatest lower bound of \\(\\mathcal{S}\\) is called its infimum and is denoted by \\(\\inf \\mathcal{S}\\). Definition 2.3 (completeness axiom) Every nonempty subset \\(\\mathcal{S}\\) of \\(\\mathbb{R}\\) that is bounded above has a least upper bound. That is, \\(\\sup \\mathcal{S}\\) exists and is a real number. Theorem 2.2 Given nonempty subsets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) of \\(\\mathbb{R}\\), let \\(\\mathcal{C}\\) denote the set \\[ \\mathcal{C}=\\left\\{ x+y:x\\in \\mathcal{A}\\text{ and }y\\in \\mathcal{B}\\right\\} . \\] If \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) have suprema, then \\(\\mathcal{C}\\) has a supremum and \\(\\sup \\mathcal{C}=\\sup \\mathcal{A}+\\sup \\mathcal{B}\\). Proof. Let \\(\\sup \\mathcal{A}=a\\) and \\(\\sup \\mathcal{B}=b\\). If \\(z\\in \\mathcal{C}\\), then \\(z=x+y\\) for some \\(x\\in \\mathcal{A}\\) and \\(y\\in \\mathcal{B}\\). Thus \\(z=x+y\\leq a+b\\), so \\(a+b\\) is an upper bound for \\(\\mathcal{C}\\). By the completeness axiom, \\(\\mathcal{C}\\) has at least an upper bound, say \\(\\sup \\mathcal{C}=c\\). We must show that \\(c=a+b\\). Since \\(c\\) is the least upper bound for \\(\\mathcal{C}\\), we have \\(c\\leq a+b\\). To see that \\(a+b\\leq c\\), choose any \\(\\epsilon&gt;0\\). Since \\(a=\\sup \\mathcal{A}\\), \\(a-\\epsilon\\) is not an upper bound for \\(\\mathcal{A}\\), and there must exist \\(x\\in \\mathcal{A}\\) such that \\(a-\\epsilon&lt;x\\). Similarly, since \\(b=\\sup \\mathcal{B}\\), there exists \\(y\\in \\mathcal{B}\\) such that \\(b-\\epsilon&lt;y\\). Combining these inequalities, we have \\[ a+b-2\\epsilon&lt;x+y\\leq c. \\] That is, \\(a+b&lt;c+2\\epsilon\\) for every \\(\\epsilon&gt;0\\). Thus, by Theorem 2.1, \\(a+b\\leq c\\). Finally, since \\(c\\leq a+b\\) and \\(c\\geq a+b\\), we conclude that \\(c=a+b\\). References "],
["probability-theory.html", "Chapter 3 Probability theory 3.1 Background material 3.2 Transformations and expectations 3.3 Multiple random variables 3.4 Properties of a random sample", " Chapter 3 Probability theory 3.1 Background material Theorem 3.1 If \\(P\\) is a probability function, then \\(P\\left(A\\right)=\\sum_{i=1}^{\\infty}P\\left(A\\cap C_{i}\\right)\\) for any partition \\(C_{1},C_{2},\\ldots\\); \\(P\\left(\\cup_{i=1}^{\\infty}A_{i}\\right)\\leq\\sum_{i=1}^{\\infty}P\\left(A_{i}\\right)\\) for any sets \\(A_{1},A_{2},\\ldots\\). Theorem 3.2 (Bayes’ Rule) Let \\(A_{1},A_{2},\\ldots\\) be a partition of the sample space \\(\\Omega\\), and let \\(B\\) be any set. Then, for each \\(i=1,2,\\ldots\\), \\[ P\\left(A_{i}|B\\right)=\\frac{P\\left(B|A_{i}\\right)P\\left(A_{i}\\right)}{\\sum_{j=1}^{\\infty}P\\left(B|A_{j}\\right)P\\left(A_{j}\\right)}. \\] Proof. From the definition of conditional probability, we have \\[\\begin{align*} P\\left(A_{i}|B\\right) &amp; =\\frac{P\\left(A_{i}\\cap B\\right)}{P\\left(B\\right)} \\\\ &amp; =\\frac{P\\left(B|A_{i}\\right)P\\left(A_{i}\\right)}{P\\left(B\\right)}\\tag{conditional probability} \\\\ &amp; =\\frac{P\\left(B|A_{i}\\right)P\\left(A_{i}\\right)}{\\sum_{j=1}^{\\infty}P\\left(B\\cap A_{j}\\right)}\\tag{the $A_{j}$ partition $\\Omega$} \\\\ &amp; =\\frac{P\\left(B|A_{i}\\right)}{\\sum_{j=1}^{\\infty}P\\left(B|A_{j}\\right)P\\left(A_{j}\\right)}.\\tag{conditional probability} \\end{align*}\\] Definition 3.1 The cumulative distribution function or cdf of a random variable \\(X\\), denoted by \\(F_{X}\\left(x\\right)\\), is defined by \\(F_{X}\\left(x\\right)=P_{X}\\left(X\\leq x\\right)\\), for all \\(x\\). Theorem 3.3 The function \\(F\\left(x\\right)\\) is a cdf if and only if the following three conditions hold: \\(\\lim_{x\\rightarrow-\\infty}F\\left(x\\right)=0\\) and \\(\\lim_{x\\rightarrow\\infty}F\\left(x\\right)=1\\). \\(F\\left(x\\right)\\) is a nondecreasing function of \\(x\\). \\(F\\left(x\\right)\\) is right-continuous; that is, for every number \\(x_{0}\\), \\(\\lim_{x\\downarrow x_{0}}F\\left(x\\right)=F\\left(x_{0}\\right)\\). (This is Theorem 1.5.3 from Casella and Berger (2002).) Proof. PROOF GOES HERE 3.2 Transformations and expectations 3.2.1 Distributions of functions of a random variable When transformations are made, it is important to keep track of the sample spaces of the random variables; otherwise, much confusion can arise. When the transformation is from \\(X\\) to \\(Y=g\\left(X\\right)\\), it is most convenient to use \\[\\begin{equation} \\mathcal{X}=\\left\\{ x:f_{X}\\left(x\\right)&gt;0\\right\\} \\quad\\text{and}\\quad\\mathcal{Y}=\\left\\{ y:y=g\\left(x\\right)\\text{ for some }x\\in\\mathcal{X}\\right\\}. \\tag{3.1} \\end{equation}\\] Theorem 3.4 Let \\(X\\) have cdf \\(F_{X}\\left(x\\right)\\), let \\(Y=g\\left(X\\right)\\), and let \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) be defined as in Equation (3.1). If \\(g\\) is an increasing function on \\(\\mathcal{X}\\), \\(F_{Y}\\left(y\\right)=F_{X}\\left(g^{-1}\\left(y\\right)\\right)\\) for \\(y\\in\\mathcal{Y}\\). If \\(g\\) is a decreasing function on \\(\\mathcal{X}\\) and \\(X\\) is a continuous random variable, \\(F_{Y}\\left(y\\right)=1-F_{X}\\left(g^{-1}\\left(y\\right)\\right)\\) for \\(y\\in\\mathcal{Y}\\). (This is Theorem 2.1.3 from Casella and Berger (2002).) Proof. \\(g\\) is a monotone function, i.e., it maps each \\(x\\) to a single \\(y\\), and each \\(y\\) comes from at most one \\(x\\). In this case that \\(g\\) is increasing, we have \\[\\begin{align*} \\left\\{ x\\in\\mathcal{X}:g\\left(x\\right)\\leq y\\right\\} &amp; =\\left\\{ x\\in\\mathcal{X}:g^{-1}\\left(g\\left(x\\right)\\right)\\leq g^{-1}\\left(y\\right)\\right\\} \\\\ &amp; =\\left\\{ x\\in\\mathcal{X}:x\\leq g^{-1}\\left(y\\right)\\right\\}, \\end{align*}\\] and the cdf of \\(Y\\) is \\[\\begin{align*} F_{Y}\\left(y\\right) &amp; =P\\left(\\left\\{ Y\\leq y\\right\\} \\right) \\\\ &amp; =P\\left(\\left\\{ g\\left(X\\right)\\leq y\\right\\} \\right) \\\\ &amp; =P\\left(\\left\\{ x\\in\\mathcal{X}:g\\left(x\\right)\\leq y\\right\\} \\right) \\\\ &amp; =P\\left(\\left\\{ x\\in\\mathcal{X}:x\\leq g^{-1}\\left(y\\right)\\right\\} \\right) \\\\ &amp; =\\int_{-\\infty}^{g^{-1}\\left(y\\right)}f_{X}\\left(x\\right)\\dif x \\\\ &amp; =F_{X}\\left(g^{-1}\\left(y\\right)\\right). \\end{align*}\\] In the case that \\(g\\) is decreasing, we have \\[\\begin{align*} \\left\\{ x\\in\\mathcal{X}:g\\left(x\\right)\\leq y\\right\\} &amp; =\\left\\{ x\\in\\mathcal{X}:g^{-1}\\left(g\\left(x\\right)\\right)\\geq g^{-1}\\left(y\\right)\\right\\} \\\\ &amp; =\\left\\{ x\\in\\mathcal{X}:x\\geq g^{-1}\\left(y\\right)\\right\\}, \\end{align*}\\] and the cdf of \\(Y\\) is \\[\\begin{align*} F_{Y}\\left(y\\right) &amp; =P\\left(\\left\\{ Y\\leq y\\right\\} \\right) \\\\ &amp; =P\\left(\\left\\{ g\\left(X\\right)\\leq y\\right\\} \\right) \\\\ &amp; =P\\left(\\left\\{ x\\in\\mathcal{X}:g\\left(x\\right)\\leq y\\right\\} \\right) \\\\ &amp; =P\\left(\\left\\{ x\\in\\mathcal{X}:x\\geq g^{-1}\\left(y\\right)\\right\\} \\right) \\\\ &amp; =\\int_{g^{-1}\\left(y\\right)}^{\\infty}f_{X}\\left(x\\right)\\dif x \\\\ &amp; =1-F_{X}\\left(g^{-1}\\left(y\\right)\\right).\\tag{continuity of $X$} \\end{align*}\\] Theorem 3.5 Let \\(X\\) have pdf \\(f_{X}\\left(x\\right)\\) and let \\(Y=g\\left(X\\right)\\), where \\(g\\) is a monotone function. Let \\(\\mathcal{X}=\\left\\{ x:f_{X}\\left(x\\right)&gt;0\\right\\}\\) and let \\(\\mathcal{Y}=\\left\\{ y:y=g\\left(x\\right),x\\in\\mathcal{X}\\right\\}\\). Suppose that \\(f_{X}\\left(x\\right)\\) is continuous on \\(\\mathcal{X}\\) and that \\(g^{-1}\\left(y\\right)\\) has a continuous derivative on \\(\\mathcal{Y}\\). Then the pdf of \\(Y\\) is given by \\[ f_{Y}\\left(y\\right)= \\begin{cases} f_{X}\\left(g^{-1}\\left(y\\right)\\right)\\left|\\dfrac{\\dif}{\\dif y}g^{-1}\\left(y\\right)\\right|, &amp; y\\in\\mathcal{Y}\\\\ 0, &amp; \\text{otherwise} \\end{cases}. \\] (This is Theorem 2.1.5 from Casella and Berger (2002).) Proof. From Theorem 3.4 and applying the chain rule, we have \\[ f_{Y}\\left(y\\right)= \\dfrac{\\dif}{\\dif y}F_{Y}\\left(y\\right)= f_{X}\\left(g^{-1}\\left(y\\right)\\right)\\dfrac{\\dif}{\\dif y}g^{-1}\\left(y\\right) \\] in the case that g is increasing and \\[ f_{Y}\\left(y\\right)= \\dfrac{\\dif}{\\dif y}F_{Y}\\left(y\\right)= 0-f_{X}\\left(g^{-1}\\left(y\\right)\\right)\\dfrac{\\dif}{\\dif y}g^{-1}\\left(y\\right)= -f_{X}\\left(g^{-1}\\left(y\\right)\\right)\\dfrac{\\dif}{\\dif y}g^{-1}\\left(y\\right) \\] in the case that \\(g\\) is decreasing, which can be expressed concisely as in the theorem. We will look at \\(F_{X}^{-1}\\), the inverse of the cdf \\(F_{X}\\). If \\(F_{X}\\) is strictly increasing, then \\(F_{X}^{-1}\\) is well defined by \\[\\begin{equation} F_{X}^{-1}\\left(y\\right)=x\\implies F_{X}\\left(x\\right)=y. \\tag{3.2} \\end{equation}\\] However, if \\(F_{X}\\) is constant on some interval, then \\(F_{X}^{-1}\\) is not well defined by Equation (3.2). Any \\(x\\) satisfying \\(x_{1}\\leq x\\leq x_{2}\\) satisfies \\(F_{X}\\left(x\\right)=y\\). This problem is avoided by defining \\(F_{X}^{-1}\\left(y\\right)\\) for \\(0&lt;y&lt;1\\) by \\[ F_{X}^{-1}\\left(y\\right)=\\inf\\left\\{ x:F_{X}\\left(x\\right)\\geq y\\right\\}, \\] a definition that agrees with Equation (3.2) when \\(F_{X}\\) is nonconstant and provides an \\(F_{X}^{-1}\\) that is single-valued even when \\(F_{X}\\) is not strictly increasing. Using this definition, for some interval \\(\\left(x_{1},x_{2}\\right)\\) on which \\(F_{X}\\) is constant, we have \\(F_{X}^{-1}\\left(y\\right)=x_{1}\\). At the endpoints of the range of \\(y\\), \\(F_{X}^{-1}\\left(y\\right)\\) can also be defined. \\(F_{X}^{-1}\\left(1\\right)=\\infty\\) if \\(F_{X}\\left(x\\right)&lt;1\\) for all \\(x\\) and, for any \\(F_{X}\\), \\(F_{X}^{-1}\\left(0\\right)=-\\infty\\). Theorem 3.6 (Probability integral transformation) Let \\(X\\) have continuous cdf \\(F_{X}\\left(x\\right)\\) and define the random variable \\(Y\\) as \\(Y=F_{X}\\left(X\\right)\\). Then \\(Y\\) is uniformly distributed on \\(\\left(0,1\\right)\\), that is, \\(P\\left(\\left\\{ Y\\leq y\\right\\} \\right)=y\\), \\(0&lt;y&lt;1\\). (This is Theorem 2.1.10 from Casella and Berger (2002); the following proof is given there.) Proof. For \\(Y=F_{X}\\left(X\\right)\\) we have, for \\(0&lt;y&lt;1\\), \\[\\begin{align*} P\\left(\\left\\{ Y\\leq y\\right\\} \\right) &amp; =P\\left(\\left\\{ F_{X}\\left(X\\right)\\leq y\\right\\} \\right) \\\\ &amp; =P\\left(\\left\\{ F_{X}^{-1}\\left[F_{X}\\left(X\\right)\\right]\\leq F_{X}^{-1}\\left(y\\right)\\right\\} \\right)\\tag{$F_{X}^{-1}$ is increasing} \\\\ &amp; =P\\left(\\left\\{ X\\leq F_{X}^{-1}\\left(y\\right)\\right\\} \\right)\\tag{see paragraph below} \\\\ &amp; =F_{X}\\left(F_{X}^{-1}\\left(y\\right)\\right)\\tag{definition of $F_{X}$} \\\\ &amp; =y.\\tag{continuity of $F_{X}$} \\end{align*}\\] At the endpoints we have \\(P\\left(\\left\\{ Y\\leq y\\right\\} \\right)=1\\) for \\(y\\geq 1\\) and \\(P\\left(\\left\\{ Y\\leq y\\right\\} \\right)=0\\) for \\(y\\leq 0\\), showing that \\(Y\\) has a uniform distribution. The reasoning behind the equality \\[ P\\left(\\left\\{ F_{X}^{-1}\\left(F_{X}\\left(X\\right)\\right)\\leq F_{X}^{-1}\\left(y\\right)\\right\\} \\right)=P\\left(\\left\\{ X\\leq F_{X}^{-1}\\left(y\\right)\\right\\} \\right) \\] is somewhat subtle and deserves additional attention. If \\(F_{X}\\) is strictly increasing, then it is true that \\(F_{X}^{-1}\\left(F_{X}\\left(x\\right)\\right)=x\\). However, if \\(F_{X}\\) is flat, it may be that \\(F_{X}^{-1}\\left(F_{X}\\left(x\\right)\\right)\\neq x\\). Suppose \\(F_{X}\\) contains an interval \\(\\left(x_{1},x_{2}\\right)\\) on which \\(F_{X}\\) is constant, and let \\(x\\in\\left[x_{1},x_{2}\\right]\\). Then \\(F_{X}^{-1}\\left(F_{X}\\left(x\\right)\\right)=x_{1}\\) for any \\(x\\) in this interval. Even in this case, though, the probability equality holds, since \\(P\\left(\\left\\{ X\\leq x\\right\\} \\right)=P\\left(\\left\\{ X\\leq x_{1}\\right\\} \\right)\\) for any \\(x\\in\\left[x_{1},x_{2}\\right]\\). The flat cdf denotes a region of \\(0\\) probability \\((P\\left(\\left\\{ x_{1}&lt;X\\leq x\\right\\} \\right)=F_{X}\\left(x\\right)-F_{X}\\left(x_{1}\\right)=0)\\). 3.2.2 Expected values Theorem 3.7 Let \\(X\\) be a random variable and let \\(a\\), \\(b\\), and \\(c\\) be constants. Then for any functions \\(g_{1}\\left(x\\right)\\) and \\(g_{2}\\left(x\\right)\\) whose expectations exist, \\(\\E\\left[ag_{1}\\left(X\\right)+bg_{2}\\left(X\\right)+c\\right]=a\\E\\left[g_{1}\\left(X\\right)\\right]+b\\E\\left[g_{2}\\left(X\\right)\\right]+c\\). If \\(g_{1}\\left(x\\right)\\geq 0\\) for all \\(x\\), then \\(\\E\\left[g_{1}\\left(X\\right)\\right]\\geq 0\\). If \\(g_{1}\\left(x\\right)\\geq g_{2}\\left(x\\right)\\) for all \\(x\\), then \\(\\E\\left[g_{1}\\left(X\\right)\\right]\\geq\\E\\left[g_{2}\\left(X\\right)\\right]\\). If \\(a\\leq g_{1}\\left(x\\right)\\leq b\\) for all \\(x\\), then \\(a\\leq\\E\\left[g_{1}\\left(X\\right)\\right]\\leq b\\). (This is Theorem 2.2.5 from Casella and Berger (2002); the following proof is given there.) Proof. PROOF GOES HERE 3.2.3 Moments and moment generating functions Definition 3.2 For each integer \\(n\\), the \\(n\\text{th}\\) moment of \\(X\\) or \\((F_{X}\\left(x\\right))\\), \\(\\mu&#39;_{n}\\), is \\[ \\mu&#39;_{n}=\\E\\left[X^{n}\\right]. \\] The \\(n\\text{th}\\) central moment of \\(X\\), \\(\\mu_{n}\\), is \\[ \\mu_{n}=\\E\\left[\\left(X-\\mu\\right)^{n}\\right], \\] where \\(\\mu=\\mu&#39;_{1}=\\E\\left[X\\right]\\). Definition 3.3 Let \\(X\\) be a random variable with cdf \\(F_{X}\\). The moment generating function (mgf) of \\(X\\) (or \\(F_{X}\\)), denoted by \\(M_{X}\\left(t\\right)\\), is \\[ M_{X}\\left(t\\right)=\\E\\left[\\mathrm{e}^{tX}\\right], \\] provided that the expectation exists for \\(t\\) in some neighborhood of \\(0\\). That is, there is an \\(h&gt;0\\) such that, for all \\(t\\) in \\(-h&lt;t&lt;h\\), \\(\\E\\left[\\mathrm{e}^{tX}\\right]\\) exists. If the expectation does not exist in a neighborhood of \\(0\\), we say that the moment generating function does not exist. More explicitly, we can write the mgf of \\(X\\) as \\[ M_{X}\\left(t\\right)=\\int_{-\\infty}^{\\infty}\\mathrm{e}^{tx}f_{X}\\left(x\\right)\\dif x \\] if \\(X\\) is continuous, or \\[ M_{X}\\left(t\\right)=\\sum_{x}\\mathrm{e}^{tx}P\\left(\\left\\{ X=x\\right\\} \\right) \\] if \\(X\\) is discrete. Theorem 3.8 If \\(X\\) has mgf \\(M_{X}\\left(t\\right)\\), then \\[ \\E\\left[X^{n}\\right]=M_{X}^{\\left(n\\right)}\\left(0\\right), \\] where we define \\[ M_{X}^{\\left(n\\right)}\\left(0\\right)=\\frac{\\dif^{n}}{\\dif t^{n}}M_{X}\\left(t\\right)\\Bigr\\vert_{t=0}. \\] That is, the \\(n\\text{th}\\) moment is equal to the \\(n\\text{th}\\) derivative of \\(M_{X}\\left(t\\right)\\) evaluated at \\(t=0\\). (This is Theorem 2.3.7 from Casella and Berger (2002); the following proof is given there.) Proof. Assuming that we can differentiate under the integral sign, we have \\[\\begin{align*} \\frac{\\dif}{\\dif t}M_{X}\\left(t\\right) &amp; =\\frac{\\dif}{\\dif t}\\int_{-\\infty}^{\\infty}\\mathrm{e}^{tx}f_{X}\\left(x\\right)\\dif x \\\\ &amp; =\\int_{-\\infty}^{\\infty}\\left(\\frac{\\dif}{\\dif t}\\mathrm{e}^{tx}\\right)f_{X}\\left(x\\right)\\dif x \\\\ &amp; =\\int_{-\\infty}^{\\infty}\\left(x\\mathrm{e}^{tx}\\right)f_{X}\\left(x\\right)\\dif x \\\\ &amp; =\\E\\left[X\\mathrm{e}^{tX}\\right]. \\end{align*}\\] Thus, \\[ \\frac{\\dif}{\\dif t}M_{X}\\left(t\\right)\\Bigr\\vert_{t=0}=\\E\\left[X\\mathrm{e}^{tX}\\right]\\Big\\vert_{t=0}=\\E\\left[X\\mathrm{e}^{0}\\right]=\\E\\left[X\\right]. \\] Noting that \\[ \\frac{\\dif^{n}}{\\dif t^{n}}\\mathrm{e}^{tx}=\\frac{\\dif^{n-1}}{\\dif t^{n-1}}\\left[\\frac{\\dif}{\\dif t}\\mathrm{e}^{tx}\\right]=\\frac{\\dif^{n-2}}{\\dif t^{n-2}}\\left[\\frac{\\dif}{\\dif t}x\\mathrm{e}^{tx}\\right]=\\frac{\\dif^{n-2}}{\\dif t^{n-2}}x^{2}\\mathrm{e}^{tx}=\\frac{\\dif}{\\dif t}x^{n-1}\\mathrm{e}^{tx}=x^{n}\\mathrm{e}^{tx}, \\] we can establish that \\[\\begin{align*} \\frac{\\dif^{n}}{\\dif t^{n}}M_{X}\\left(t\\right)\\Bigr\\vert_{t=0} &amp; =\\frac{\\dif^{n}}{\\dif t^{n}}\\int_{-\\infty}^{\\infty}\\mathrm{e}^{tx}f_{X}\\left(x\\right)\\dif x\\Big\\vert_{t=0} \\\\ &amp; =\\int_{-\\infty}^{\\infty}\\left(\\frac{\\dif^{n}}{\\dif t^{n}}\\mathrm{e}^{tx}\\right)f_{X}\\left(x\\right)\\dif x\\Big\\vert_{t=0} \\\\ &amp; =\\int_{-\\infty}^{\\infty}\\left(x^{n}\\mathrm{e}^{tx}\\right)f_{X}\\left(x\\right)\\dif x\\Big\\vert_{t=0} \\\\ &amp; =\\E\\left[X^{n}\\mathrm{e}^{tX}\\right]\\Big\\vert_{t=0} \\\\ &amp; =\\E\\left[X^{n}\\right]. \\end{align*}\\] Lemma 3.1 Let \\(a_{1},a_{2},\\ldots\\) be a sequence of numbers converging to \\(a\\), that is, \\(\\lim_{n\\rightarrow\\infty}a_{n}=a\\). Then \\[ \\lim_{n\\rightarrow\\infty}\\left(1+\\frac{a_{n}}{n}\\right)^{n}=\\mathrm{e}^{a}. \\] (This is Lemma 2.3.14 from Casella and Berger (2002).) Theorem 3.9 For any constants \\(a\\) and \\(b\\), the mgf of the random variable \\(aX+b\\) is given by \\[ M_{aX+b}\\left(t\\right)=\\mathrm{e}^{bt}M_{X}\\left(at\\right). \\] (This is Theorem 2.3.15 from Casella and Berger (2002).; the following proof is given there.) Proof. By definition, \\[ M_{aX+b}\\left(t\\right)= \\E\\left[\\mathrm{e}^{\\left(aX+b\\right)t}\\right]= \\E\\left[\\mathrm{e}^{\\left(aX\\right)t}\\mathrm{e}^{bt}\\right]= \\mathrm{e}^{bt}\\E\\left[\\mathrm{e}^{\\left(at\\right)X}\\right]= \\mathrm{e}^{bt}M_{X}\\left(at\\right), \\] proving the theorem. 3.3 Multiple random variables 3.3.1 Conditional distributions and independence Definition 3.4 Let \\(\\left(X,Y\\right)\\) be a bivariate random vector with joint pdf or pmf \\(f\\left(x,y\\right)\\) and marginal pdfs or pmfs \\(f_{X}\\left(x\\right)\\) and \\(f_{Y}\\left(y\\right)\\). Then \\(X\\) and \\(Y\\) are called independent random variables if, for every \\(x\\in\\mathbb{R}\\) and \\(y\\in\\mathbb{R}\\), \\(f\\left(x,y\\right)=f_{X}\\left(x\\right)f_{Y}\\left(y\\right)\\). Theorem 3.10 Let \\(X\\) and \\(Y\\) be independent random variables. For any \\(\\mathcal{A}\\subset\\mathbb{R}\\) and \\(\\mathcal{B}\\subset\\mathbb{R}\\), \\(P\\left(\\left\\{ X\\in \\mathcal{A}\\right\\} \\cap\\left\\{ Y\\in \\mathcal{B}\\right\\} \\right)=P\\left(\\left\\{ X\\in \\mathcal{A}\\right\\} \\right)P\\left(\\left\\{ Y\\in \\mathcal{B}\\right\\} \\right)\\); that is, the events \\(\\left\\{ X\\in \\mathcal{A}\\right\\}\\) and \\(\\left\\{ Y\\in \\mathcal{B}\\right\\}\\) are independent events. Let \\(g\\left(x\\right)\\) be a function only of \\(x\\) and \\(h\\left(y\\right)\\) be a function only of \\(y\\). Then \\[ \\E\\left[g\\left(X\\right)h\\left(Y\\right)\\right]=\\E\\left[g\\left(X\\right)\\right]\\E\\left[h\\left(Y\\right)\\right]. \\] (This is Theorem 4.2.10 from Casella and Berger (2002); the following proof is given there.) Proof. For continuous random variables, part (2) is proved by noting that \\[\\begin{align*} \\E\\left[g\\left(X\\right)h\\left(Y\\right)\\right] &amp; =\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}g\\left(x\\right)h\\left(y\\right)f\\left(x,y\\right)\\dif x\\dif y \\\\ &amp; =\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}g\\left(x\\right)h\\left(y\\right)f_{X}\\left(x\\right)f_{Y}\\left(y\\right)\\dif x\\dif y\\tag{independence} \\\\ &amp; =\\int_{-\\infty}^{\\infty}h\\left(y\\right)f_{Y}\\left(y\\right)\\int_{-\\infty}^{\\infty}g\\left(x\\right)f_{X}\\left(x\\right)\\dif x\\dif y \\\\ &amp; =\\left(\\int_{-\\infty}^{\\infty}g\\left(x\\right)f_{X}\\left(x\\right)\\dif x\\right)\\left(\\int_{-\\infty}^{\\infty}h\\left(y\\right)f_{Y}\\left(y\\right)\\dif y\\right) \\\\ &amp; =\\E\\left[g\\left(X\\right)\\right]\\E\\left[h\\left(Y\\right)\\right]. \\end{align*}\\] The result for discrete random variables is proved by replacing integrals by sums. Part (1) can be proved by series of steps similar to those above or by the following argument. Let \\(g\\left(x\\right)\\) be the indicator function of the set \\(\\mathcal{A}\\). Let \\(h\\left(y\\right)\\) be the indicator function of the set \\(\\mathcal{B}\\). Note that \\(g\\left(x\\right)h\\left(y\\right)\\) is the indicator function of the set \\(\\mathcal{C}\\subset\\mathbb{R}^{2}\\) defined by \\(\\mathcal{C}=\\left\\{ \\left(x,y\\right):x\\in \\mathcal{A},y\\in \\mathcal{B}\\right\\}\\). Thus using the expectation equality just proved, we have \\[\\begin{align*} P\\left(\\left\\{ X\\in A\\right\\} \\cap\\left\\{ Y\\in B\\right\\} \\right) &amp; =P\\left(\\left\\{ \\left(X,Y\\right)\\in C\\right\\} \\right) \\\\ &amp; =\\E\\left[g\\left(X\\right)h\\left(Y\\right)\\right] \\\\ &amp; =\\E\\left[g\\left(X\\right)\\right]\\E\\left[h\\left(Y\\right)\\right] \\\\ &amp; =P\\left(\\left\\{ X\\in A\\right\\} \\right)P\\left(\\left\\{ Y\\in B\\right\\} \\right). \\end{align*}\\] Theorem 3.11 Let \\(X\\) and \\(Y\\) be independent random variables with moment generating functions \\(M_{X}\\left(t\\right)\\) and \\(M_{Y}\\left(t\\right)\\). Then the moment generating function of the random variable \\(Z=X+Y\\) is given by \\(M_{Z}\\left(t\\right)=M_{X}\\left(t\\right)M_{Y}\\left(t\\right)\\). (This is Theorem 4.2.12 from Casella and Berger (2002); the following proof is given there.) Proof. Using the definition of the mgf and Theorem 3.10, we have \\[ M_{Z}\\left(t\\right)= \\E\\left[\\mathrm{e}^{tZ}\\right]= \\E\\left[\\mathrm{e}^{t\\left(X+Y\\right)}\\right]= \\E\\left[\\mathrm{e}^{tX}\\mathrm{e}^{tY}\\right]= \\E\\left[\\mathrm{e}^{tX}\\right]\\E\\left[\\mathrm{e}^{tY}\\right]= M_{X}\\left(t\\right)M_{Y}\\left(t\\right). \\] 3.3.2 Covariance and correlation Definition 3.5 The covariance of \\(X\\) and \\(Y\\) is the number defined by \\[ \\Cov\\left(X,Y\\right)=\\E\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right]. \\] Definition 3.6 The correlation of \\(X\\) and \\(Y\\) is the number defined by \\[ \\rho_{XY}=\\frac{\\Cov\\left(X,Y\\right)}{\\sigma_{X}\\sigma_{Y}}. \\] The value \\(\\rho_{XY}\\) is also called the correlation coefficient. Theorem 3.12 If \\(X\\) and \\(Y\\) are independent random variables, then \\(\\Cov\\left(X,Y\\right)=0\\) and \\(\\rho_{XY}=0\\). (This is Theorem 4.5.5 from Casella and Berger (2002); the following proof is given there.) Proof. Since \\(X\\) and \\(Y\\) are independent, we have \\(\\E\\left[XY\\right]=\\E\\left[X\\right]\\E\\left[Y\\right]\\). Thus \\[\\begin{align*} \\E\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right] &amp; =\\E\\left[XY-\\mu_{Y}X-\\mu_{X}Y+\\mu_{X}\\mu_{Y}\\right] \\\\ &amp; =\\E\\left[XY\\right]-\\mu_{Y}\\E\\left[X\\right]-\\mu_{X}\\E\\left[Y\\right]+\\mu_{X}\\mu_{Y} \\\\ &amp; =\\E\\left[X\\right]\\E\\left[Y\\right]-\\E\\left[Y\\right]\\E\\left[X\\right]-\\E\\left[X\\right]\\E\\left[Y\\right]+\\E\\left[X\\right]\\E\\left[Y\\right] \\\\ &amp; =0 \\end{align*}\\] and \\[ \\rho_{XY}=\\frac{\\Cov\\left(X,Y\\right)}{\\sigma_{X}\\sigma_{Y}}=\\frac{0}{\\sigma_{X}\\sigma_{Y}}=0. \\] Theorem 3.13 If \\(X\\) and \\(Y\\) are any two random variables and \\(a\\) and \\(b\\) are any two constants, then \\[ \\Var\\left(aX+bY\\right)=a^{2}\\Var\\left(X\\right)+b^{2}\\Var\\left(Y\\right)+2ab\\Cov\\left(X,Y\\right). \\] If \\(X\\) and \\(Y\\) are independent random variables, then \\[ \\Var\\left(aX+bY\\right)=a^{2}\\Var\\left(X\\right)+b^{2}\\Var\\left(Y\\right). \\] (This is Theorem 4.5.6 from Casella and Berger (2002); the following proof is given there.) Proof. We have \\[\\begin{align*} \\Var\\left(aX+bY\\right) &amp; =\\E\\left[\\left(\\left(aX+bY\\right)-\\E\\left[aX+bY\\right]\\right)^{2}\\right] \\\\ &amp; =\\E\\left[\\left(aX+bY-a\\E\\left[X\\right]-b\\E\\left[Y\\right]\\right)^{2}\\right] \\\\ &amp; =\\E\\left[\\left(aX+bY-a\\mu_{X}-b\\mu_{Y}\\right)^{2}\\right] \\\\ &amp; =\\E\\left[\\left(a\\left(X-\\mu_{X}\\right)+b\\left(Y-\\mu_{Y}\\right)\\right)^{2}\\right] \\\\ &amp; =\\E\\left[\\left(a\\left(X-\\mu_{X}\\right)\\right)^{2}-2ab\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)+\\left(b\\left(Y-\\mu_{Y}\\right)\\right)^{2}\\right] \\\\ &amp; =\\E\\left[a^{2}\\left(X-\\mu_{X}\\right)^{2}\\right]-\\E\\left[2ab\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right]+\\E\\left[b^{2}\\left(Y-\\mu_{Y}\\right)^{2}\\right] \\\\ &amp; =a^{2}\\E\\left[\\left(X-\\mu_{X}\\right)^{2}\\right]-2ab\\E\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right]+b^{2}\\E\\left[\\left(Y-\\mu_{Y}\\right)^{2}\\right] \\\\ &amp; =a^{2}\\Var\\left(X\\right)+b^{2}\\Var\\left(Y\\right)-2ab\\Cov\\left(X,Y\\right). \\end{align*}\\] If \\(X\\) and \\(Y\\) are independent, it follows from Theorem 3.12 that \\(\\Cov\\left(X,Y\\right)=0\\) and the second equality is immediate from the first. Theorem 3.14 For any random variables \\(X\\) and \\(Y\\), \\(-1\\leq\\rho_{XY}\\leq 1\\). \\(\\left|\\rho_{XY}\\right|=1\\) if and only if there exist numbers \\(a\\neq 0\\) and \\(b\\) such that \\(P\\left(\\left\\{ Y=aX+b\\right\\} \\right)=1\\). If \\(\\rho_{XY}=1\\), then \\(a&gt;0\\), and if \\(\\rho_{XY}=-1\\), then \\(a&lt;0\\). (This is Theorem 4.5.7 from Casella and Berger (2002); the following proof is given there.) Proof. PROOF GOES HERE 3.3.3 Multivariate distributions Theorem 3.15 Let \\(X_{1},\\ldots,X_{n}\\) be mutually independent random variables with mgfs \\(M_{X_{1}}\\left(t\\right),\\ldots M_{X_{n}}\\left(t\\right)\\). Let \\(Z=X_{1}+\\cdots+X_{n}\\). Then the mgf of \\(Z\\) is \\[ M_{Z}\\left(t\\right)=M_{X_{1}}\\left(t\\right)\\cdot\\cdots\\cdot M_{X_{n}}\\left(t\\right). \\] In particular, if \\(X_{1},\\ldots,X_{n}\\) all have the same distribution with mgf \\(M_{X}\\left(t\\right)\\), then \\(M_{Z}\\left(t\\right)=\\left(M_{X}\\left(t\\right)\\right)^{n}\\). (This is Theorem 4.6.7 from Casella and Berger (2002), which is a generalization of Theorem 3.11). 3.3.4 Inequalities Lemma 3.2 (Young’s Inequality) Let \\(a\\) and \\(b\\) be any positive numbers, and let \\(p\\) and \\(q\\) be any positive numbers (necessarily greater than 1) satisfying \\[ \\frac{1}{p}+\\frac{1}{q}=1. \\] Then \\[ \\frac{1}{p}a^{p}+\\frac{1}{q}b^{q}\\geq ab \\] with equality if and only if \\(a^{p}=b^{q}\\). (This is Lemma 4.7.1 from Casella and Berger (2002); the following proof is given there). Proof. Fix \\(b\\), and consider the function \\[ g\\left(a\\right)=\\frac{1}{p}a^{p}+\\frac{1}{q}b^{q}-ab. \\] To minimize \\(g\\left(a\\right)\\), differentiate and set equal to \\(0\\): \\[ \\frac{\\dif}{\\dif a}g\\left(a\\right)=0\\implies a^{p-1}-b=0\\implies b=a^{p-1}. \\] We will evaluate the second derivative of \\(g\\) with respect to \\(a\\) at \\[ b=a^{p-1}\\implies b^{1/\\left(p-1\\right)}=\\left(a^{p-1}\\right)^{1/\\left(p-1\\right)}\\implies a=b^{1/\\left(p-1\\right)} \\] to verify that this is a minimum. \\[\\begin{align*} \\frac{\\dif^{2}}{\\dif a^{2}}\\left[g\\left(a\\right)\\right|_{a=b^{1/\\left(p-1\\right)}} &amp; =\\frac{\\dif}{\\dif a}\\left[a^{p-1}-b\\right|_{a=b^{1/\\left(p-1\\right)}} \\\\ &amp; =\\left[\\left(p-1\\right)a^{p-2}\\right|_{a=b^{1/\\left(p-1\\right)}} \\\\ &amp; =\\left(p-1\\right)\\left(b^{1/\\left(p-1\\right)}\\right)^{p-2} \\\\ &amp; =\\left(p-1\\right)b^{\\left(p-2\\right)/\\left(p-1\\right)} \\\\ &amp; =\\left(p-1\\right)b^{\\left(p-1-1\\right)/\\left(p-1\\right)} \\\\ &amp; =\\left(p-1\\right)b^{\\left[\\left(p-1\\right)/\\left(p-1\\right)\\right]-1/\\left(p-1\\right)} \\\\ &amp; =\\left(p-1\\right)b^{1-1/\\left(p-1\\right)} \\end{align*}\\] We have \\(p&gt;1\\) and \\(b&gt;0\\), so that \\(p-1&gt;0\\) and \\(b^{1-1/\\left(p-1\\right)}&gt;0\\). It follows that \\(b=a^{p-1}\\) is a minimum. We have \\[ \\frac{1}{p}+\\frac{1}{q}=1\\implies\\frac{1}{q}=1-\\frac{1}{p}=\\frac{p-1}{p}\\implies q\\left(p-1\\right)=p, \\] so that the value of \\(g\\left(a\\right)\\) at the minimum is \\[ \\frac{1}{p}a^{p}+\\frac{1}{q}\\left(a^{p-1}\\right)^{q}-aa^{p-1}=\\frac{1}{p}a^{p}+\\frac{1}{q}a^{p}-a^{p}=a^{p}\\left(\\frac{1}{p}+\\frac{1}{q}-1\\right)=a^{p}\\left(1-1\\right)=0. \\] Hence the minimum is \\(0\\) and the inequality is established. The domain of \\(g\\) is \\(\\left\\{ a:0&lt;a&lt;\\infty\\right\\}\\) and we have \\(p&gt;1\\), so that for some fixed \\(b\\), \\(g&#39;\\left(a\\right)=a^{p-1}-b\\) is increasing in \\(a\\). Thus, the minimum we found is unique, so that equality holds only if \\(a^{p-1}=b\\), which is equivalent to \\[ a^{p-1}=b\\implies a^{p/q}=b\\implies\\left(a^{p/q}\\right)^{q}=b^{q}\\implies a^{p}=b^{q}. \\] Theorem 3.16 (Hölder’s Inequality) Let \\(X\\) and \\(Y\\) be any two random variables, and let \\(p\\) and \\(q\\) satisfy Lemma 3.2. Then \\[ \\left|\\E\\left[XY\\right]\\right|\\leq \\E\\left[\\left|XY\\right|\\right]\\leq \\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}. \\] (This is Theorem 4.7.2 from Casella and Berger (2002); the following proof is given there.) Proof. The first inequality follows from \\(-\\left|XY\\right|\\leq XY\\leq\\left|XY\\right|\\) and Theorem 3.7. To prove the second inequality, define \\[ a=\\frac{\\left|X\\right|}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}}\\quad\\text{and}\\quad b=\\frac{\\left|Y\\right|}{\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}}. \\] Applying Lemma 3.2, we get \\[\\begin{align*} \\frac{1}{p}a^{p}+\\frac{1}{q}b^{q} &amp; \\geq ab \\\\ \\implies\\frac{1}{p}\\left(\\frac{\\left|X\\right|}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}}\\right)^{p}+\\frac{1}{q}\\left(\\frac{\\left|Y\\right|}{\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}}\\right)^{q} &amp; \\geq\\frac{\\left|X\\right|\\left|Y\\right|}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}} \\\\ \\implies\\frac{1}{p}\\frac{\\left|X\\right|^{p}}{\\E\\left[\\left|X\\right|^{p}\\right]}+\\frac{1}{q}\\frac{\\left|Y\\right|^{q}}{\\E\\left[\\left|Y\\right|^{q}\\right]} &amp; \\geq\\frac{\\left|XY\\right|}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}}. \\end{align*}\\] Taking the expectation of both sides gives \\[\\begin{align*} \\E\\left[\\frac{1}{p}\\frac{\\left|X\\right|^{p}}{\\E\\left[\\left|X\\right|^{p}\\right]}+\\frac{1}{q}\\frac{\\left|Y\\right|^{q}}{\\E\\left[\\left|Y\\right|^{q}\\right]}\\right] &amp; \\geq\\E\\left[\\frac{\\left|XY\\right|}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}}\\right] \\\\ \\implies\\frac{1}{p\\E\\left[\\left|X\\right|^{p}\\right]}\\E\\left[\\left|X\\right|^{p}\\right]+\\frac{1}{q\\E\\left[\\left|Y\\right|^{q}\\right]}\\E\\left[\\left|Y\\right|^{q}\\right] &amp; \\geq\\E\\left[\\frac{\\left|XY\\right|}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}}\\right] \\\\ \\implies\\frac{1}{p}+\\frac{1}{q} &amp; \\geq\\frac{1}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}}\\E\\left[\\left|XY\\right|\\right] \\\\ \\implies1 &amp; \\geq\\frac{1}{\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}}\\E\\left[\\left|XY\\right|\\right] \\\\ \\implies\\E\\left[\\left|XY\\right|\\right] &amp; \\leq\\left(\\E\\left[\\left|X\\right|^{p}\\right]\\right)^{1/p}\\left(\\E\\left[\\left|Y\\right|^{q}\\right]\\right)^{1/q}. \\end{align*}\\] Perhaps the most famous special case of Hölder’s Inequality is that for which \\(p=q=2\\). Theorem 3.17 (Cauchy-Schwarz Inequality) For any two random variables \\(X\\) and \\(Y\\), \\[ \\left|\\E\\left[XY\\right]\\right|\\leq\\E\\left[\\left|XY\\right|\\right]\\leq\\left(\\E\\left[\\left|X\\right|^{2}\\right]\\right)^{1/2}\\left(\\E\\left[\\left|Y\\right|^{2}\\right]\\right)^{1/2}. \\] (This is Theorem 4.7.3 from Casella and Berger (2002).) Definition 3.7 A function \\(g\\left(x\\right)\\) is convex if \\(g\\left(\\lambda x+\\left(1-\\lambda\\right)y\\right)\\leq\\lambda g\\left(x\\right)+\\left(1-\\lambda\\right)g\\left(y\\right)\\), for all \\(x\\) and \\(y\\), and \\(0&lt;\\lambda&lt;1\\). The function \\(g\\left(x\\right)\\) is concave if \\(-g\\left(x\\right)\\) is convex. Theorem 3.18 (Jensen’s Inequality) For any random variable \\(X\\), if \\(g\\left(x\\right)\\) is a convex function, then \\[ \\E\\left[g\\left(X\\right)\\right]\\geq g\\left(\\E\\left[X\\right]\\right). \\] Equality holds if and only if, for every line \\(a+bx\\) that is tangent to \\(g\\left(x\\right)\\) at \\(x=\\E\\left[X\\right]\\), \\(P\\left(g\\left(X\\right)=a+bX\\right)=1\\). 3.4 Properties of a random sample 3.4.1 Sums of random variables from a random sample Definition 3.8 The sample variance is the statistic defined by \\[ S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}. \\] The sample standard deviation is the statistic defined by \\(S=\\sqrt{S^{2}}\\). Theorem 3.19 Let \\(x_{1},\\ldots,x_{n}\\) be any numbers and \\(\\bar{x}=\\left(x_{1}+\\cdots+x_{n}\\right)/n\\). Then \\(\\min_{a}\\sum_{i=1}^{n}\\left(x_{i}-a\\right)^{2}=\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\), \\(\\left(n-1\\right)s^{2}=\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}=\\sum_{i=1}^{n}x_{i}^{2}-n\\bar{x}^{2}\\). (This is Theorem 5.2.4 from Casella and Berger (2002).; the following proof is given there.) Proof. To prove part (1), add and subtract \\(\\bar{x}\\) to get \\[\\begin{align*} \\sum_{i=1}^{n}\\left(x_{i}-a\\right)^{2} &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}+\\bar{x}-a\\right)^{2} \\\\ &amp; =\\sum_{i=1}^{n}\\left[\\left(x_{i}-\\bar{x}\\right)+\\left(\\bar{x}-a\\right)\\right]\\left[\\left(x_{i}-\\bar{x}\\right)+\\left(\\bar{x}-a\\right)\\right] \\\\ &amp; =\\sum_{i=1}^{n}\\left[\\left(x_{i}-\\bar{x}\\right)^{2}+\\left(x_{i}-\\bar{x}\\right)\\left(\\bar{x}-a\\right)+\\left(\\bar{x}-a\\right)\\left(x_{i}-\\bar{x}\\right)+\\left(\\bar{x}-a\\right)^{2}\\right] \\\\ &amp; =\\sum_{i=1}^{n}\\left[\\left(x_{i}-\\bar{x}\\right)^{2}+2\\left(x_{i}-\\bar{x}\\right)\\left(\\bar{x}-a\\right)+\\left(\\bar{x}-a\\right)^{2}\\right] \\\\ &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+\\sum_{i=1}^{n}\\left[2\\left(x_{i}-\\bar{x}\\right)\\left(\\bar{x}-a\\right)\\right]+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2} \\\\ &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+2\\sum_{i=1}^{n}\\left(x_{i}\\bar{x}-ax_{i}-\\bar{x}^{2}+a\\bar{x}\\right)+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2} \\\\ &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+2\\left(\\bar{x}\\sum_{i=1}^{n}x_{i}-a\\sum_{i=1}^{n}x_{i}-\\sum_{i=1}^{n}\\bar{x}^{2}+\\sum_{i=1}^{n}a\\bar{x}\\right)+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2} \\\\ &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+2\\left(\\bar{x}\\left(n\\bar{x}\\right)-a\\left(n\\bar{x}\\right)-n\\bar{x}^{2}+na\\bar{x}\\right)+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2} \\\\ &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+2\\left(n\\bar{x}^{2}-na\\bar{x}-n\\bar{x}^{2}+na\\bar{x}\\right)+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2} \\\\ &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+2\\left(0\\right)+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2} \\\\ &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2}. \\end{align*}\\] It is now clear that the right-hand side is minimized at \\(a=\\bar{x}\\). To prove part (2), take \\(a=0\\) in the above, i.e., \\[\\begin{align*} \\sum_{i=1}^{n}\\left(x_{i}-a\\right)^{2} &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+\\sum_{i=1}^{n}\\left(\\bar{x}-a\\right)^{2} \\\\ \\implies\\sum_{i=1}^{n}\\left(x_{i}-0\\right)^{2} &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+\\sum_{i=1}^{n}\\left(\\bar{x}-0\\right)^{2} \\\\ \\implies\\sum_{i=1}^{n}x_{i}^{2} &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+\\sum_{i=1}^{n}\\bar{x}^{2} \\\\ \\implies\\sum_{i=1}^{n}x_{i}^{2}-\\sum_{i=1}^{n}\\bar{x}^{2} &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2} \\\\ \\implies\\sum_{i=1}^{n}x_{i}^{2}-n\\bar{x}^{2} &amp; =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}. \\end{align*}\\] The sample variance is defined as \\[ s^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\implies\\left(n-1\\right)s^{2}=\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}, \\] so the final equality of part (2) holds. 3.4.2 Sampling from the normal distribution Theorem 3.20 Let \\(X_{1},\\ldots,X_{n}\\) be a random sample from a \\(\\mathcal{N}\\left(\\mu,\\sigma^{2}\\right)\\) distribution, and let \\(\\bar{X}=\\left(1/n\\right)\\sum_{i=1}^{n}X_{i}\\) and \\(S^{2}=\\left[1/\\left(n-1\\right)\\right]\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}\\). Then \\(\\bar{X}\\) and \\(S^{2}\\) are independent random variables, \\(\\bar{X}\\) has a \\(\\mathcal{N}\\left(\\mu,\\sigma^{2}/n\\right)\\) distribution, \\(\\left(n-1\\right)S^{2}/\\sigma^{2}\\) has a chi-squared distribution with \\(n-1\\) degrees of freedom. (This is Theorem 5.3.1 from Casella and Berger (2002).; the following proof is given there.) Proof. PROOF GOES HERE Definition 3.9 Let \\(X_{1},\\ldots,X_{n}\\) be a random sample from a \\(\\mathcal{N}\\left(\\mu,\\sigma^{2}\\right)\\) distribution. The quantity \\(\\left(\\bar{X}-\\mu\\right)/\\left(S/\\sqrt{n}\\right)\\) has Student’s \\(t\\) distribution with \\(n-1\\) degrees of freedom. Equivalently, a random variable \\(T\\) has Student’s \\(t\\) distribution with \\(p\\) degrees of freedom, and we write \\(T\\sim t_{p}\\) if it has pdf \\[ f_{T}\\left(t\\right)=\\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)}\\frac{1}{\\left(p\\pi\\right)^{1/2}}\\frac{1}{\\left(1+t^{2}/p\\right)^{\\left(p+1\\right)/2}},\\quad-\\infty&lt;t&lt;\\infty. \\] 3.4.3 Order statistics The order statistics of a random sample \\(X_{1},\\ldots,X_{n}\\) are the sample values placed in ascending order. They are denoted by \\(X_{\\left(1\\right)},\\ldots,X_{\\left(n\\right)}\\). The order statistics are random variables that satisfy \\(X_{\\left(1\\right)}\\leq\\ldots\\leq X_{\\left(n\\right)}\\), and in particular, \\(X_{\\left(1\\right)}=\\underset{1\\leq i\\leq n}{\\min}X_{i}\\) and \\(X_{\\left(n\\right)}=\\underset{1\\leq i\\leq n}{\\max}X_{i}\\). Theorem 3.21 Let \\(X_{1},\\ldots,X_{n}\\) be a random sample from a discrete distribution with pmf \\(f_{X}\\left(x_{i}\\right)=p_{i}\\), where \\(x_{1}&lt;x_{2}&lt;\\cdots\\) are the possible values of \\(X\\) in ascending order. Define \\[\\begin{align*} P_{0} &amp; =0 \\\\ P_{1} &amp; =p_{1} \\\\ P_{2} &amp; =p_{1}+p_{2} \\\\ &amp; \\vdots \\\\ P_{i} &amp; =p_{1}+p_{2}+\\cdots+p_{i} \\\\ &amp; \\vdots \\end{align*}\\] Let \\(X_{\\left(1\\right)},\\ldots,X_{\\left(n\\right)}\\) denote the order statistics from the sample. Then \\[ P\\left(\\left\\{ X_{\\left(j\\right)}\\leq x_{i}\\right\\} \\right)=\\sum_{k=j}^{n}\\binom{n}{k}P_{i}^{k}\\left(1-P_{i}\\right)^{n-k} \\] and \\[ P\\left(\\left\\{ X_{\\left(j\\right)}=x_{i}\\right\\} \\right)=\\sum_{k=j}^{n}\\binom{n}{k}\\left[P_{i}^{k}\\left(1-P_{i}\\right)^{n-k}-P_{i-1}^{k}\\left(1-P_{i-1}\\right)^{n-k}\\right]. \\] (This is Theorem 5.4.3 from Casella and Berger (2002).) Proof. PROOF GOES HERE Theorem 3.22 Let \\(X_{\\left(1\\right)},\\ldots,X_{\\left(n\\right)}\\) denote the order statistics of a random sample, \\(X_{1},\\ldots,X_{n}\\), from a continuous population with cdf \\(F_{X}\\left(x\\right)\\) and pdf \\(f_{X}\\left(x\\right)\\). Then the pdf of \\(X_{\\left(j\\right)}\\) is \\[ f_{X_{\\left(j\\right)}}\\left(x\\right)=\\frac{n!}{\\left(j-1\\right)!\\left(n-j\\right)!}f_{X}\\left(x\\right)\\left[F_{X}\\left(x\\right)\\right]^{j-1}\\left[1-F_{X}\\left(x\\right)\\right]^{n-j}. \\] (This is Theorem 5.4.4 from Casella and Berger (2002).) Proof. PROOF GOES HERE Theorem 3.23 Let \\(X_{\\left(1\\right)},\\ldots,X_{\\left(n\\right)}\\) denote the order statistics of a random sample, \\(X_{1},\\ldots,X_{n}\\), from a continuous population with cdf \\(F_{X}\\left(x\\right)\\) and pdf \\(f_{X}\\left(x\\right)\\). Then the joint pdf of \\(X_{\\left(i\\right)} and X_{\\left(j\\right)}\\), \\(1\\leq i&lt;j\\leq n\\), is \\[\\begin{align*} f_{X_{\\left(i\\right)},X_{\\left(j\\right)}}\\left(u,v\\right) &amp; =\\frac{n!}{\\left(i-1\\right)!\\left(j-1-i\\right)!\\left(n-j\\right)!}f_{X}\\left(u\\right)f_{X}\\left(v\\right) \\\\ &amp;\\quad\\times\\left[F_{X}\\left(u\\right)\\right]^{i-1}\\left[F_{X}\\left(v\\right)-F_{X}\\left(u\\right)\\right]^{j-1-i}\\left[1-F_{X}\\left(v\\right)\\right]^{n-j} \\end{align*}\\] for \\(-\\infty&lt;u&lt;v&lt;\\infty\\). (This is Theorem 5.4.6 from Casella and Berger (2002).) Proof. PROOF GOES HERE 3.4.4 Convergence concepts Theorem 3.24 (Strong Law of Large Numbers) Let \\(X_{1},X_{2},\\ldots\\) be iid random variables with \\(\\E\\left[X_{i}\\right]=\\mu\\) and \\(\\Var\\left(X_{i}\\right)=\\sigma^{2}&lt;\\infty\\), and define \\(\\bar{X}_{n}=\\left(1/n\\right)\\sum_{i=1}^{n}X_{i}\\). Then, for every \\(\\epsilon&gt;0\\), \\[ P\\left(\\lim_{n\\rightarrow\\infty}\\left|\\bar{X}_{n}-\\mu\\right|&lt;\\epsilon\\right)=1; \\] that is, \\(\\bar{X}_{n}\\) converges almost surely to \\(\\mu\\). Proof. PROOF GOES HERE Theorem 3.25 (Central Limit Theorem) Let \\(X_{1},X_{2},\\ldots\\) be a sequence of iid random variables whose mgfs exist in a neighborhood of \\(0\\) (that is, \\(M_{X_{i}}\\left(t\\right)\\) exists for \\(\\left|t\\right|&lt;h\\), for some positive \\(h\\)). Let \\(\\E\\left[X_{i}\\right]=\\mu\\) and \\(\\Var\\left(X_{i}\\right)=\\sigma^{2}&gt;0\\). (Both \\(\\mu\\) and \\(\\sigma^{2}\\) are finite since the mgf exists.) Define \\(\\bar{X}_{n}=\\left(1/n\\right)\\sum_{i=1}^{n}X_{i}\\). Let \\(G_{n}\\left(x\\right)\\) denote the cdf of \\(\\sqrt{n}\\left(\\bar{X}-\\mu\\right)/\\sigma\\). Then, for any \\(x\\in\\mathbb{R}\\), \\[ \\lim_{n\\rightarrow\\infty}G_{n}\\left(x\\right)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-y^{2}/2}\\dif y; \\] that is, \\(\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)/\\sigma\\) has a limiting standard normal distribution. (This is Theorem 5.5.14 from Casella and Berger (2002); the following proof is given there.) Proof. Let \\(Z\\sim\\mathcal{N}\\left(0,1\\right)\\), so that the mgf of \\(Z\\) given by \\(M_{Z}\\left(t\\right)=\\mathrm{e}^{0\\cdot t+\\left(1\\cdot t^{2}\\right)/2}=\\mathrm{e}^{t^{2}/2}\\). We will show that, for \\(\\left|t\\right|&lt;h\\), the mgf of \\(\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)/\\sigma\\) converges to \\(\\mathrm{e}^{t^{2}/2}\\). Define \\(Y_{i}=\\left(X_{i}-\\mu\\right)/\\sigma\\), and let \\(M_{Y}\\left(t\\right)\\) denote the common mgf of the \\(Y_{i}\\text{&#39;s}\\), which exists for \\(\\left|t\\right|&lt;\\sigma h\\) and is given by Theorem 3.9. We have \\[ \\frac{X_{i}-\\mu}{\\sigma}=Y_{i}\\implies X_{i}-\\mu=\\sigma Y_{i}\\implies X_{i}=\\sigma Y_{i}+\\mu, \\] so that \\[\\begin{align*} \\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma} &amp; =\\frac{\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}-\\mu\\right)}{\\sigma} \\\\ &amp; =\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\sigma Y_{i}+\\mu\\right)-\\mu\\right] \\\\ &amp; =\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{1}{n}\\left(\\sigma\\sum_{i=1}^{n}Y_{i}+n\\mu\\right)-\\mu\\right] \\\\ &amp; =\\frac{\\sqrt{n}}{\\sigma}\\left[\\frac{\\sigma}{n}\\sum_{i=1}^{n}Y_{i}+\\mu-\\mu\\right] \\\\ &amp; =\\frac{\\sqrt{n}}{\\sigma}\\left(\\frac{\\sigma}{n}\\sum_{i=1}^{n}Y_{i}\\right) \\\\ &amp; =\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}Y_{i}. \\end{align*}\\] Then, from the properties of mgfs (see Theorems 3.9 and 3.15), we have \\[ M_{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)/\\sigma}\\left(t\\right)=M_{\\sum_{i=1}^{n}Y_{i}/\\sqrt{n}}\\left(t\\right)=M_{\\sum_{i=1}^{n}Y_{i}}\\left(\\frac{t}{\\sqrt{n}}\\right)=\\left(M_{Y}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right)^{n}. \\] We now expand \\(M_{Y}\\left(t/\\sqrt{n}\\right)\\) in a Taylor series (power series) around 0. We have \\[ M_{Y}\\left(\\frac{t}{\\sqrt{n}}\\right)=\\sum_{k=0}^{\\infty}M_{Y}^{\\left(k\\right)}\\left(0\\right)\\frac{\\left(t/\\sqrt{n}\\right)^{k}}{k!}, \\] where \\(M_{Y}^{\\left(k\\right)}\\left(0\\right)=\\left(\\dif^{k}/\\dif t^{k}\\right)M_{Y}\\left(t\\right)\\vert_{t=0}\\). Since the mgfs exist for \\(\\left|t\\right|&lt;h\\), the power series expansion is valid if \\(t&lt;\\sqrt{n}\\sigma h\\). We have \\[\\begin{align*} M_{Y}^{\\left(0\\right)} &amp; =\\E\\left[Y^{0}\\right]=\\E\\left[1\\right]=1, \\\\ M_{Y}^{\\left(1\\right)} &amp; =\\E\\left[Y^{1}\\right]=\\E\\left[\\frac{X-\\mu}{\\sigma}\\right]=\\frac{1}{\\sigma}\\left(\\E\\left[X\\right]-\\E\\left[\\mu\\right]\\right)=\\frac{1}{\\sigma}\\left(\\mu-\\mu\\right)=0, \\end{align*}\\] and, noting that \\[ \\Var\\left(X\\right)=\\E\\left[X^{2}\\right]-\\left(\\E\\left[X\\right]\\right)^{2}\\implies\\sigma^{2}=\\E\\left[X^{2}\\right]-\\mu^{2}\\implies\\E\\left[X^{2}\\right]=\\mu^{2}+\\sigma^{2}, \\] we have \\[\\begin{align*} M_{Y}^{\\left(2\\right)} &amp; =\\E\\left[Y^{2}\\right] \\\\ &amp; =\\E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{2}\\right] \\\\ &amp; =\\frac{1}{\\sigma^{2}}\\E\\left[X^{2}-2\\mu X+\\mu^{2}\\right] \\\\ &amp; =\\frac{1}{\\sigma^{2}}\\left(\\E\\left[X^{2}\\right]-2\\mu\\E\\left[X\\right]+\\E\\left[\\mu^{2}\\right]\\right) \\\\ &amp; =\\frac{1}{\\sigma^{2}}\\left(\\mu^{2}+\\sigma^{2}-2\\mu^{2}+\\mu^{2}\\right) \\\\ &amp; =\\frac{1}{\\sigma^{2}}\\left(\\sigma^{2}\\right) \\\\ &amp; =1. \\end{align*}\\] (By construction, the mean and variance of Y are 0 and 1, respectively.) Then, we have \\[\\begin{align*} M_{Y}\\left(\\frac{t}{\\sqrt{n}}\\right) &amp; =\\sum_{k=0}^{\\infty}M_{Y}^{\\left(k\\right)}\\left(0\\right)\\left(\\frac{t/\\sqrt{n}}{k!}\\right)^{k} \\\\ &amp; =1\\frac{\\left(t\\sqrt{n}\\right)^{0}}{0!}+0\\frac{\\left(t\\sqrt{n}\\right)}{1!}+1\\frac{\\left(t\\sqrt{n}\\right)^{2}}{2!}+\\sum_{k=3}^{\\infty}M_{Y}^{\\left(k\\right)}\\left(0\\right)\\frac{\\left(t/\\sqrt{n}\\right)^{k}}{k!} \\\\ &amp; =T_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)+R_{2}\\left(\\frac{t}{\\sqrt{n}}\\right), \\end{align*}\\] where \\[ T_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)=1+\\frac{\\left(t/\\sqrt{n}\\right)^{2}}{2!}\\quad\\text{and}\\quad R_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)=\\sum_{k=3}^{\\infty}M_{Y}^{\\left(k\\right)}\\left(0\\right)\\frac{\\left(t/\\sqrt{n}\\right)^{k}}{k!}. \\] We have \\(n&gt;0\\), so for fixed \\(t\\neq 0\\), the quantity \\(t/\\sqrt{n}\\rightarrow 0\\) as \\(n\\rightarrow\\infty\\). Then, noting that \\(M_{Y}^{\\left(2\\right)}\\left(0\\right)\\) exists, it follows from Theorem 3.26 that \\[ \\lim_{t/\\sqrt{n}\\rightarrow0}\\frac{M_{Y}\\left(t/\\sqrt{n}\\right)-T_{2}\\left(t/\\sqrt{n}\\right)}{\\left(t/\\sqrt{n}-0\\right)^{2}}=0\\implies\\lim_{n\\rightarrow\\infty}\\frac{R_{2}\\left(t/\\sqrt{n}\\right)}{\\left(t/\\sqrt{n}\\right)^{2}}=0. \\] Since \\(t\\) is fixed, we also have \\[ \\lim_{n\\rightarrow\\infty}\\frac{R_{2}\\left(t/\\sqrt{n}\\right)}{\\left(1/\\sqrt{n}\\right)^{2}}=\\lim_{n\\rightarrow\\infty}nR_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)=0, \\] and this is also true at \\(t=0\\) since \\[ R_{2}\\left(\\frac{0}{\\sqrt{n}}\\right)=R_{2}\\left(0\\right)=\\sum_{k=3}^{\\infty}M_{Y}^{\\left(k\\right)}\\left(0\\right)\\frac{0^{k}}{k!}=\\sum_{k=3}^{\\infty}M_{Y}^{\\left(k\\right)}\\left(0\\right)\\cdot0=0. \\] Thus, for any fixed \\(t\\), we can write \\[\\begin{align*} \\lim_{n\\rightarrow\\infty}\\left(M_{Y}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right)^{n} &amp; =\\lim_{n\\rightarrow\\infty}\\left[1+\\frac{\\left(t/\\sqrt{n}\\right)^{2}}{2}+R_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^{n} \\\\ &amp; =\\lim_{n\\rightarrow\\infty}\\left[1+\\frac{t^{2}}{2n}+R_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^{n} \\\\ &amp; =\\lim_{n\\rightarrow\\infty}\\left[1+\\frac{1}{n}\\left(\\frac{t^{2}}{2}+nR_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right)\\right]^{n}. \\end{align*}\\] Setting \\(a_{n}=\\left(t^{2}/2\\right)+nR_{2}\\left(t/\\sqrt{n}\\right)\\), we have \\[ \\lim_{n\\rightarrow\\infty}a_{n}=\\lim_{n\\rightarrow\\infty}\\left[\\frac{t^{2}}{2}+nR_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]=\\lim_{n\\rightarrow\\infty}\\frac{t^{2}}{2}+\\lim_{n\\rightarrow\\infty}nR_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)=\\frac{t^{2}}{2}+0=\\frac{t^{2}}{2}, \\] i.e., the sequence \\(a_{1},a_{2},\\ldots\\) converges to \\(a=t^{2}/2\\) as \\(n\\rightarrow\\infty\\). It follows from Lemma 3.1 that \\[ \\lim_{n\\rightarrow\\infty}\\left(M_{Y}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right)^{n}=\\lim_{n\\rightarrow\\infty}\\left[1+\\frac{1}{n}\\left(\\frac{t^{2}}{2}+nR_{2}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right)\\right]^{n}=\\lim_{n\\rightarrow\\infty}\\left[1+\\frac{a_{n}}{n}\\right]^{n}=\\mathrm{e}^{a}=\\mathrm{e}^{t^{2}/2}. \\] Since \\(\\mathrm{e}^{t^{2}/2}\\) is the mgf of the \\(\\mathcal{N}\\left(0,1\\right)\\) distribution, the theorem is proved. Definition 3.10 If a function \\(g\\left(x\\right)\\) has derivatives of order \\(r\\), that is, \\(g^{\\left(r\\right)}\\left(x\\right)=\\frac{\\dif^{r}}{\\dif x^{r}}g\\left(x\\right)\\) exists, then for any constant \\(a\\), the Taylor polynomial of order \\(r\\) about \\(a\\) is \\[ T_{r}\\left(x\\right)=\\sum_{i=0}^{r}\\frac{g^{\\left(i\\right)}\\left(a\\right)}{i!}\\left(x-a\\right)^{i}. \\] Theorem 3.26 If \\[ g^{\\left(r\\right)}\\left(a\\right)=\\frac{\\dif^{r}}{\\dif x^{r}}g\\left(x\\right)\\Bigr\\vert_{x=a} \\] exists, then \\[ \\lim_{x\\rightarrow a}\\frac{g\\left(x\\right)-T_{r}\\left(x\\right)}{\\left(x-a\\right)^{r}}=0. \\] (This is Theorem 5.5.21 from Casella and Berger (2002)). References "],
["linear-algebra.html", "Chapter 4 Linear algebra", " Chapter 4 Linear algebra Theorem 4.1 (Fredholm Alternative) Let \\(\\mathbf{A}\\in\\mathbf{M}_{m,n}\\left(\\mathbb{R}\\right)\\), let \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\), and let \\(\\mathbf{b}\\in\\mathbb{R}^{m}\\). Then, there are two mutually exclusive possibilities: The system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) has a unique solution \\(\\mathbf{x}\\) for each \\(\\mathbf{b}\\). In particular, the system has the solution \\(\\mathbf{x}=\\mathbf{0}\\) for \\(\\mathbf{b}=\\mathbf{0}\\). The homogeneous equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) has exactly \\(p\\) linearly independent solutions \\(\\left\\{ \\mathbf{x}_{i}\\right\\} _{i=1}^{p}\\) for some \\(p\\geq 1\\). Definition 4.1 Suppose that \\(\\mathbf{A}\\in\\mathbf{M}_{m,p}\\left(\\mathbb{R}\\right)\\), and suppose that \\(\\mathbf{B}\\in\\mathbf{M}_{p,n}\\left(\\mathbb{R}\\right)\\). Then, the Wronskian of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is \\(\\left\\langle \\mathbf{A},\\mathbf{B}\\right\\rangle \\coloneqq\\mathbf{A}\\mathbf{B}-\\mathbf{B}\\mathbf{A}\\). Definition 4.2 The range of a matrix \\(\\mathbf{A}\\), denoted \\(\\text{range}\\left(\\mathbf{A}\\right)\\), is the space spanned by the columns of \\(\\mathbf{A}\\). Definition 4.3 The null space of a matrix \\(\\mathbf{A}\\in\\mathbf{M}_{m,n}\\left(\\mathbb{R}\\right)\\), denoted \\(\\text{null}\\left(\\mathbf{A}\\right)\\), is the set of vectors \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) that satisfy \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\). Theorem 4.2 (Invertible Matrix Theorem) Let \\(\\mathbf{A}\\in\\mathbf{M}_{m,m}\\left(\\mathbb{R}\\right)\\). Then, the following are equivalent: \\(\\mathbf{A}^{-1}\\) exists. \\(\\mathrm{rank}\\left(\\mathbf{A}\\right)=m\\). \\(\\mathrm{range}\\left(\\mathbf{A}\\right)=\\mathbb{R}^{m}\\). \\(\\mathrm{null}\\left(\\mathbf{A}\\right)=\\mathbf{0}\\). Theorem 4.3 If \\(\\left\\{ \\mathbf{v}_{i}\\right\\} _{i=1}^{r}\\) are eigenvectors that correspond to distinct eigenvalues \\(\\left\\{ \\lambda_{i}\\right\\} _{i=1}^{r}\\) of an \\(n\\times n\\) matrix \\(\\mathbf{A}\\), then the set \\(\\left\\{ \\mathbf{v}_{i}\\right\\} _{i=1}^{r}\\) is linearly independent. Proof. Suppose that the set \\(\\left\\{ \\mathbf{v}_{i}\\right\\} _{i=1}^{r}\\) is linearly dependent, and let \\(p\\in\\left\\{ 1,\\ldots,r\\right\\}\\) be the least index such that \\(\\mathbf{v}_{p+1}\\) is a linear combination of the preceding (linearly independent) vectors \\(\\left\\{ \\mathbf{v}_{i}\\right\\} _{i=1}^{p}\\). Then, there exist weights \\(\\left\\{ c_{i}\\right\\} _{i=1}^{p}\\) such that \\[ \\mathbf{v}_{p+1}=c_{1}\\mathbf{v}_{1}+\\cdots+c_{p}\\mathbf{v}_{p}=\\sum_{i=1}^{p}c_{i}\\mathbf{v}_{i}. \\] Left-multiplying each side of this equality by \\(\\mathbf{A}\\) gives \\[ \\mathbf{A}\\mathbf{v}_{p+1}= \\mathbf{A}\\sum_{i=1}^{p}c_{i}\\mathbf{v}_{i}= \\sum_{i=1}^{p}c_{i}\\mathbf{A}\\mathbf{v}_{i} \\implies\\lambda_{p+1}\\mathbf{v}_{p+1} =\\sum_{i=1}^{p}c_{i}\\lambda_{i}\\mathbf{v}_{i}. \\] Noting that \\[ \\lambda_{p+1}\\mathbf{v}_{p+1}=\\lambda_{p+1}\\sum_{i=1}^{p}c_{i}\\mathbf{v}_{i}=\\sum_{i=1}^{p}c_{i}\\lambda_{p+1}\\mathbf{v}_{i}, \\] subtracting one equation from the other yields \\[ \\lambda_{p+1}\\mathbf{v}_{p+1}-\\lambda_{p+1}\\mathbf{v}_{p+1}=\\sum_{i=1}^{p}c_{i}\\lambda_{i}\\mathbf{v}_{i}-\\sum_{i=1}^{p}c_{i}\\lambda_{p+1}\\mathbf{v}_{i}\\implies\\mathbf{0}=\\sum_{i=1}^{p}c_{i}\\left(\\lambda_{i}-\\lambda_{p+1}\\right)\\mathbf{v}_{i}. \\] By definition, an eigenvector is nonzero, and by construction, the set \\(\\left\\{ \\mathbf{v}_{i}\\right\\} _{i=1}^{p}\\) is linearly independent. Thus, all of the weights \\(c_{i}\\left(\\lambda_{i}-\\lambda_{p+1}\\right)\\) must be zero. Because the eigenvalues are distinct, it cannot be the case that any factor \\(\\lambda_{i}-\\lambda_{p+1}\\) is zero, so it follows that \\(c_{i}=0\\) for \\(i\\in\\left\\{ 1,\\ldots,p\\right\\}\\). But if this is the case, then \\[ \\mathbf{v}_{p+1}=\\sum_{i=1}^{p}c_{i}\\mathbf{v}_{i}=\\sum_{i=1}^{p}0\\mathbf{v}_{i}=\\mathbf{0}, \\] which contradicts the fact that \\(\\mathbf{v}_{p+1}\\) is an eigenvector, hence nonzero. Thus, \\(\\left\\{ \\mathbf{v}_{i}\\right\\} _{i=1}^{r}\\) cannot be linearly dependent, and therefore must be linearly independent. Proposition 4.1 A scalar \\(\\lambda\\) is an eigenvalue of an \\(n\\times n\\) matrix \\(\\mathbf{A}\\) if and only if \\(\\lambda\\) satisfies the characteristic equation \\(\\det\\left(\\mathbf{A}-\\lambda\\mathbf{I}\\right)=0\\). Proposition 4.2 An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) has exactly \\(n\\) eigenvalues, including multiplicities. Proof. The determinant of \\(\\mathbf{A}-\\lambda\\mathbf{I}\\) for \\(\\lambda\\in\\mathbb{C}\\) is a polynomial in \\(\\lambda\\) of degree at most \\(n\\). It follows from the Fundamental Theorem of Algebra that the characteristic equation \\(\\det\\left(\\mathbf{A}-\\lambda\\mathbf{I}\\right)=0\\) has exactly \\(n\\) complex roots, including multiplicities. The previous proposition implies that each such root is an eigenvalue of \\(\\mathbf{A}\\). Theorem 4.4 A real, symmetric \\(n\\times n\\) matrix \\(\\mathbf{A}\\) has \\(n\\) real eigenvalues, including multiplicities. Proof. Let \\(\\mathbf{x}\\in\\mathbb{C}^{n}\\), and define \\(q=\\bar{\\mathbf{x}}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}\\). Observe that \\[\\begin{align*} \\bar{q} &amp; =\\overline{\\bar{\\mathbf{x}}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}} \\\\ &amp; =\\overline{\\bar{\\mathbf{x}}^{\\mathsf{T}}}\\bar{\\mathbf{A}}\\bar{\\mathbf{x}} \\\\ &amp; =\\bar{\\bar{\\mathbf{x}}}^{\\mathsf{T}}\\mathbf{A}\\bar{\\mathbf{x}}\\tag{$\\mathbf{A}$ is real} \\\\ &amp; =\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\bar{\\mathbf{x}} \\\\ &amp; =\\left(\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\bar{\\mathbf{x}}\\right)^{\\mathsf{T}}\\tag{$\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\bar{\\mathbf{x}}$ is a scalar} \\\\ &amp; =\\bar{\\mathbf{x}}^{\\mathsf{T}}\\mathbf{A}^{\\mathsf{T}}\\left(\\mathbf{x}^{\\mathsf{T}}\\right)^{\\mathsf{T}} \\\\ &amp; =\\bar{\\mathbf{x}}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}\\tag{$\\mathbf{A}$ is symmetric} \\\\ &amp; =q, \\end{align*}\\] hence \\(q\\) is real. Now suppose that \\(\\mathbf{x}\\) is an eigenvector of \\(\\mathbf{A}\\) with associated eigenvalue \\(\\lambda\\). Then, \\[ q= \\bar{\\mathbf{x}}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}= \\bar{\\mathbf{x}}^{\\mathsf{T}}\\lambda\\mathbf{x}= \\lambda\\bar{\\mathbf{x}}^{\\mathsf{T}}\\mathbf{x}= \\lambda \\begin{bmatrix} \\bar{x}_{1} &amp; \\bar{x}_{2} &amp; \\cdots &amp; \\bar{x}_{n} \\end{bmatrix} \\begin{bmatrix} x_{1}\\\\ x_{2}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix}= \\lambda\\left(\\bar{x}_{1}x_{1}+\\bar{x}_{2}x_{2}+\\cdots+\\bar{x}_{n}x_{n}\\right). \\] Now, we can write some \\(c\\in\\mathbb{C}\\) as the sum of its real and complex parts, i.e., \\(c=a+\\iota b\\), where \\(a,b\\in\\mathbb{R}\\), so that \\[ \\bar{c}c=\\overline{\\left(a+\\iota b\\right)}\\left(a+\\iota b\\right)=\\left(a-\\iota b\\right)\\left(a+\\iota b\\right)=a^{2}+\\iota ab-\\iota ab-\\iota^{2}b^{2}=a^{2}+b^{2}. \\] \\(a\\) and \\(b\\) are real, so it follows that \\(\\bar{c}c\\) is real, hence that each \\(\\bar{x}_{i}x_{i}\\) is real. \\(q\\) is real, and \\(\\bar{\\mathbf{x}}^{\\mathsf{T}}\\mathbf{x}\\) is real, so it follows that \\(\\lambda\\) must also be real. From Proposition 4.2, \\(\\mathbf{A}\\) has exactly \\(n\\) eigenvalues, and we have shown that each eigenvalue \\(\\lambda\\) is real, proving the theorem. Theorem 4.5 (Spectral Theorem) A non-degenerate matrix \\(\\mathbf{A}\\in\\mathbf{M}_{m,m}\\left(\\mathbb{R}\\right)\\) has a decomposition of the form \\(\\mathbf{A}=\\mathbf{X}\\boldsymbol{\\Lambda}\\mathbf{X}^{-1}\\), provided that \\(\\mathbf{X}^{-1}\\in\\mathbf{M}_{m,m}\\left(\\mathbb{R}\\right)\\) exists, and where \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix whose entries are the eigenvalues of \\(\\mathbf{A}\\). Proof. PROOF GOES HERE Definition 4.4 A unitary matrix \\(\\mathbf{U}\\in\\mathbf{M}_{m,m}\\left(\\mathbb{R}\\right)\\) has the property \\(\\mathbf{U}^{-1}=\\mathbf{U}^{\\mathsf{H}}\\), where \\(\\mathbf{A}^{\\mathsf{H}}\\) denotes the Hermitian conjugate (conjugate transpose) of \\(\\mathbf{A}\\). Theorem 4.6 (Unitary Decomposition) A symmetric matrix \\(\\mathbf{A}\\in\\mathbf{M}_{m,m}\\left(\\mathbb{R}\\right)\\) admits the unitary diagonalization \\(\\mathbf{A}=\\mathbf{Q}\\boldsymbol{\\Lambda}\\mathbf{Q}^{-1}\\), where \\(\\mathbf{Q}\\in\\mathbf{M}_{m,m}\\left(\\mathbb{R}\\right)\\) is unitary. "],
["common-families-of-distributions.html", "Chapter 5 Common families of distributions 5.1 Exponential families 5.2 Location and scale families", " Chapter 5 Common families of distributions 5.1 Exponential families A family of pdfs (or pmfs) indexed by a parameter \\(\\boldsymbol{\\theta}\\) is called a \\(k\\)-parameter exponential family if it can be expressed as \\[ f\\left(x|\\boldsymbol{\\theta}\\right)=h\\left(x\\right)c\\left(\\boldsymbol{\\theta}\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\omega_{j}\\left(\\boldsymbol{\\theta}\\right)t_{j}\\left(x\\right)\\right\\} \\] where \\(h\\left(x\\right)\\geq 0\\), \\(c\\left(\\boldsymbol{\\theta}\\right)\\geq 0\\), and \\(t_{1}\\left(x\\right),\\ldots,t_{k}\\left(x\\right)\\) are real-valued functions of \\(x\\), and \\(\\omega_{1}\\left(\\boldsymbol{\\theta}\\right),\\ldots,\\omega_{k}\\left(\\boldsymbol{\\theta}\\right)\\) are real-valued functions of the possibly vector-valued parameter \\(\\boldsymbol{\\theta}\\). I.e., \\(f\\left(x|\\boldsymbol{\\theta}\\right)\\) can be expressed in three parts: a part that depends only on the random variable(s), a part that depends only on the parameter(s), and a part that depends on both the random variable(s) and the parameter(s). Most of the parametric models you may have studied are exponential families, e.g., normal, gamma, beta, binomial, negative binomial, Poisson, and multinomial. The uniform distribution is not an exponential family (see Example 5.5 below). Example 5.1 (Logistic regression) TO DO: this example doesn’t make sense here, move it. For \\(Y_{1},Y_{2},\\ldots,Y_{n}\\), let \\(Y_{i}\\sim\\text{Bernoulli}\\left(p\\right)\\), i.e., \\[ Y_{i}=\\begin{cases} 0, &amp; \\text{if no event}\\\\ 1, &amp; \\text{if event.} \\end{cases} \\] Then the logistic regression model is \\[ \\log\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k} \\] where \\(\\log\\left(p/\\left(1-p\\right)\\right)\\) is called the logit link. Example 5.2 (Binomial random variables) Let \\(X\\sim\\mathcal{B}\\left(n,p\\right)\\), where \\(p\\in\\left(0,1\\right)\\). Recall that \\(X\\) represents the number of successes in \\(n\\) i.i.d. Bernoulli trials and its pmf is given by \\[ f\\left(x|p\\right) =\\binom{n}{x}p^{x}\\left(1-p\\right)^{n-x} \\] for \\(x=0,1,\\ldots,n\\) and \\(f\\left(x|p\\right)=0\\) otherwise. Express \\(f\\left(x|p\\right)\\) in exponential family form. \\[\\begin{align*} f\\left(x|p\\right) &amp; =\\binom{n}{x}p^{x}\\left(1-p\\right)^{n-x} \\\\ &amp; =\\binom{n}{x}p^{x}\\left(1-p\\right)^{n}\\left(1-p\\right)^{-x} \\\\ &amp; =\\binom{n}{x}\\left(1-p\\right)^{n}\\left(\\frac{p^{x}}{\\left(1-p\\right)^{x}}\\right) \\\\ &amp; =\\binom{n}{x}\\left(1-p\\right)^{n}\\left(\\frac{p}{1-p}\\right)^{x} \\\\ &amp; =\\binom{n}{x}\\left(1-p\\right)^{n}\\exp\\left\\{ \\log\\left(\\frac{p}{1-p}\\right)^{x}\\right\\} \\\\ &amp; =\\underbrace{\\binom{n}{x}}_{h\\left(x\\right)}\\underbrace{\\left(1-p\\right)^{n}}_{c\\left(p\\right)}\\exp\\left\\{ \\underbrace{x}_{t_{1}\\left(x\\right)}\\underbrace{\\log\\left(\\frac{p}{1-p}\\right)}_{\\omega_{1}\\left(p\\right)}\\right\\} \\end{align*}\\] Example 5.3 (Poisson random variables) Let \\(X\\sim\\text{Poisson}\\left(\\lambda\\right)\\), where \\(\\lambda&gt;0\\). Recall that \\(X\\) represents the frequency with which a specified event occurs given some fixed dimension, such as space or time, and its pmf is given by \\[ f\\left(x|\\lambda\\right)=\\frac{\\mathrm{e}^{-\\lambda}\\lambda^{x}}{x!} \\] for \\(x=0,1,2,\\ldots\\) and \\(f\\left(x|\\lambda\\right)=0\\) otherwise. Express \\(f\\left(x|\\lambda\\right)\\) in exponential family form. \\[ f\\left(x|\\lambda\\right)=\\frac{\\mathrm{e}^{-\\lambda}\\lambda^{x}}{x!}=\\frac{1}{x!}\\mathrm{e}^{-\\lambda}\\exp\\left\\{ \\log\\left(\\lambda^{x}\\right)\\right\\} =\\frac{1}{x!}\\mathrm{e}^{-\\lambda}\\exp\\left\\{ x\\log\\lambda\\right\\} \\] Then, we have \\(h\\left(x\\right)=1/x!\\), \\(c\\left(\\lambda\\right)=\\mathrm{e}^{-\\lambda}\\), \\(t_{1}\\left(x\\right)=x\\), and \\(\\omega_{1}\\left(\\lambda\\right)=\\log\\lambda\\). In a Poisson regression, we have \\(\\log\\left(\\lambda\\right)=\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{k}X_{k}\\). Example 5.4 (Normal random variables) Let \\(X\\sim\\mathcal{N}\\left(\\mu,\\sigma^{2}\\right)\\), where \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma&gt;0\\). A pdf for \\(X\\) is given by \\[ f\\left(x|\\mu,\\sigma^{2}\\right)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ -\\frac{\\left(x-\\mu\\right)^{2}}{2\\sigma^{2}}\\right\\} \\] for \\(x\\in\\mathbb{R}\\). Express \\(f\\left(x|\\mu,\\sigma^{2}\\right)\\) in exponential family form. Suppose \\(\\sigma\\) is known. \\[\\begin{align*} f\\left(x|\\mu\\right) &amp; =\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ -\\frac{x^{2}-2\\mu x+\\mu^{2}}{2\\sigma^{2}}\\right\\} \\\\ &amp; =\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ -\\frac{x^{2}}{2\\sigma^{2}}\\right\\} \\exp\\left\\{ -\\frac{\\mu^{2}}{2\\sigma^{2}}\\right\\} \\exp\\left\\{ -\\frac{-2\\mu x}{2\\sigma^{2}}\\right\\} \\\\ &amp; =\\underbrace{\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ -\\frac{x^{2}}{2\\sigma^{2}}\\right\\} }_{h\\left(x\\right)}\\underbrace{\\exp\\left\\{ -\\frac{\\mu^{2}}{2\\sigma^{2}}\\right\\} }_{c\\left(\\mu\\right)}\\exp\\left\\{ \\underbrace{\\frac{\\mu}{\\sigma^{2}}}_{\\omega_{1}\\left(\\mu\\right)}\\cdot\\underbrace{x}_{t_{1}\\left(x\\right)}\\right\\} \\\\ \\end{align*}\\] Suppose \\(\\sigma\\) is unknown. \\[\\begin{align*} f\\left(x|\\mu,\\sigma^{2}\\right) &amp; =\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ -\\frac{\\left(x-\\mu\\right)^{2}}{2\\sigma^{2}}\\right\\} \\\\ &amp; =\\frac{1}{\\sqrt{2\\pi}}\\left(\\sigma^{2}\\right)^{-1/2}\\exp\\left\\{ -\\frac{x^{2}-2\\mu x+\\mu^{2}}{2\\sigma^{2}}\\right\\} \\\\ &amp; =\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ \\log\\left(\\sigma^{2}\\right)^{-1/2}\\right\\} \\exp\\left\\{ -\\frac{x^{2}-2\\mu x}{2\\sigma^{2}}\\right\\} \\exp\\left\\{ -\\frac{\\mu^{2}}{2\\sigma^{2}}\\right\\} \\\\ &amp; =\\underbrace{\\frac{1}{\\sqrt{2\\pi}}}_{h\\left(x\\right)}\\underbrace{\\exp\\left\\{ -\\frac{\\mu^{2}}{2\\sigma^{2}}-\\frac{1}{2}\\log\\sigma^{2}\\right\\} }_{c\\left(\\mu,\\sigma^{2}\\right)}\\exp\\left\\{ \\underbrace{\\frac{1}{\\sigma^{2}}}_{\\omega_{1}\\left(\\mu,\\sigma^{2}\\right)}\\cdot\\underbrace{\\left(-\\frac{x^{2}}{2}\\right)}_{t_{1}\\left(x\\right)}+\\underbrace{\\frac{\\mu}{\\sigma^{2}}}_{\\omega_{2}\\left(\\mu,\\sigma^{2}\\right)}\\cdot\\underbrace{x}_{t_{2}\\left(x\\right)}\\right\\} \\end{align*}\\] Thus, in the case that \\(\\sigma\\) is unknown, \\(f\\left(x|\\mu,\\sigma^{2}\\right)\\) is a two-parameter exponential family, i.e., we have \\(k=2\\) for \\(\\sum_{j=1}^{k}\\omega_{j}\\left(\\theta\\right)t_{j}\\left(x\\right)\\). Definition 5.1 The indicator function of a set \\(\\mathcal{A}\\), denoted by \\(I_{\\mathcal{A}}\\left(x\\right)\\), is the function \\[ I_{\\mathcal{A}}\\left(x\\right)= \\begin{cases} 1, &amp; x\\in \\mathcal{A}\\\\ 0, &amp; x\\notin \\mathcal{A} \\end{cases}. \\] Example 5.5 (Uniform random variables) Let \\(X\\sim\\mathcal{U}\\left(0,\\theta\\right)\\), where \\(\\theta&gt;0\\). A pdf for \\(X\\) is given by \\[ f\\left(x|\\theta\\right)=\\frac{1}{\\theta-0}=\\frac{1}{\\theta} \\] for \\(0&lt;x&lt;\\theta\\). Express \\(f\\left(x|\\theta\\right)\\) in exponential family form, if possible. Let \\(\\mathcal{A}=\\left\\{ x:x\\in\\left(0,\\theta\\right)\\right\\}\\) and let \\(I_{\\mathcal{A}}\\) be the indicator function of \\(\\mathcal{A}\\).Then, we can write \\(f\\left(x|\\theta\\right)\\) as \\[ f\\left(x|\\theta\\right)= \\frac{1}{\\theta}I_{A}\\left(x\\right)= \\frac{1}{\\theta}I_{\\left(0,\\theta\\right)}\\left(x\\right). \\] Notice that \\(I_{\\left(0,\\theta\\right)}\\left(x\\right)\\) is not a function of \\(x\\) exclusively, not a function of \\(\\theta\\) exclusively, and cannot be written as an exponential. Because the entire pdf must be incorporated into \\(h\\left(x\\right)\\), \\(c\\left(\\theta\\right)\\), \\(t_{j}\\left(x\\right)\\), and \\(\\omega_{j}\\left(\\theta\\right)\\), it follows that the family of pdfs given by \\(f\\left(x|\\theta\\right)\\) is not an exponential family. Example 5.6 (Three-parameter exponential family distribution) Consider the family of distributions with densities \\[ f\\left(x|\\theta\\right) =\\frac{2}{\\Gamma\\left(1/4\\right)}\\exp\\left[-\\left(x-\\theta\\right)^{4}\\right] \\] for \\(x\\in\\mathbb{R}\\). Express \\(f\\left(x|\\theta\\right)\\) in exponential family form. Recall that the binomial theorem states that \\[ \\left(x+y\\right)^{n}=\\sum_{k=0}^{n}\\binom{n}{k}x^{k}y^{n-k}, \\] so we have \\[\\begin{align*} f\\left(x|\\theta\\right) &amp; =\\frac{2}{\\Gamma\\left(1/4\\right)}\\exp\\left[-\\left(x-\\theta\\right)^{4}\\right] \\\\ &amp; =\\frac{2}{\\Gamma\\left(1/4\\right)}\\exp\\left\\{ -\\sum_{k=0}^{4}\\binom{4}{k}x^{k}\\left(-\\theta\\right)^{4-k}\\right\\} \\\\ &amp; =\\frac{2}{\\Gamma\\left(1/4\\right)}\\exp\\left\\{ -\\left[1\\cdot1\\cdot\\theta^{4}-4x\\theta^{3}+6x^{2}\\theta^{2}-4x^{3}\\theta+1\\cdot x^{4}\\cdot1\\right]\\right\\} \\\\ &amp; =\\underbrace{\\frac{2}{\\Gamma\\left(1/4\\right)}\\exp\\left\\{ -x^{4}\\right\\} }_{h\\left(x\\right)}\\underbrace{\\exp\\left\\{ -\\theta^{4}\\right\\} }_{c\\left(\\theta\\right)}\\exp\\left\\{ \\underbrace{4x^{3}}_{t_{1}\\left(x\\right)}\\underbrace{\\theta}_{\\omega_{1}\\left(\\theta\\right)}\\underbrace{-6x^{2}}_{t_{2}\\left(x\\right)}\\underbrace{\\theta^{2}}_{\\omega_{2}\\left(\\theta\\right)}+\\underbrace{4x}_{t_{3}\\left(x\\right)}\\underbrace{\\theta^{3}}_{\\omega_{3}\\left(\\theta\\right)}\\right\\}. \\end{align*}\\] Theorem 5.1 Random samples from \\(k\\)-parameter exponential families have joint distributions which are \\(k\\)-parameter exponential families. Proof. Suppose that a random variable \\(X\\) has a pdf \\(f\\left(x|\\theta\\right)\\), and that \\(f\\) is part of an exponential family, so that \\(f\\) can be written as \\[ f=h\\left(x\\right)c\\left(\\theta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}t_{j}\\left(x\\right)\\omega_{j}\\left(\\theta\\right)\\right\\} . \\] Now suppose that \\(X_{1},X_{2},\\ldots,X_{n}\\) is a random sample from a population having the distribution of \\(X\\). It follows that the \\(X_{i}\\text{&#39;s}\\) are independent and identically distributed, and that each \\(X_{i}\\) has the same cdf as \\(X\\), and therefore that \\(f\\left(x|\\theta\\right)\\) is a pdf for each \\(X_{i}\\). Then, the joint pdf of the \\(X_{i}\\text{&#39;s}\\) is given by \\[\\begin{align*} f\\left(x_{1},x_{2},\\ldots,x_{n}|\\theta\\right) &amp; =\\prod_{i=1}^{n}f\\left(x_{i}|\\theta\\right) \\\\ &amp; =\\prod_{i=1}^{n}\\left[h\\left(x_{i}\\right)c\\left(\\theta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}t_{j}\\left(x_{i}\\right)\\omega_{j}\\left(\\theta\\right)\\right\\} \\right] \\\\ &amp; =\\left[\\prod_{i=1}^{n}h\\left(x_{i}\\right)\\right]\\left[c\\left(\\theta\\right)\\right]^{n}\\exp\\left\\{ \\sum_{j=1}^{k}\\sum_{i=1}^{n}t_{j}\\left(x_{i}\\right)\\omega_{j}\\left(\\theta\\right)\\right\\} \\end{align*}\\] Then, let \\[ h^{*}\\left(x\\right)=\\prod_{i=1}^{n}h\\left(x_{i}\\right)\\quad\\text{and}\\quad c^{*}\\left(\\theta\\right)=\\left[c\\left(\\theta\\right)\\right]^{n}, \\] so that we have \\[\\begin{align*} f\\left(x_{1},x_{2},\\ldots,x_{n}|\\theta\\right) &amp; =\\left[\\prod_{i=1}^{n}h\\left(x_{i}\\right)\\right]\\left[c\\left(\\theta\\right)\\right]^{n}\\exp\\left\\{ \\sum_{j=1}^{k}\\sum_{i=1}^{n}t_{j}\\left(x_{i}\\right)\\omega_{j}\\left(\\theta\\right)\\right\\} \\\\ &amp; =h^{*}\\left(x\\right)c^{*}\\left(\\theta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\left(\\omega_{j}\\left(\\theta\\right)\\sum_{i=1}^{n}t_{j}\\left(x_{i}\\right)\\right)\\right\\} . \\end{align*}\\] Now, let \\(T_{j}\\left(x\\right) =\\sum_{i=1}^{n}t_{j}\\left(x_{i}\\right)\\), so that \\[ f\\left(x_{1},x_{2},\\ldots,x_{n}|\\theta\\right) =h^{*}\\left(x\\right)c^{*}\\left(\\theta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\omega_{j}\\left(\\theta\\right)T_{j}\\left(x\\right)\\right\\} . \\] Thus, the joint pdf \\(f\\left(x_{1},x_{2},\\ldots,x_{n}|\\theta\\right)\\) is a \\(k\\)-parameter exponential family. 5.1.1 Natural parameters An exponential family is sometimes reparametrized as \\[ f\\left(x|\\boldsymbol{\\eta}\\right) =h\\left(x\\right)c^{*}\\left(\\boldsymbol{\\eta}\\right)\\exp\\left(\\sum_{j=1}^{k}\\eta_{j}t_{j}\\left(x\\right)\\right), \\] where the natural parameters are defined by \\(\\eta_{j}=\\omega_{j}\\left(\\theta\\right)\\) and the natural parameter space is \\[ \\left\\{ \\boldsymbol{\\eta}=\\left(\\eta_{1},\\ldots,\\eta_{k}\\right):\\int h\\left(x\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\eta_{j}t_{j}\\left(x\\right)\\right\\} \\dif x&lt;\\infty\\right\\} \\] so that \\[ c^{*}\\left(\\boldsymbol{\\eta}\\right)=\\frac{1}{\\int h\\left(x\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\eta_{j}t_{j}\\left(x\\right)\\dif x\\right\\} }, \\] which ensures that the pdf integrates to 1. Example 5.7 (Binomial random variables) Express the pmf of \\(X\\sim\\text{Binomial}\\left(n,p\\right)\\) using a natural parameterization. From Example 5.2, the pmf of \\(X\\) can be written as \\[ f\\left(x|p\\right) =\\binom{n}{x}\\left(1-p\\right)^{n}\\exp\\left\\{ x\\log\\left(\\frac{p}{1-p}\\right)\\right\\} , \\] where \\(k=1\\) and \\[ \\omega_{1}\\left(p\\right) =\\log\\frac{p}{1-p}. \\] Then, let \\(\\eta=\\omega_{1}\\left(p\\right)\\), so that \\[ \\mathrm{e}^{\\eta}=\\frac{p}{1-p}\\implies p=\\mathrm{e}^{\\eta}\\left(1-p\\right)=\\mathrm{e}^{\\eta}-\\mathrm{e}^{\\eta}p\\implies\\mathrm{e}^{\\eta}=p\\left(1+\\mathrm{e}^{\\eta}\\right)\\implies p=\\frac{\\mathrm{e}^{\\eta}}{1+\\mathrm{e}^{\\eta}}. \\] Then, we have \\[ c\\left(p\\right)=\\left(1-p\\right)^{n}\\implies c\\left(\\eta\\right)=\\left(1-\\frac{\\mathrm{e}^{\\eta}}{1+\\mathrm{e}^{\\eta}}\\right)^{n}=\\left(\\frac{1}{1+\\mathrm{e}^{\\eta}}\\right)^{n} \\] and \\[ f\\left(x|\\eta\\right) =\\binom{n}{x}\\left(\\frac{1}{1+\\mathrm{e}^{\\eta}}\\right)^{n}\\exp\\left(x\\eta\\right). \\] Example 5.8 (Poisson random variables) Express the pmf of \\(X\\sim\\text{Poisson}\\left(\\lambda\\right)\\) using a natural parameterization. From Example 5.3, the pmf of \\(X\\) can be written as \\[ f\\left(x|\\lambda\\right) =\\frac{1}{x!}\\mathrm{e}^{-\\lambda}\\exp\\left\\{ x\\log\\lambda\\right\\} , \\] where \\(k=1\\) and \\(\\omega_{1}\\left(\\lambda\\right) =\\log\\lambda\\). Then, let \\(\\eta=\\omega_{1}\\left(\\lambda\\right)\\), so that \\[ \\eta=\\log\\lambda\\implies\\mathrm{e}^{\\eta}=\\exp\\left(\\log\\lambda\\right)\\implies\\mathrm{e}^{\\eta}=\\lambda. \\] Then, we have \\(c\\left(\\lambda\\right)=\\mathrm{e}^{-\\lambda}\\implies c\\left(\\eta\\right)=\\exp\\left(-\\mathrm{e}^{\\eta}\\right)\\) and \\[ f\\left(x|\\eta\\right) =\\frac{1}{x!}\\exp\\left(-\\mathrm{e}^{\\eta}\\right)\\exp\\left(x\\eta\\right). \\] Example 5.9 (Bernoulli random variables) Express the pmf of \\(X\\sim\\text{Bernoulli}\\left(p\\right)\\) using a natural parameterization. Noting that \\(X\\sim\\text{Binomial}\\left(1,p\\right)\\), from Example 5.7, we have \\[ f\\left(x|\\eta\\right) =\\binom{n}{x}\\left(\\frac{1}{1+\\mathrm{e}^{\\eta}}\\right)^{n}\\exp\\left(x\\eta\\right). \\] With \\(n=1\\), we have \\[ f\\left(x|\\eta\\right)=\\binom{1}{x}\\left(\\frac{1}{1+\\mathrm{e}^{\\eta}}\\right)\\exp\\left(x\\eta\\right)=1\\cdot\\left(\\frac{1}{1+\\mathrm{e}^{\\eta}}\\right)\\exp\\left(x\\eta\\right)=\\frac{1}{1+\\mathrm{e}^{\\eta}}\\exp\\left(x\\eta\\right). \\] Example 5.10 (Normal random variables) Express the pdf of \\(X\\sim\\mathcal{N}\\left(\\mu,\\sigma^{2}\\right)\\) using a natural parameterization, where \\(\\sigma&gt;0\\) is unknown. From Example 5.4, we have \\[ f\\left(x|\\mu,\\sigma^{2}\\right) =\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ -\\frac{\\mu^{2}}{2\\sigma^{2}}-\\frac{1}{2}\\log\\sigma^{2}\\right\\} \\exp\\left\\{ -x^{2}\\frac{1}{2\\sigma^{2}}+x\\frac{\\mu}{\\sigma^{2}}\\right\\} . \\] Then, let \\(\\eta_{1}=\\omega_{1}\\left(\\mu,\\sigma^{2}\\right)\\), so that \\[ \\eta_{1}=\\frac{1}{\\sigma^{2}}\\implies\\sigma^{2}\\eta_{1}=1\\implies\\sigma^{2}=\\frac{1}{\\eta_{1}} \\] and let \\(\\eta_{2}=\\omega_{2}\\left(\\mu,\\sigma^{2}\\right)\\), so that \\[ \\eta_{2}=\\frac{\\mu}{\\sigma^{2}}\\implies\\mu=\\sigma^{2}\\eta_{2}=\\frac{\\eta_{2}}{\\eta_{1}}. \\] Then, we have \\[\\begin{align*} c\\left(\\mu,\\sigma^{2}\\right) &amp; =\\exp\\left\\{ -\\frac{\\mu^{2}}{2\\sigma^{2}}-\\frac{1}{2}\\log\\sigma^{2}\\right\\} \\\\ \\implies c^{*}\\left(\\eta_{1},\\eta_{2}\\right) &amp; =\\exp\\left\\{ -\\frac{\\left(\\frac{\\eta_{2}}{\\eta_{1}}\\right)^{2}}{2\\left(\\frac{1}{\\eta_{1}}\\right)}-\\frac{1}{2}\\log\\frac{1}{\\eta_{1}}\\right\\} \\\\ &amp; =\\exp\\left\\{ -\\frac{\\frac{\\eta_{2}^{2}}{\\eta_{1}^{2}}}{\\frac{2}{\\eta_{1}}}-\\frac{1}{2}\\log\\frac{1}{\\eta_{1}}\\right\\} \\\\ &amp; =\\exp\\left\\{ -\\frac{\\eta_{2}^{2}}{2\\eta_{1}}+\\log\\left(\\frac{1}{\\eta_{1}}\\right)^{-1/2}\\right\\} \\\\ &amp; =\\exp\\left\\{ -\\frac{\\eta_{2}^{2}}{2\\eta_{1}}+\\log\\left(\\left(\\eta_{1}\\right)^{-1}\\right)^{-1/2}\\right\\} \\\\ &amp; =\\exp\\left\\{ -\\frac{\\eta_{2}^{2}}{2\\eta_{1}}+\\log\\sqrt{\\eta_{1}}\\right\\} \\end{align*}\\] and \\[ f\\left(x|\\eta_{1},\\eta_{2}\\right) =\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{ -\\frac{\\eta_{2}^{2}}{2\\eta_{1}}+\\log\\sqrt{\\eta_{1}}\\right\\} \\exp\\left\\{ -\\frac{\\eta_{1}x^{2}}{2}+\\eta_{2}x\\right\\} . \\] Theorem 5.2 Let \\(X\\) have density in an exponential family. Then, \\(\\E\\left[t_{j}\\left(X\\right)\\right]=-\\dfrac{\\partial}{\\partial\\eta_{j}}\\log c^{*}\\left(\\eta\\right)\\), \\(\\Var\\left(t_{j}\\left(X\\right)\\right)=-\\dfrac{\\partial^{2}}{\\partial\\eta_{j}^{2}}\\log c^{*}\\left(\\eta\\right)\\), and the moment-generating function for \\(\\left(X_{1},\\ldots,X_{k}\\right)\\) is \\[ M_{\\left(X_{1},\\ldots,X_{k}\\right)}\\left(s_{1},\\ldots,s_{k}\\right)=\\E\\left[\\exp\\left\\{\\sum_{j=1}^{k}s_{j}X_{j}\\right\\}\\right]. \\] Proof. We begin with the pdf of an exponential family, i.e., \\[\\begin{align*} 1 &amp; =\\int f\\left(x|\\theta\\right)\\dif x \\\\ &amp; =\\int h\\left(x\\right)c\\left(\\theta\\right)\\exp\\left(\\sum_{i=1}^{k}\\omega_{i}\\left(\\theta\\right)t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x, \\end{align*}\\] where the second equality follows because \\(f\\) is in an exponential family, and where the third equality is the natural parameterization of \\(f\\). Taking the derivative of both sides with repect to \\(\\eta_{j}\\) gives \\[\\begin{align*} \\frac{\\partial}{\\partial\\eta_{j}}1 &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ \\implies0 &amp; =\\int\\frac{\\partial}{\\partial\\eta_{j}}\\left[h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\right]\\dif x \\\\ &amp; =\\int\\left[h\\left(x\\right)\\left[\\frac{\\partial}{\\partial\\eta_{j}}\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)+c^{*}\\left(\\eta\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\right]\\right]\\dif x \\\\ &amp; =\\int h\\left(x\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; \\quad+\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\int h\\left(x\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; \\quad+\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\left(\\sum_{i=1}^{k}\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\int h\\left(x\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x+\\E\\left[\\sum_{i=1}^{k}\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{i}t_{i}\\left(X\\right)\\right], \\end{align*}\\] where the final equality follows from the definition of expected value. Observe that for some differentiable function \\(g\\left(x\\right)\\), we have \\[ g&#39;\\left(x\\right)=\\frac{g\\left(x\\right)}{g\\left(x\\right)}g&#39;\\left(x\\right)=g\\left(x\\right)\\frac{\\dif}{\\dif x}\\log\\left(g\\left(x\\right)\\right), \\] which leads to \\[\\begin{align*} 0 &amp; =\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\log\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x+\\E\\left[\\sum_{i=1}^{k}\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{i}t_{i}\\left(X\\right)\\right] \\\\ &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\left(\\log c^{*}\\left(\\eta\\right)\\right)\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x+\\E\\left[\\sum_{i=1}^{k}\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{i}t_{i}\\left(X\\right)\\right] \\\\ &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\left(\\log c^{*}\\left(\\eta\\right)\\right)\\cdot1+\\E\\left[\\sum_{i=1}^{k}\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{i}t_{i}\\left(X\\right)\\right], \\end{align*}\\] where the final equality follows from the fact that the integral of a pdf over its range of positivity is equal to \\(1\\). Then, \\[\\begin{align*} -\\frac{\\partial}{\\partial\\eta_{j}}\\log c^{*}\\left(\\eta\\right) &amp; =\\E\\left[\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{1}t_{1}\\left(X\\right)+\\ldots+\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{j}t_{j}\\left(X\\right)+\\ldots+\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{k}t_{k}\\left(X\\right)\\right] \\\\ &amp; =\\E\\left[0\\cdot t_{1}\\left(X\\right)+\\ldots+1\\cdot t_{j}\\left(X\\right)+\\ldots+0\\cdot t_{k}\\left(X\\right)\\right] \\\\ &amp; =\\E\\left[t_{j}\\left(X\\right)\\right], \\end{align*}\\] proving the first claim. Then, \\[\\begin{align*} -\\frac{\\partial^{2}}{\\partial\\eta_{j}^{2}}\\log c^{*}\\left(\\eta\\right) &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\left(-\\frac{\\partial}{\\partial\\eta_{j}}\\log c^{*}\\left(\\eta\\right)\\right) \\\\ &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\E\\left[t_{j}\\left(X\\right)\\right] \\\\ &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\int t_{j}\\left(x\\right)h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\int t_{j}\\left(x\\right)h\\left(x\\right)\\frac{\\partial}{\\partial\\eta_{j}}c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\int t_{j}\\left(x\\right)h\\left(x\\right)\\left[\\frac{\\partial}{\\partial\\eta_{j}}\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)+c^{*}\\left(\\eta\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\right]\\dif x \\\\ &amp; =\\int t_{j}\\left(x\\right)h\\left(x\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; \\quad+\\int t_{j}\\left(x\\right)h\\left(x\\right)c^{*}\\left(\\eta\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x. \\end{align*}\\] The first summand becomes \\[\\begin{align*} &amp; \\quad\\,\\int t_{j}\\left(x\\right)h\\left(x\\right)c^{*}\\left(\\eta\\right)\\frac{\\partial}{\\partial\\eta_{j}}\\log\\left(c^{*}\\left(\\eta\\right)\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\log\\left(c^{*}\\left(\\eta\\right)\\right)\\int t_{j}\\left(x\\right)h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\frac{\\partial}{\\partial\\eta_{j}}\\log\\left(c^{*}\\left(\\eta\\right)\\right)\\E\\left[t_{j}\\left(X\\right)\\right] \\\\ &amp; =\\left(-\\E\\left[t_{j}\\left(X\\right)\\right]\\right)\\E\\left[t_{j}\\left(X\\right)\\right] \\\\ &amp; =-\\left(\\E\\left[t_{j}\\left(X\\right)\\right]\\right)^{2}, \\end{align*}\\] where the penultimate equality follows from the first part of the proof. The second summand becomes \\[\\begin{align*} &amp; \\quad\\,\\int t_{j}\\left(x\\right)h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\left(\\sum_{i=1}^{k}\\frac{\\partial}{\\partial\\eta_{j}}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\int t_{j}\\left(x\\right)h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\left(\\frac{\\partial}{\\partial\\eta_{1}}n_{1}t_{1}\\left(x\\right)+\\cdots+\\frac{\\partial}{\\partial\\eta_{k}}n_{k}t_{k}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\int t_{j}\\left(x\\right)h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\left(0+\\cdots+1\\cdot t_{j}\\left(x\\right)+\\cdots+0\\right)\\dif x \\\\ &amp; =\\int\\left(t_{j}\\left(x\\right)\\right)^{2}h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left(\\sum_{i=1}^{k}\\eta_{i}t_{i}\\left(x\\right)\\right)\\dif x \\\\ &amp; =\\E\\left[\\left(t_{j}\\left(X\\right)\\right)^{2}\\right]. \\end{align*}\\] For some random variable \\(Y\\) with defined second central moment, we have \\[\\begin{align*} \\Var\\left(Y\\right) &amp; =\\E\\left[\\left(Y-\\E\\left[Y\\right]\\right)^{2}\\right] \\\\ &amp; =\\E\\left[Y^{2}-2Y\\E\\left[Y\\right]+\\left(\\E\\left[Y\\right]\\right)^{2}\\right] \\\\ &amp; =\\E\\left[Y^{2}\\right]-2\\E\\left[Y\\E\\left[Y\\right]\\right]+\\E\\left[\\left(\\E\\left[Y\\right]\\right)^{2}\\right] \\\\ &amp; =\\E\\left[Y^{2}\\right]-2\\E\\left[Y\\right]\\E\\left[Y\\right]+\\left(\\E\\left[Y\\right]\\right)^{2}\\tag{$\\E\\left[Y\\right]$ is constant} \\\\ &amp; =\\E\\left[Y^{2}\\right]-2\\left(\\E\\left[Y\\right]\\right)^{2}+\\left(\\E\\left[Y\\right]\\right)^{2} \\\\ &amp; =\\E\\left[Y^{2}\\right]-\\left(\\E\\left[Y\\right]\\right)^{2}. \\end{align*}\\] It follows that \\[ -\\frac{\\partial^{2}}{\\partial\\eta_{j}^{2}}\\log c^{*}\\left(\\eta\\right)=-\\left(\\E\\left[t_{j}\\left(X\\right)\\right]\\right)^{2}+\\E\\left[\\left(t_{j}\\left(X\\right)\\right)^{2}\\right]=\\Var\\left(t_{j}\\left(X\\right)\\right), \\] proving the second claim. Example 5.11 (Expected value of a binomial random variable) Find the expected value of \\(X\\sim\\text{Binomial}\\left(n,p\\right)\\). We will find \\(\\E\\left[X\\right]\\) by applying Theorem 5.2. From Example 5.7, the pmf of \\(X\\) is given by \\[ f\\left(x|\\eta\\right) =\\binom{n}{x}\\left(\\frac{1}{1+\\mathrm{e}^{\\eta}}\\right)^{n}\\exp\\left(x\\eta\\right), \\] where \\(\\eta=\\log\\left(p/\\left(1-p\\right)\\right)\\), so that \\(p=1/\\left(1+\\mathrm{e}^{\\eta}\\right)\\). From the general form of a natural parameterization, we have \\(k=1\\), \\(t\\left(x\\right)=x\\), and \\(c^{*}\\left(\\eta\\right)=\\left(1/\\left(1+\\mathrm{e}^{\\eta}\\right)\\right)^{n}\\). Then, we have \\[\\begin{align*} \\E\\left[X\\right] &amp; =\\E\\left[t\\left(X\\right)\\right] \\\\ &amp; =-\\frac{\\partial}{\\partial\\eta}\\log\\left(\\frac{1}{1+\\mathrm{e}^{\\eta}}\\right)^{n} \\\\ &amp; =-\\frac{\\partial}{\\partial\\eta}\\log\\left(1+\\mathrm{e}^{\\eta}\\right)^{-n} \\\\ &amp; =-\\frac{\\partial}{\\partial\\eta}\\left(-n\\log\\left(1+\\mathrm{e}^{\\eta}\\right)\\right) \\\\ &amp; =n\\frac{\\partial}{\\partial\\eta}\\log\\left(1+\\mathrm{e}^{\\eta}\\right) \\\\ &amp; =n\\frac{\\mathrm{e}^{\\eta}}{1+\\mathrm{e}^{\\eta}} \\\\ &amp; =np. \\end{align*}\\] Theorem 5.3 If \\(X\\) has a \\(k\\)-parameter exponential family distribution indexed by the natural parameters, then for any \\(\\eta\\) on the interior of the natural parameter space, the mgf of \\(\\left(t_{1}\\left(X\\right),\\ldots,t_{k}\\left(X\\right)\\right)\\) exists and is given by \\[ M_{\\left(t_{1}\\left(X\\right),\\ldots,t_{k}\\left(X\\right)\\right)}\\left(s_{1},\\ldots,s_{k}\\right) =\\frac{c^{*}\\left(\\eta\\right)}{c^{*}\\left(\\eta+s\\right)} \\] where \\(\\eta+s\\) is the vector \\(\\left(\\eta_{1}+s_{1},\\ldots,\\eta_{k}+s_{k}\\right)\\). Proof. Suppose that \\(X\\) is a \\(k\\)-parameter exponential family distribution indexed by the natural parameters. Then, from Section 5.1.1, it has a pdf given by \\[ f\\left(x|\\eta\\right) =h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\eta_{j}t_{j}\\left(x\\right)\\right\\} . \\] It follows from Theorem 5.2 that \\[ M_{\\left(t_{1}\\left(X\\right),\\ldots,t_{k}\\left(X\\right)\\right)}\\left(s_{1},\\ldots,s_{k}\\right)=\\E\\left[\\mathrm{e}^{\\sum_{j=1}^{k}s_{j}t_{j}\\left(X\\right)}\\right], \\] with \\(X_{i}\\) replaced by \\(t_{i}\\left(X\\right)\\). Then, we have \\[\\begin{align*} \\E\\left[\\mathrm{e}^{\\sum_{j=1}^{k}s_{j}t_{j}\\left(X\\right)}\\right] &amp; =\\int\\exp\\left\\{ \\sum_{j=1}^{k}s_{j}t_{j}\\left(x\\right)\\right\\} h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\eta_{j}t_{j}\\left(x\\right)\\right\\} \\dif x \\\\ &amp; =\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}s_{j}t_{j}\\left(x\\right)+\\sum_{j=1}^{k}\\eta_{j}t_{j}\\left(x\\right)\\right\\} \\dif x \\\\ &amp; =\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\left(s_{j}+\\eta_{j}\\right)t_{j}\\left(x\\right)\\right\\} \\dif x \\\\ &amp; =\\frac{c^{*}\\left(\\eta+s\\right)}{c^{*}\\left(\\eta+s\\right)}\\int h\\left(x\\right)c^{*}\\left(\\eta\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\left(s_{j}+\\eta_{j}\\right)t_{j}\\left(x\\right)\\right\\} \\dif x \\\\ &amp; =\\frac{c^{*}\\left(\\eta\\right)}{c^{*}\\left(\\eta+s\\right)}\\int h\\left(x\\right)c^{*}\\left(\\eta+s\\right)\\exp\\left\\{ \\sum_{j=1}^{k}\\left(s_{j}+\\eta_{j}\\right)t_{j}\\left(x\\right)\\right\\} \\dif x \\\\ &amp; =\\frac{c^{*}\\left(\\eta\\right)}{c^{*}\\left(\\eta+s\\right)}\\cdot\\int f\\left(x|\\eta+s\\right)\\dif x \\\\ &amp; =\\frac{c^{*}\\left(\\eta\\right)}{c^{*}\\left(\\eta+s\\right)}\\cdot1 \\\\ &amp; =\\frac{c^{*}\\left(\\eta\\right)}{c^{*}\\left(\\eta+s\\right)}, \\end{align*}\\] establishing the claim. Definition 5.2 A curved exponential family is a family of densities of the form given in Section 5.1 for which the dimension of the vector \\(\\boldsymbol{\\theta}\\) is equal to \\(d&lt;k\\), where \\(k\\) is the number of terms in the sum in the exponent. If \\(d=k\\), the family is a full exponential family. 5.2 Location and scale families Location families, scale families, and location-scale families are constructed by specifying a single pdf, \\(f\\left(x\\right)\\), called the standard pdf for the family. Then, all other pdfs in the family are generated by transforming the standard pdf in a prescribed way. 5.2.1 Location families Definition 5.3 Let \\(f\\left(x\\right)\\) be any pdf. Then, the family of pdfs indexed by \\(\\mu\\), \\(f\\left(x-\\mu\\right)\\), is called the location family with respect to the standard pdf \\(f\\), and \\(\\mu\\) is called the location parameter. For example, \\(f\\left(x\\right)\\sim\\mathcal{N}\\left(0,1^{2}\\right)\\), \\(\\mathcal{N}\\left(\\mu,1^{2}\\right)\\) is a location family. The location parameter \\(\\mu\\) simply shifts the pdf \\(f\\left(x\\right)\\) so that the shape of the graph is unchanged but the point on the graph that was above \\(x=0\\) under \\(f\\left(x\\right)\\) is above \\(x=\\mu\\) for \\(f\\left(x-\\mu\\right)\\), thus \\[ P\\left(\\left\\{ -1\\leq X\\leq2|X\\sim f\\left(x\\right)\\right\\} \\right) =P\\left(\\left\\{ \\mu-1\\leq X\\leq\\mu+2|X\\sim f\\left(x-\\mu\\right)\\right\\} \\right). \\] Figure 5.1 shows the normal distribution with \\(\\sigma^{2}=1^{2}\\) and \\(\\mu\\in\\left\\{-2,0,2\\right\\}\\) in green, blue, and red, respectively. Figure 5.1: example of a normal location family 5.2.2 Scale families Definition 5.4 Let \\(f\\left(x\\right)\\) be any pdf. Then, for any \\(\\sigma&gt;0\\), the family of pdfs \\(\\left(1/\\sigma\\right)f\\left(x/\\sigma\\right)\\), indexed by the parameter \\(\\sigma\\), is called the scale family with standard pdf \\(f\\left(x\\right)\\) and \\(\\sigma\\) is called the scale parameter of the family. For example, \\(f\\left(x\\right)\\sim\\mathcal{N}\\left(0,1^{2}\\right)\\), \\(\\mathcal{N}\\left(0,\\sigma^{2}\\right)\\) is a scale family. The effect of introducing the scale parameter \\(\\sigma\\) is either to stretch (\\(\\sigma&gt;1\\)) or to contract (\\(\\sigma&lt;1\\)) the graph of \\(f\\left(x\\right)\\) while still maintaining the same basic shape of the graph. Figure 5.2 shows the normal distribution with \\(\\mu=0\\) and \\(\\sigma^{2}\\in\\left\\{0.75^{2},1^{2},1.5^{2}\\right\\}=1^{2}\\) green, red, and blue, respectively. Figure 5.2: example of a normal scale family "],
["em-algorithm.html", "Chapter 6 EM algorithm 6.1 Motivation 6.2 EM algorithm 6.3 Example: Gaussian mixture 6.4 Factor analysis", " Chapter 6 EM algorithm 6.1 Motivation Suppose that we measure the height and weight of a sample of adult men, adult women, and children. Let \\(\\mathbf{w}^{\\left(i\\right)}\\in\\mathbb{R}^{2}\\) be the height and weight of the \\(i\\text{th}\\) person in the sample. Suppose that our data become corrupted, and we lose the age and gender for each \\(\\mathbf{w}^{\\left(i\\right)}\\), i.e., we do not know whether the \\(i\\text{th}\\) subject was an adult man, an adult woman, or a child. Then, a natural goal would be to attempt to classify the data points to regain this information. Let \\(\\bar{\\mathbf{w}}_{1}\\), \\(\\bar{\\mathbf{w}}_{2}\\), and \\(\\bar{\\mathbf{w}}_{3}\\) be the means for the men, women, and children, respectively (and recall that each \\(\\bar{\\mathbf{w}}_{i}\\) is a point in \\(\\mathbb{R}^{2}\\)). Because our data were corrupted, we do not know the \\(\\bar{\\mathbf{w}}_{i}\\) a priori. Our task then is to choose each \\(\\bar{\\mathbf{w}}_{i}\\) and to assign each sample \\(\\mathbf{w}^{\\left(i\\right)}\\) to a class (man, woman, child), so that \\[ a\\left(i\\right)=\\begin{cases} 1, &amp; \\text{if the }i\\text{th sample is a man}\\\\ 2, &amp; \\text{if the }i\\text{th sample is a woman}\\\\ 3, &amp; \\text{if the }i\\text{th sample is a child} \\end{cases}. \\] Suppose further that we wish to choose the \\(\\bar{\\mathbf{w}}_{i}\\) and make the assignments \\(a\\left(i\\right)\\) in some principled fashion. Let the error associated with the assignments be \\[ \\mathrm{Err}\\left(\\bar{\\mathbf{w}}_{1},\\bar{\\mathbf{w}}_{2},\\bar{\\mathbf{w}}_{3},\\mathbf{a}\\right)= \\sum_{i=1}^{N}\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{a\\left(i\\right)}\\right\\Vert^{2}, \\] where \\(\\mathbf{a}=\\left(a\\left(1\\right),\\ldots,a\\left(N\\right)\\right)\\). We can interpret the error as the sum of the squared assignment errors, where the \\(i\\text{th}\\) assignment error is the distance between \\(\\mathbf{w}^{\\left(i\\right)}\\) and the mean of the group to which it is assigned. Our goal then is to minimize the assignment error. 6.1.1 \\(k\\)-means Observe that \\(\\bar{\\mathbf{w}}_{i}\\in\\mathbb{R}^{2}\\), and that \\(\\mathbf{a}\\in\\left\\{1,2,3\\right\\}^{N}\\), i.e., \\(\\mathbf{a}\\) is a vector of length \\(N\\) whose possible values are 1, 2, and 3. It is not immediately clear that techniques previously introduced for optimization in \\(\\mathbb{R}^{d}\\) will be helpful given the discrete nature of \\(\\mathbf{a}\\). We now introduce a new classification algorithm, \\(k\\)-means. Choose initial assignments \\(\\mathbf{a}^{\\left(0\\right)}\\) at random. Given \\(\\mathbf{a}^{\\left(0\\right)}\\), calculate new means \\(\\bar{\\mathbf{w}}_{i}^{\\left(1\\right)}\\). Given \\(\\bar{\\mathbf{w}}_{i}^{\\left(1\\right)}\\), calculate new assignments \\(\\mathbf{a}^{\\left(1\\right)}\\). Repeat steps 2-3 until convergence (the assignments do not change from one iteration to the next). We now consider whether this algorithm will, in fact, converge. Recall that a descent algorithm applied to some objective function \\(f\\left(x\\right)\\) satisfies \\(f\\left(x^{\\left(1\\right)}\\right)\\geq f\\left(x^{\\left(2\\right)}\\right)\\geq f\\left(x^{\\left(3\\right)}\\right)\\geq\\cdots\\). Consider step 2 of the algorithm, where we calculate the group means given the assignments. Let \\(\\mathcal{M}\\) be the set of all men, and suppose that \\(\\left|\\mathcal{M}\\right|=M\\), i.e., \\(M\\) of the samples are men. Then, we can minimize the error for the men by solving \\[ \\min_{\\bar{\\mathbf{w}}_{1}}\\sum_{i\\in\\mathcal{M}}\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}. \\] To ease notation, and without loss of generality, suppose that the samples are ordered such that the first \\(M\\) correspond to the men, so that we can simply evaluate the above sum from \\(i=1\\) to \\(M\\). We will now minimize this analytically. Letting \\[ Q\\left(\\bar{\\mathbf{w}}_{1}\\right)=\\sum_{i=1}^{M}\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}, \\] note that we can minimize the error by solving \\[ \\mathbf{0} = \\nabla Q\\left(\\bar{\\mathbf{w}}_{1}\\right) = \\nabla\\left(\\sum_{i=1}^{M}\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}\\right) = \\sum_{i=1}^{M}\\nabla\\left(\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}\\right), \\] where we have used the linearity of the gradient. Next, observe that \\[ \\begin{align*} \\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2} &amp; = \\left(\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right)^{\\mathsf{T}}\\left(\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right) \\\\ &amp; = \\left(\\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}-\\bar{\\mathbf{w}}_{1}^{\\mathsf{T}}\\right)\\left(\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right) \\\\ &amp; = \\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\mathbf{w}^{\\left(i\\right)}-\\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\bar{\\mathbf{w}}_{1}-\\bar{\\mathbf{w}}_{1}^{\\mathsf{T}}\\mathbf{w}^{\\left(i\\right)}+\\bar{\\mathbf{w}}_{1}^{\\mathsf{T}}\\bar{\\mathbf{w}}_{1}. \\end{align*} \\] Now, \\(\\bar{\\mathbf{w}}_{1}^{\\mathsf{T}}\\mathbf{w}^{\\left(i\\right)}\\) is a scalar, hence is equal to its transpose, so it follows that \\[ \\begin{align*} \\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2} &amp; = \\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\mathbf{w}^{\\left(i\\right)}-2\\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\bar{\\mathbf{w}}_{1}+\\bar{\\mathbf{w}}_{1}^{\\mathsf{T}}\\bar{\\mathbf{w}}_{1} \\\\ &amp; = \\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\mathbf{w}^{\\left(i\\right)}-2\\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\bar{\\mathbf{w}}_{1}+\\bar{\\mathbf{w}}_{1}^{\\mathsf{T}}\\mathbf{I}_{2}\\bar{\\mathbf{w}}_{1}, \\end{align*} \\] where \\(\\mathbf{I}_{k}\\) is the \\(k\\)-dimensional identity matrix. Let \\(\\mathbf{A}=\\mathbf{I}_{2}\\), let \\(\\mathbf{b}=-2\\mathbf{w}^{\\left(i\\right)}\\), and let \\(c=\\left(\\mathbf{w}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\mathbf{w}^{\\left(i\\right)}\\), so that \\[ \\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}=\\bar{\\mathbf{w}}_{1}^{\\mathsf{T}}\\mathbf{A}\\bar{\\mathbf{w}}_{1}+\\mathbf{b}^{\\mathsf{T}}\\bar{\\mathbf{w}}_{1}+c \\] which has the form of a general quadratic. PROVE THIS IN AN EARLIER SECTION It follows that the gradient of \\(\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}\\) is given by \\[ \\nabla\\left(\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}\\right)= 2\\mathbf{A}\\bar{\\mathbf{w}}_{1}+\\mathbf{b}=2\\mathbf{I}_{2}\\bar{\\mathbf{w}}_{1}-2\\mathbf{w}^{\\left(i\\right)}=2\\left(\\bar{\\mathbf{w}}_{1}-\\mathbf{w}^{\\left(i\\right)}\\right). \\] Thus, the minimization is solved by \\[ \\begin{align*} \\mathbf{0} &amp; = \\sum_{i=1}^{M}\\nabla\\left(\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{1}\\right\\Vert^{2}\\right) \\\\ &amp; = \\sum_{i=1}^{M}2\\left(\\bar{\\mathbf{w}}_{1}-\\mathbf{w}^{\\left(i\\right)}\\right) \\\\ &amp; = 2\\left(\\sum_{i=1}^{M}\\bar{\\mathbf{w}}_{1}-\\sum_{i=1}^{M}\\mathbf{w}^{\\left(i\\right)}\\right) \\\\ &amp; = 2\\left(M\\bar{\\mathbf{w}}_{1}-\\sum_{i=1}^{M}\\mathbf{w}^{\\left(i\\right)}\\right) \\\\ \\implies M\\bar{\\mathbf{w}}_{1} &amp; = \\sum_{i=1}^{M}\\mathbf{w}^{\\left(i\\right)} \\\\ \\implies\\hat{\\bar{\\mathbf{w}}}_{1} &amp; = \\frac{1}{M}\\sum_{i=1}^{M}\\mathbf{w}^{\\left(i\\right)}, \\end{align*} \\] i.e., the point that minimizes the error for the men is the mean (center of mass) of the observations (it is easy to show that this optimum is a minimum). By symmetry, the errors for the women and the children are minimized by \\(\\hat{\\bar{\\mathbf{w}}}_{2}\\) and \\(\\hat{\\bar{\\mathbf{w}}}_{3}\\), respectively. We now consider step 3 of the algorithm, where we choose assignments based on the group means just calculated. Observe that the overall error can be written as \\[ \\mathrm{Err}\\left(\\bar{\\mathbf{w}}_{1},\\bar{\\mathbf{w}}_{2},\\bar{\\mathbf{w}}_{3},\\mathbf{a}\\right)= \\sum_{i\\in\\mathcal{M}}\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{a\\left(i\\right)}\\right\\Vert^{2}+\\sum_{i\\in\\mathcal{W}}\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{a\\left(i\\right)}\\right\\Vert^{2}+\\sum_{i\\in\\mathcal{C}}\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{a\\left(i\\right)}\\right\\Vert^{2}, \\] where \\(\\mathcal{W}\\) and \\(\\mathcal{C}\\) are the sets of samples of women and children, respectively. We have shown that we can minimize the error for each group by setting \\(\\bar{\\mathbf{w}}_{a\\left(i\\right)}\\) to the respective group mean. It follows that we should assign the \\(i\\text{th}\\) sample to the closest group whose mean is closest, i.e., the group for which \\(\\left\\Vert\\mathbf{w}^{\\left(i\\right)}-\\bar{\\mathbf{w}}_{a\\left(i\\right)}\\right\\Vert^{2}\\) is smallest. We can therefore view the \\(k\\)-means algorithm as an alternating minimization. Having chosen initial assignments \\(\\mathbf{a}^{\\left(0\\right)}\\), we can view the algorithm as first solving the minimization \\[ \\hat{\\bar{\\mathbf{w}}}_{1},\\hat{\\bar{\\mathbf{w}}}_{2},\\hat{\\bar{\\mathbf{w}}}_{3}=\\min_{\\bar{\\mathbf{w}}_{1},\\bar{\\mathbf{w}}_{2},\\bar{\\mathbf{w}}_{3}}\\mathrm{Err}\\left(\\bar{\\mathbf{w}}_{1},\\bar{\\mathbf{w}}_{2},\\bar{\\mathbf{w}}_{3},\\mathbf{a}^{\\left(0\\right)}\\right), \\] which we have seen is solved by setting each \\(\\hat{\\bar{\\mathbf{w}}}_{i}\\) to the mean of the corresponding samples. We then solve the minimization \\[ \\hat{\\mathbf{a}}=\\min_{\\mathbf{a}}\\mathrm{Err}\\left(\\hat{\\bar{\\mathbf{w}}}_{1},\\hat{\\bar{\\mathbf{w}}}_{2},\\hat{\\bar{\\mathbf{w}}}_{3},\\mathbf{a}\\right), \\] which we have argued is solved by setting \\(a\\left(i\\right)\\) to the group whose mean is closest. It follows that \\[ \\mathrm{Err}\\left(\\hat{\\bar{\\mathbf{w}}}_{1},\\hat{\\bar{\\mathbf{w}}}_{2},\\hat{\\bar{\\mathbf{w}}}_{3},\\hat{\\mathbf{a}}\\right)\\leq\\mathrm{Err}\\left(\\bar{\\mathbf{w}}_{1},\\bar{\\mathbf{w}}_{2},\\bar{\\mathbf{w}}_{3},\\mathbf{a}^{\\left(0\\right)}\\right), \\] i.e., \\(k\\)-means is a descent algorithm, hence will converge (though possibly to a local minimum), and where \\(\\bar{\\mathbf{w}}_{i}\\) is calculated using the initial assignments. 6.2 EM algorithm 6.2.1 Algorithmic perspective We now introduce the expectation-maximization algorithm, which has a probabilistic component (unlike \\(k\\)-means). The EM algorithm was introduced in the landmark paper of Dempster, Laird, and Rubin (1977), which generalized methods that had previously been applied in a variety of special cases. We will initially take an algorithmic perspective. For simplicity, suppose that we now consider only the height of the samples, and suppose that the height of the \\(i\\text{th}\\) sample is drawn from the Gaussian mixture \\[ w\\sim \\begin{cases} \\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right), &amp; \\text{with probability }p_{1} &amp; \\text{(men)} \\\\ \\mathcal{N}\\left(\\mu_{2},\\sigma_{2}^{2}\\right), &amp; \\text{with probability }p_{2} &amp; \\text{(women)} \\\\ \\mathcal{N}\\left(\\mu_{3},\\sigma_{3}^{2}\\right), &amp; \\text{with probability }1-p_{1}-p_{2} &amp; \\text{(children)} \\end{cases}. \\] Then, the likelihood is a function of \\(\\boldsymbol{\\theta}=\\left(p_{1},p_{2},\\mu_{1},\\mu_{2},\\mu_{3},\\sigma_{1}^{2},\\sigma_{2}^{2},\\sigma_{3}^{2}\\right)\\). We would like to maximize the log-likelihood \\(\\ell\\left(\\boldsymbol{\\theta}\\right)\\) of the data (and the value of \\(\\boldsymbol{\\theta}\\) at the maximum will be our estimate for the parameters). This optimization will be very difficult (if not intractable) using techniques introduced thus far. Observe that the optimization would be easy if we knew the assignment (which samples are men, which are women, and which are children). We can regard these assignments \\(a\\left(i\\right)\\) as missing. Now, if we knew the assignments, then it would be easy to calculate \\(\\boldsymbol{\\theta}_{\\text{MLE}}\\). As before, consider the \\(M\\) samples for men. These are distributed according to \\(\\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right)\\), so it follows that PROVE THIS IN AN EARLIER SECTION \\[ \\hat{\\mu}_{\\text{MLE}}=\\frac{1}{M}\\sum_{i\\in\\mathcal{M}}w^{\\left(i\\right)}\\quad\\text{and}\\quad \\hat{\\sigma}_{\\text{MLE}}^{2}=\\frac{1}{M}\\sum_{i\\in\\mathcal{M}}\\left(w^{\\left(i\\right)}-\\hat{\\mu}_{\\text{MLE}}\\right)^2, \\] and \\(\\hat{p}_{1}=M/N\\) (the fraction of samples that are men). We can similarly determine the remaining parameters. Now, suppose that we know \\(\\boldsymbol{\\theta}\\), and we wish to make assignments. Recall that \\(w^{\\left(i\\right)}\\) is sampled from a Gaussian mixture. One possibility is to assign \\(w^{\\left(i\\right)}\\) to the component of the mixture model with the highest probability, known as a hard assignment (similar to \\(k\\)-means). Because this strategy does not respect the underlying probability distribution, such assignments will not maximimize the log-likelihood. Rather, we will make soft assignments, which allocate probability to the mixture components, and which represent the difference between \\(k\\)-means and the EM algorithm. In this framework, we consider \\[ P\\left(w^{\\left(i\\right)}\\text{ and }w^{\\left(i\\right)}\\text{ is a man}\\right)= P\\left(w^{\\left(i\\right)}\\text{ is a man}\\right)P\\left(w^{\\left(i\\right)}|w^{\\left(i\\right)}\\text{ is a man}\\right)= p_{1}\\cdot\\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right)\\left(w^{\\left(i\\right)}\\right), \\] where the first equality follows from conditional probability, and where \\(\\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right)\\) represents the density of a Gaussian evaluated at \\(w^{\\left(i\\right)}\\). We can apply the same reasoning to \\(P\\left(w^{\\left(i\\right)}\\text{ and }w^{\\left(i\\right)}\\text{ is a woman}\\right)\\) and \\(P\\left(w^{\\left(i\\right)}\\text{ and }w^{\\left(i\\right)}\\text{ is a child}\\right)\\). Then, we will assign the sample to the men with weight \\[ \\frac{P\\left(w^{\\left(i\\right)}\\text{ and man}\\right)}{P\\left(w^{\\left(i\\right)}\\text{ and man}\\right)+P\\left(w^{\\left(i\\right)}\\text{ and woman}\\right)+P\\left(w^{\\left(i\\right)}\\text{ and child}\\right)}, \\] where the denominator represents the total probability of the sample, and where by “man” we mean “\\(w^{\\left(i\\right)}\\) is a man.” Thus, the assignments are three numbers that sum to 1. Letting \\(q_{i}\\) be the fraction of the \\(i\\text{th}\\) sample assigned to the “men” class, we now consider how to calculate \\(\\boldsymbol{\\theta}\\). We have fractional samples, so that \\[ \\hat{\\mu}_{1}=\\frac{\\sum_{i=1}^{N}q_{i}w^{\\left(i\\right)}}{\\sum_{i=1}^{N}q_{i}}\\quad\\text{and}\\quad \\hat{\\sigma}_{1}^{2}=\\frac{1}{\\sum_{i=1}^{N}q_{i}}\\sum_{i=1}^{N}\\left(q_{i}w^{\\left(i\\right)}-\\hat{\\mu}_{1}\\right)^{2}, \\] and similarly, \\(\\hat{p}_{1}=\\sum_{i=1}^{N}q_{i}/N\\). (Observe that if \\(q_{i}=1\\) for all \\(i\\), then the above expressions simplify to the expressions for hard assignments.) This gives rise to the EM algorithm: Choose \\(\\boldsymbol{\\theta}^{\\left(0\\right)}\\), using prior information if possible. Given \\(\\boldsymbol{\\theta}^{\\left(0\\right)}\\), update the (soft) assignments. Given the soft assignments, update \\(\\boldsymbol{\\theta}\\). Repeat steps 2-3 until convergence (the assignments do not change from one iteration to the next by some desired tolerance). Note the similarity to \\(k\\)-means, though because it is difficult to randomly choose soft assignments, we begin by choosing \\(\\boldsymbol{\\theta}\\) rather than the assignments. 6.2.2 Statistical perspective We now present EM from a statistical perspective. We will reset notation to match that commonly used in the literature, denoting by \\(x^{\\left(i\\right)}\\) the observed data. Following our example above, suppose that each \\(x^{\\left(i\\right)}\\) is a sample from a Gaussian mixture model, which is again parameterized by \\(\\boldsymbol{\\theta}=\\left(p_{1},p_{2},\\mu_{1},\\mu_{2},\\mu_{3},\\sigma_{1}^{2},\\sigma_{2}^{2},\\sigma_{3}^{2}\\right)\\). We will denote by \\(z^{\\left(i\\right)}\\) the missing data, which in this example represents the Gaussian from which the \\(i\\text{th}\\) sample was drawn, so that \\(z^{\\left(i\\right)}\\in\\left\\{1,2,3\\right\\}\\). We refer to the combination of the observed data and the missing data as the complete data, sometimes written \\(y^{\\left(i\\right)}=\\left(x^{\\left(i\\right)},z^{\\left(i\\right)}\\right)\\). Then, the EM algorithm consists of two steps. 6.2.2.1 E step Givens and Hoeting (2012) describes the EM algorithm as seeking to iteratively maximize the log-likelihood of the observed data \\(\\ell\\left(\\boldsymbol{\\theta}|\\mathbf{x}\\right)\\) with respect to \\(\\boldsymbol{\\theta}\\). Let \\(f\\left(\\mathbf{z},\\mathbf{x}|\\boldsymbol{\\theta}\\right)\\) be the joint density of the complete data, with corresponding log-likelihood \\(\\ell\\left(\\boldsymbol{\\theta}|\\mathbf{z},\\mathbf{x}\\right)\\). Consider the expectation of \\(\\ell\\left(\\boldsymbol{\\theta}|\\mathbf{z},\\mathbf{x}\\right)\\) conditioned on the observed data \\(\\mathbf{x}\\), i.e., \\[ Q\\left(\\boldsymbol{\\theta}&#39;,\\boldsymbol{\\theta}^{\\left(t\\right)}\\right) = \\E_{\\mathbf{z}|\\mathbf{x},\\boldsymbol{\\theta}^{\\left(t\\right)}}\\left[\\ell\\left(\\boldsymbol{\\theta}&#39;|\\mathbf{z},\\mathbf{x}\\right)\\right] = \\E\\left[\\ell\\left(\\boldsymbol{\\theta}&#39;|\\mathbf{z},\\mathbf{x}\\right)|\\mathbf{x},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right], \\] where \\(\\boldsymbol{\\theta}^{\\left(t\\right)}\\) is the current maximizer of \\(\\ell\\), and where the first equality emphasizes that the expectation is taken with respect to \\(\\mathbf{z}\\) (though we will typically prefer the notation of the second equality). Then, we have \\[ \\begin{align*} Q\\left(\\boldsymbol{\\theta}&#39;,\\boldsymbol{\\theta}^{\\left(t\\right)}\\right) &amp; = \\E\\left[\\ell\\left(\\boldsymbol{\\theta}&#39;|\\mathbf{z},\\mathbf{x}\\right)|\\mathbf{x},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right] \\\\ &amp; = \\E\\left[\\sum_{i=1}^{N}\\log\\mathcal{L}\\left(\\boldsymbol{\\theta}&#39;|z^{\\left(i\\right)},x^{\\left(i\\right)}\\right)|\\mathbf{x},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right] \\\\ &amp; = \\E\\left[\\sum_{i=1}^{N}\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\boldsymbol{\\theta}&#39;\\right)|\\mathbf{x},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right]. \\end{align*} \\] The expectation step of the algorithm involves computing the above quantity, to which we shall return shortly. 6.2.2.2 M step In the maximization step, we will maximize \\(Q\\left(\\boldsymbol{\\theta}&#39;,\\boldsymbol{\\theta}^{\\left(t\\right)}\\right)\\) over \\(\\boldsymbol{\\theta}&#39;\\), setting \\(\\boldsymbol{\\theta}^{\\left(t+1\\right)}\\) equal to the maximizer of \\(Q\\), i.e., \\[ \\boldsymbol{\\theta}^{\\left(t+1\\right)}\\gets\\argmax_{\\boldsymbol{\\theta}&#39;}Q\\left(\\boldsymbol{\\theta}&#39;,\\boldsymbol{\\theta}^{\\left(t\\right)}\\right) \\] By alternating between the E and M steps, the algorithm maximizes \\(\\ell\\left(\\boldsymbol{\\theta}\\right)\\) as an ascent algorithm, i.e., \\(\\ell\\left(\\boldsymbol{\\theta}^{\\left(1\\right)}\\right)\\leq\\ell\\left(\\boldsymbol{\\theta}^{\\left(2\\right)}\\right)\\leq\\ell\\left(\\boldsymbol{\\theta}^{\\left(3\\right)}\\right)\\leq\\cdots\\). 6.2.2.3 Back to the E step It follows from the linearity of expectation that \\[ \\begin{align*} Q\\left(\\boldsymbol{\\theta}&#39;,\\boldsymbol{\\theta}^{\\left(t\\right)}\\right) &amp; = \\sum_{i=1}^{N}\\E\\left[\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\boldsymbol{\\theta}&#39;\\right)|\\mathbf{x},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right] \\\\ &amp; = \\sum_{i=1}^{N}\\sum_{z=1}^{3}\\log\\left(f\\left(z^{\\left(i\\right)}=z,x^{\\left(i\\right)}|\\boldsymbol{\\theta}&#39;\\right)\\right)P\\left(z^{\\left(i\\right)}=z\\right), \\end{align*} \\] where the second equality follows from the definition of expected value, and where \\(P\\left(z^{\\left(i\\right)}=z\\right)=P\\left(z^{\\left(i\\right)}=z|x^{\\left(i\\right)},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right)\\), which corresponds to the soft assignment. To see this, observe that \\[ P\\left(z^{\\left(i\\right)}=1|x^{\\left(i\\right)},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right) = \\frac{P\\left(z^{\\left(i\\right)}=1,x^{\\left(i\\right)}|\\boldsymbol{\\theta}^{\\left(t\\right)}\\right)}{P\\left(x^{\\left(i\\right)}|\\boldsymbol{\\theta}^{\\left(t\\right)}\\right)} = \\frac{p_{1}\\cdot\\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right)\\left(x^{\\left(i\\right)}\\right)}{\\sum_{z=1}^{3}P\\left(z^{\\left(i\\right)}=z,x^{\\left(i\\right)}|\\boldsymbol{\\theta}^{\\left(t\\right)}\\right)}, \\] where the notation \\(\\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right)\\left(x^{\\left(i\\right)}\\right)\\) represents evaluating the density of the specified Gaussian at \\(x^{\\left(i\\right)}\\), and where we have applied the law of total probability. The value of EM is that it separates \\(\\boldsymbol{\\theta}^{\\left(t\\right)}\\) and \\(\\boldsymbol{\\theta}&#39;\\), i.e., the maximization \\[ \\max_{\\boldsymbol{\\theta}^{\\left(t\\right)}}\\E\\left[\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\boldsymbol{\\theta}^{\\left(t\\right)}\\right)|x^{\\left(i\\right)},\\boldsymbol{\\theta}^{\\left(t\\right)}\\right] \\] is not tractable due to the circular interaction between \\(z^{\\left(i\\right)}\\) and \\(\\boldsymbol{\\theta}^{\\left(t\\right)}\\). 6.2.3 Proof sketch We now consider why EM works. Let \\(x^{\\left(1\\right)},\\ldots,x^{\\left(N\\right)}\\) be samples from \\(X\\left(\\theta\\right)\\). The generic likelihood problem is \\[ \\max_{\\theta}\\sum_{i=1}^{N}\\log f\\left(x^{\\left(i\\right)}|\\theta\\right), \\] where \\(f\\) is the density of \\(X\\left(\\theta\\right)\\). It it not at all clear how this is related to \\[ \\E\\left[\\sum_{i=1}^{N}\\log f\\left(x^{\\left(i\\right)},z^{\\left(i\\right)}|\\theta&#39;\\right)|x^{\\left(i\\right)},\\theta\\right], \\] and in fact \\[ \\log f\\left(x^{\\left(i\\right)}|\\theta\\right)\\neq\\E\\left[\\log f\\left(x^{\\left(i\\right)},z^{\\left(i\\right)}|\\theta&#39;\\right)|x^{\\left(i\\right)},\\theta\\right], \\] where we have moved the expectation inside the sum and dropped the summation notation for clarity. Let \\(z^{\\left(i\\right)}\\in\\left\\{1,2\\right\\}\\) be the missing data, e.g., suppose \\(z^{\\left(i\\right)}\\) represents the distribution from which \\(x^{\\left(i\\right)}\\) was drawn in a mixture model. We now consider \\[ \\begin{align*} \\log f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right) &amp; = \\log\\left(\\frac{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right)}{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)}\\right) \\\\ &amp; = \\sum_{z=1}^{2}\\log\\left(\\frac{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right)}{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)}\\right)P\\left(z^{\\left(i\\right)}=z|x^{\\left(i\\right)},\\theta\\right) \\\\ &amp; = \\sum_{z=1}^{2}\\left[\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)+\\log\\frac{f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right)}{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)}\\right]P\\left(z^{\\left(i\\right)}=z|x^{\\left(i\\right)},\\theta\\right) \\\\ &amp; = \\sum_{z=1}^{2}\\left[\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)+\\log\\left(\\frac{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)}{f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right)}\\right)^{-1}\\right]P\\left(z^{\\left(i\\right)}=z|x^{\\left(i\\right)},\\theta\\right) \\\\ &amp; = \\sum_{z=1}^{2}\\left[\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)-\\log\\frac{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)}{f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right)}\\right]P\\left(z^{\\left(i\\right)}=z|x^{\\left(i\\right)},\\theta\\right) \\\\ &amp; = \\sum_{z=1}^{2}\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)P\\left(z^{\\left(i\\right)}=z|x^{\\left(i\\right)},\\theta\\right) \\\\ &amp; \\quad-\\sum_{z=1}^{2}\\log\\frac{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)}{f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right)}P\\left(z^{\\left(i\\right)}=z|x^{\\left(i\\right)},\\theta\\right) \\\\ &amp; = \\E\\left[\\log f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)|x^{\\left(i\\right)},\\theta\\right]-\\E\\left[\\log\\frac{f\\left(z^{\\left(i\\right)},x^{\\left(i\\right)}|\\theta&#39;\\right)}{f\\left(x^{\\left(i\\right)}|\\theta&#39;\\right)}|x^{\\left(i\\right)},\\theta\\right] \\\\ &amp; = Q\\left(\\theta&#39;,\\theta\\right)-\\E\\left[\\log f\\left(z^{\\left(i\\right)}|x^{\\left(i\\right)},\\theta&#39;\\right)|x^{\\left(i\\right)},\\theta\\right] \\\\ &amp; = Q\\left(\\theta&#39;,\\theta\\right)-H\\left(\\theta&#39;,\\theta\\right). \\end{align*} \\] Thus, we see that the log-likelihood can be written in terms of the \\(Q\\left(\\theta&#39;,\\theta\\right)\\) from the E step and a new function \\(H\\left(\\theta&#39;,\\theta\\right)\\). We now consider the M step. Now, it is a consequence of Theorem 3.18 that \\(H\\left(\\theta,\\theta\\right)\\geq H\\left(\\theta&#39;,\\theta\\right)\\). Thus, by choosing \\(\\theta^{\\left(t+1\\right)}\\) to maximize \\(Q\\left(\\theta&#39;,\\theta^{\\left(t\\right)}\\right)\\) over \\(\\theta&#39;\\), we are guaranteed that \\(H\\left(\\theta&#39;,\\theta^{\\left(t\\right)}\\right)\\) will be smaller than \\(H\\left(\\theta^{\\left(t\\right)},\\theta^{\\left(t\\right)}\\right)\\). It follows that \\[ \\log f\\left(x^{\\left(i\\right)}|\\theta^{\\left(t+1\\right)}\\right)\\geq\\log f\\left(x^{\\left(i\\right)}|\\theta^{\\left(t\\right)}\\right), \\] hence that EM is an ascent algorithm. 6.3 Example: Gaussian mixture We now return to our Gaussian mixture problem. For simplicity, we will consider only the men and women, i.e., we consider a mixture of two Gaussians, so that \\[ w\\sim \\begin{cases} \\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right), &amp; \\text{with probability }p_{1} &amp; \\text{(men)} \\\\ \\mathcal{N}\\left(\\mu_{2},\\sigma_{2}^{2}\\right), &amp; \\text{with probability }(1-p_{1}) &amp; \\text{(women)} \\end{cases}, \\] with corresponding density \\[ f\\left(w|\\boldsymbol{\\theta}\\right)=p_{1}\\phi_{\\boldsymbol{\\theta}_{1}}\\left(w\\right)+\\left(1-p_{1}\\right)\\phi_{\\boldsymbol{\\theta}_{2}}\\left(w\\right), \\] where \\(\\phi_{\\boldsymbol{\\theta}}\\left(w\\right)\\) is the Gaussian density parameterized by \\(\\boldsymbol{\\theta}\\) evaluated at \\(w\\). Thus, the log-likelihood is \\[ \\begin{align*} \\ell\\left(\\boldsymbol{\\theta}|\\mathbf{w}\\right) &amp; =\\log\\mathcal{L}\\left(\\boldsymbol{\\theta}|\\mathbf{w}\\right)\\\\ &amp; =\\log\\prod_{i=1}^{N}\\left[p_{1}\\phi_{\\boldsymbol{\\theta}_{1}}\\left(w_{i}\\right)+\\left(1-p_{1}\\right)\\phi_{\\boldsymbol{\\theta}_{2}}\\left(w_{i}\\right)\\right],\\\\ &amp; =\\sum_{i=1}^{N}\\log\\left[p_{1}\\phi_{\\boldsymbol{\\theta}_{1}}\\left(w_{i}\\right)+\\left(1-p_{1}\\right)\\phi_{\\boldsymbol{\\theta}_{2}}\\left(w_{i}\\right)\\right]. \\end{align*} \\] \\[ \\begin{align*} \\ell\\left(\\boldsymbol{\\theta}|\\mathbf{w}\\right)&amp; = \\log\\prod_{i=1}^{N}f\\left(w^{\\left(i\\right)}|\\boldsymbol{\\theta}\\right) \\\\ &amp; = \\sum_{i=1}^{N}\\log f\\left(w^{\\left(i\\right)}|\\boldsymbol{\\theta}\\right) \\\\ &amp; =\\sum_{i=1}^{N}\\log\\left(p_{1}\\phi_{\\boldsymbol{\\theta}_{1}}\\left(w\\right)+p_{2}\\phi_{\\boldsymbol{\\theta}_{2}}\\left(w\\right)+(1-p_{1}-p_{2})\\phi_{\\boldsymbol{\\theta}_{3}}\\left(w\\right)\\right) \\end{align*}. \\] Pretending that \\(\\boldsymbol{\\theta}\\) (and the class of each sample) is unknown, we define a random variable \\[ Z_{i}=\\begin{cases} 1, &amp; \\text{if the sample came from }\\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right)\\\\ 0, &amp; \\text{if the sample came from }\\mathcal{N}\\left(\\mu_{2},\\sigma_{2}^{2}\\right) \\end{cases} \\] to represent the hidden data (similar to the 3-Gaussian case). From Friedman, Hastie, and Tibshirani (2009), the complete-data log-likelihood is \\[ \\ell\\left(\\boldsymbol{\\theta}|\\mathbf{w},\\mathbf{z}\\right)=\\sum_{i=1}^{N}\\left[z_{i}\\log\\phi_{\\boldsymbol{\\theta}_{1}}\\left(w_{i}\\right)+\\left(1-z_{i}\\right)\\log\\phi_{\\boldsymbol{\\theta}_{2}}\\left(w_{i}\\right)\\right]+\\sum_{i=1}^{N}\\left[z_{i}\\log p_{1}+\\left(1-z_{i}\\right)\\log\\left(1-p_{1}\\right)\\right]. \\] The probability that \\(Z=1\\) conditioned on \\(W\\) is \\[ \\begin{align*} P\\left(\\left\\{ Z=1|W=w\\right\\} \\right) &amp; =\\frac{P\\left(\\left\\{ W=w|Z=1\\right\\} \\right)P\\left(\\left\\{ Z=1\\right\\} \\right)}{P\\left(\\left\\{ W=w\\right\\} \\right)} \\\\ &amp; =\\frac{\\phi_{\\boldsymbol{\\theta}_{1}}\\left(w\\right)\\cdot p_{1}}{p_{1}\\phi_{\\boldsymbol{\\theta}_{1}}\\left(w\\right)+\\left(1-p_{1}\\right)\\phi_{\\boldsymbol{\\theta}_{2}}\\left(w\\right)}. \\end{align*} \\] Let \\[ \\gamma_{i}\\left(\\boldsymbol{\\theta}\\right)= \\E\\left[Z_{i}|\\boldsymbol{\\theta},\\mathbf{W}\\right]= P\\left(\\left\\{ Z_{i}=1|\\boldsymbol{\\theta},\\mathbf{W}\\right\\} \\right) \\] be the responsibility of model 1 (\\(\\mathcal{N}\\left(\\mu_{1},\\sigma_{1}^{2}\\right)\\)) for observation \\(i\\). Then, given an initial guess \\(\\boldsymbol{\\theta}^{\\left(0\\right)}\\), we compute \\[ \\hat{\\gamma}_{i}\\left(\\boldsymbol{\\theta}\\right)=P\\left(\\left\\{ Z_{i}=1|\\boldsymbol{\\theta},\\mathbf{W}\\right\\} \\right)=\\frac{\\hat{p}_{1}\\phi_{\\hat{\\boldsymbol{\\theta}}_{1}}\\left(w_{i}\\right)}{\\hat{p}_{1}\\phi_{\\hat{\\boldsymbol{\\theta}}_{1}}\\left(w_{i}\\right)+\\left(1-\\hat{p}_{1}\\right)\\phi_{\\hat{\\boldsymbol{\\theta}}_{2}}\\left(w_{i}\\right)},\\quad i=1,2,\\ldots,n. \\] Then, we compute the weighted means and variances \\[ \\hat{\\mu}_{1}=\\frac{\\sum_{i=1}^{N}\\hat{\\gamma}_{i}w_{i}}{\\sum_{i=1}^{N}\\hat{\\gamma}_{i}},\\qquad\\hat{\\sigma}_{1}^{2}=\\frac{\\sum_{i=1}^{N}\\hat{\\gamma}_{i}\\left(w_{i}-\\hat{\\mu}_{1}\\right)^{2}}{\\sum_{i=1}^{N}\\hat{\\gamma}_{i}} \\] \\[ \\hat{\\mu}_{2}=\\frac{\\sum_{i=1}^{N}\\left(1-\\hat{\\gamma}_{i}\\right)w_{i}}{\\sum_{i=1}^{N}\\left(1-\\hat{\\gamma}_{i}\\right)},\\quad\\hat{\\sigma}_{2}^{2}=\\frac{\\sum_{i=1}^{N}\\left(1-\\hat{\\gamma}_{i}\\right)\\left(w_{i}-\\hat{\\mu}_{2}\\right)^{2}}{\\sum_{i=1}^{N}\\left(1-\\hat{\\gamma}_{i}\\right)} \\] and the mixing probability \\(\\hat{p}_{1}=\\sum_{i=1}^{N}\\hat{\\gamma}_{i}/n\\). We will iterate until convergence. #&#39; Calculate P(Z = 1 | W = w) #&#39; #&#39; @param x numeric vector of samples #&#39; @param theta numeric parameter vector (mu1, mu2, var1, var2, p) #&#39; #&#39; @return probability that Z = 1 given W = w prob_z &lt;- function(x, theta) { num &lt;- theta[5] * dnorm(x, mean = theta[1], sd = sqrt(theta[3])) denom &lt;- num + (1 - theta[5]) * dnorm(x, mean = theta[2], sd = sqrt(theta[4])) num / denom } #&#39; Calculate the parameter vector #&#39; #&#39; @param x numeric vector of samples #&#39; @param prob vector of probabilities P(Z = 1 | theta, X) #&#39; #&#39; @return parameter vector (mu1, mu2, var1, var2, p) find_params &lt;- function(x, prob) { mu1 &lt;- sum(prob * x) / sum(prob) var1 &lt;- sum(prob * (x - mu1) ^ 2) / sum(prob) mu2 &lt;- sum((1 - prob) * x) / sum(1 - prob) var2 &lt;- sum((1 - prob) * (x - mu2) ^ 2) / sum(1 - prob) # estimate for p1 is just the mean of the probability vector p &lt;- mean(prob) c(mu1, mu2, var1, var2, p) } #&#39; EM algorithm #&#39; #&#39; @param x numeric vector of samples #&#39; @param theta0 initial parameter vector (mu1, mu2, var1, var2, p) #&#39; @param eps iteration tolerance #&#39; #&#39; @return estimated parameter vector em_est &lt;- function(x, theta0, eps = 1e-6) { theta_old &lt;- theta0 # whatever epsilon was specified, we need our initial difference to be larger d &lt;- eps + 1 while (all(abs(d) &gt; eps)) { # E-step prob &lt;- prob_z(x, theta_old) # M-step theta_new &lt;- find_params(x, prob) d &lt;- theta_new - theta_old theta_old &lt;- theta_new } theta_old } We now simulate observations using plausible height data. #&#39; Sample from Gaussian mixture model parameterized by theta #&#39; #&#39; @param n number of samples #&#39; @param theta numeric parameter vector (mu1, mu2, var1, var2, p) #&#39; #&#39; @return samples from mixture model sample_mixture &lt;- function(n, theta) { # with probability p = theta[5], X ~ N(mu1, var1) component &lt;- runif(n) &lt;= theta[5] samples &lt;- numeric(n) samples[component] &lt;- rnorm( sum(component), mean = theta[1], sd = sqrt(theta[3]) ) samples[!component] &lt;- rnorm( sum(!component), mean = theta[2], sd = sqrt(theta[4]) ) dplyr::tibble( gender = if_else(component, &quot;man&quot;, &quot;woman&quot;), height = samples ) } set.seed(123) data &lt;- sample_mixture(1000, theta = c(178, 165, 10 ^ 2, 9 ^ 2, 0.4)) data %&gt;% dplyr::group_by(gender) %&gt;% dplyr::summarize_all(funs(length, mean, sd)) ## # A tibble: 2 x 4 ## gender length mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 man 404 178. 10.1 ## 2 woman 596 165. 8.97 We will also plot the data. data %&gt;% ggplot2::ggplot(aes(x = height, fill = gender)) + ggplot2::stat_density(position = &quot;identity&quot;, alpha = 0.5) We now apply the algorithm. To pick a starting point \\(\\boldsymbol{\\theta}^{\\left(0\\right)}\\), Friedman, Hastie, and Tibshirani (2009) recommend selecting two of the samples \\(w^{\\left(i\\right)}\\) and \\(w^{\\left(j\\right)}\\) at random and setting \\(\\mu_{1}=w^{\\left(i\\right)}\\) and \\(\\mu_{2}=w^{\\left(j\\right)}\\). They recommend setting the initial estimates for \\(\\sigma_{1}^{2}\\) and \\(\\sigma_{2}^{2}\\) equal to the sample variance, i.e., \\[ \\sigma_{1}^{2}=\\sigma_{2}^{2}=\\frac{1}{N}\\sum_{i=1}^{N}\\left(w^{\\left(i\\right)}-\\bar{w}\\right)^{2},\\quad\\text{where}\\quad\\bar{w}=\\frac{1}{N}\\sum_{i=1}^{N}w^{\\left(i\\right)}. \\] And, for an initial estimate for \\(p_{1}\\), they recommend 0.5. mu0 &lt;- sample(data$height, size = 2, replace = FALSE) var0 &lt;- with(data, sum((height - mean(height)) ^ 2) / length(height)) theta0 &lt;- c(mu0, rep(var0, 2), 0.5) theta_est &lt;- em_est(data$height, theta0) theta_est ## [1] 163.6802881 175.9983849 70.0014149 108.9100842 0.4577089 We see that we our estimate is quite good given our knowledge of the true parameters. The fact that the position of \\(\\mu_{1}\\) and \\(\\sigma_{1}^{2}\\) are “reversed” from their “true” positions is not a problem. The algorithm is blind to our initial assignments, which were in any case arbitrary. Rather, the estimate should be interpreted as “the observation is drawn from a Gaussian with mean \\(\\hat{\\mu}_{1}\\) and variance \\(\\hat{\\sigma}_{1}^{2}\\) with probability \\(\\hat{p}_{1}\\) and from a Gaussian with mean \\(\\hat{\\mu}_{2}\\) and variance \\(\\hat{\\sigma}_{2}^{2}\\) with probability \\(1-\\hat{p}_{1}\\).” 6.4 Factor analysis Outline from Dempster References "],
["markov-chain-monte-carlo.html", "Chapter 7 Markov Chain Monte Carlo 7.1 Motivation 7.2 Markov chain 7.3 Detailed balance 7.4 Metropolis-Hastings 7.5 Gibbs Sampling", " Chapter 7 Markov Chain Monte Carlo 7.1 Motivation We now introduce a general sampling technique that is often used to sample from intractable distributions. Previous techniques we have seen have required information that we may not always have, such as the inverse cumulative distribution function (CDF inversion), or a distribution from which we know how to sample that envelops a target distribution (accept-reject). We begin by introducing two seemingly intractable problems. 7.1.1 Ising model The Ising model is used in statistical mechanics to model ferromagnetism. The vertices in the graph below represent magnetic dipole moments of atomic spins, and each vertex takes one of the values \\(\\left\\{-1,1\\right\\}\\), with black and white representing \\(-1\\) and \\(1\\), respectively. Each vertex can take one of two possible values, and there are 25 such vertices, so that there are \\(2^{25}\\) possible configurations. For an \\(n\\times n\\) grid, there are \\(2^{n^{2}}\\) possible configurations. It is clear that even for modest \\(n\\), e.g., \\(n=10\\), the number of possible configurations will be huge. Suppose that we wish to sample uniformly from the set of all possible configurations. It is not clear that we have a procedure that will produce uniform samples, i.e., a procedure such that each of the \\(2^{n^{2}}\\) possible configurations is equally likely. Suppose further that we wish to sample according to a more complicated scheme. Let \\(E\\) be the set of all possible configurations, so that \\(\\left|E\\right|=2^{n^{2}}\\). Let \\(v\\in E\\) be some configuration, and let \\(\\sigma_{ij}\\) be the state of vertex \\(\\left(i,j\\right)\\), so that \\(\\sigma_{ij}\\in\\left\\{-1,1\\right\\}\\), and where we have indexed the vertices as a matrix, i.e., \\(i\\) indicates the row and \\(j\\) the column. Let \\[ H\\left(v\\right)=\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sigma_{ij}\\left(\\sigma_{i+1,j}+\\sigma_{i,j+1}+\\sigma_{i-1,j}+\\sigma_{i,j-1}\\right), \\] i.e., \\(H\\left(v\\right)\\) is the sum of the product of the state of each vertex multiplied by its neighboring vertices (and suppose that we “wrap” vertices on an edge). Let \\(X\\) be the random variable that samples from \\(E\\) according to \\(P\\left(X=v\\right)=c\\cdot\\mathrm{e}^{H\\left(v\\right)}\\), where \\(c\\) is some normalizing constant (to make this a valid probability distribution). Again, it is not clear that we have a procedure to sample from \\(X\\). 7.1.2 Intractable posterior distribution Latent Dirichlet Allocation is a generative Bayesian model that describes a process for creating documents. A document is viewed as a collection of words drawn from one or more latent topics. In this model, neither grammar nor the order of words is considered. Rather, the document is viewed as a “bag of words,” with each word arising from a particular topic. Each document has a distribution over topics, and each topic has a distribution over words. We are typically interested in the posterior distribution of the assignments of words to topics \\[ P\\left(\\mathbf{z}|\\mathbf{w}\\right)= \\frac{P\\left(\\mathbf{w},\\mathbf{z}\\right)}{\\sum_{\\mathbf{z}}P\\left(\\mathbf{w},\\mathbf{z}\\right)}, \\] where \\(\\mathbf{z}\\) represents the topics and \\(\\mathbf{w}\\) represents the words in the corpus. Griffiths and Steyvers (2004) note that “this distribution cannot be computed directly, because the sum in the denominator does not factorize and involves \\(T^{n}\\) terms, where \\(n\\) is the total number of word instances in the corpus.” Blei, Ng, and Jordan (2003) estimate the distribution using a version of the expectation-maximization algorithm. We will see that it is also possible to estimate the distribution using Markov Chain Monte Carlo methods. 7.1.3 MCMC is a sampling technique Monte Carlo methods are a class of computational methods for estimating some quantity via sampling. The idea behind Markov Chain Monte Carlo is to build a Markov chain \\(X\\left(t\\right)\\) with state space \\(S=E\\) and stationary distribution \\(\\boldsymbol{\\pi}\\) such that \\(\\boldsymbol{\\pi}\\) is the distribution on \\(E\\) from which we want to sample (\\(\\pi\\left(i\\right)=c\\cdot\\mathrm{e}^{H\\left(i\\right)}\\) for the Ising model). We will soon see that, under certain conditions, the limiting distribution of the Markov chain is \\(\\boldsymbol{\\pi}\\). Thus, if we can build such a chain, then we can run our sampler for a long (but finite) time, and we will be able to sample from \\(\\boldsymbol{\\pi}\\). We can therefore view MCMC as a sampling technique, where given \\(\\boldsymbol{\\pi}\\), our task is to find \\(\\mathbf{P}\\) such that \\(\\boldsymbol{\\pi}^{\\mathsf{T}}\\mathbf{P}=\\boldsymbol{\\pi}^{\\mathsf{T}}\\). 7.2 Markov chain We begin by setting some notation. We define a (square) matrix \\(\\mathbf{A}\\) raised to the \\(k\\text{th}\\) power as \\[ \\mathbf{A}^{k}=\\prod_{i=1}^{k}\\mathbf{A}. \\] Thus, \\(\\mathbf{A}^{2}=\\mathbf{A}\\mathbf{A}\\), \\(\\mathbf{A}^{3}=\\mathbf{A}\\mathbf{A}\\mathbf{A}\\), and so on. Theorem 7.1 Let \\(X\\left(t\\right)\\) be a finite-state Markov chain with transition probability matrix \\(\\mathbf{P}\\). Then, \\[ P\\left(\\left\\{X\\left(t\\right)=j\\right\\}|\\left\\{X\\left(0\\right)=i\\right\\}\\right)=\\left(\\mathbf{P}^{t}\\right)_{ij}, \\] i.e., the probability that the chain is in state \\(j\\) given that it started in state \\(i\\) is the \\(\\left(i,j\\right)\\text{th}\\) entry of the transition probability matrix raised to the power \\(t\\). We are now to consider the limiting behavior of Markov chains. Definition 7.1 A distribution \\(\\boldsymbol{\\pi}\\) is said to be a stationary distribution for a Markov chain \\(X\\left(t\\right)\\) with state space \\(S\\) and transition probability matrix \\(\\mathbf{P}\\) if \\(\\boldsymbol{\\pi}^{\\mathsf{T}}\\mathbf{P}=\\boldsymbol{\\pi}^{\\mathsf{T}}\\). Observe that \\(\\boldsymbol{\\pi}^{\\mathsf{T}}\\) is a left eigenvector with eigenvalue 1. We now consider how to compute \\(\\boldsymbol{\\pi}\\). One option is to solve \\(\\boldsymbol{\\pi}^{\\mathsf{T}}\\mathbf{P}=\\boldsymbol{\\pi}^{\\mathsf{T}}\\) as a linear algebra problem, i.e., solve \\[ \\sum_{i=1}^{n}\\pi\\left(i\\right)=1\\quad\\text{subject to}\\quad\\pi\\left(i\\right)\\geq 0. \\] Solving such constrained optimization problems is in general difficult, especially as the dimension of the problem increases. A second option comes from considering the limiting behavior of a finite-state Markov chain \\(X\\left(t\\right)\\). The Perron-Frobenius theorem implies that \\(X\\left(t\\right)\\) has a stationary distribution. Theorem 7.2 Let \\(X\\left(t\\right)\\) be a Markov chain with finitely many states and stationary distribution \\(\\boldsymbol{\\pi}\\). If the stationary distribution is unique (equivalently, the chain is irreducible or ergodic), then \\[ \\lim_{t\\rightarrow\\infty}X\\left(t\\right)\\sim\\boldsymbol{\\pi}. \\] Proof. Let \\(X\\left(t\\right)\\) be a Markov chain with transition probability matrix \\(\\mathbf{P}\\). For simplicity, we will prove the result in the case that \\(\\mathbf{P}\\) is symmetric (the result holds for non-symmetric \\(\\mathbf{P}\\), but the proof is considerably more complicated). From Theorem 7.1, we have \\[ P\\left(X\\left(t\\right)=j|X\\left(0\\right)=i\\right)=\\left(\\mathbf{P}^{t}\\right)_{ij}. \\] Now, \\(\\mathbf{P}\\) is symmetric, so it follows from Theorem 4.6 that \\(\\mathbf{P}=\\mathbf{Q}\\mathbf{D}\\mathbf{Q}^{\\mathsf{T}}\\), where we have used the fact that the transpose of an orthogonal matrix is equal to its inverse. Thus, \\[ P\\left(X\\left(t\\right)=j|X\\left(0\\right)=i\\right)= \\left(\\mathbf{P}^{t}\\right)_{ij}= \\left(\\left(\\mathbf{Q}\\mathbf{D}\\mathbf{Q}^{\\mathsf{T}}\\right)^{t}\\right)_{ij}. \\] Next, observe that \\[ \\left(\\mathbf{Q}\\mathbf{D}\\mathbf{Q}^{\\mathsf{T}}\\right)\\left(\\mathbf{Q}\\mathbf{D}\\mathbf{Q}^{\\mathsf{T}}\\right)= \\mathbf{Q}\\mathbf{D}\\mathbf{Q}^{-1}\\mathbf{Q}\\mathbf{D}\\mathbf{Q}^{\\mathsf{T}}= \\mathbf{Q}\\mathbf{D}\\mathbf{I}\\mathbf{D}\\mathbf{Q}^{\\mathsf{T}}= \\mathbf{Q}\\mathbf{D}^{2}\\mathbf{Q}^{\\mathsf{T}}. \\] It follows that \\(\\left(\\mathbf{Q}\\mathbf{D}\\mathbf{Q}^{\\mathsf{T}}\\right)^{t}=\\mathbf{Q}\\mathbf{D}^{t}\\mathbf{Q}^{\\mathsf{T}}\\). Now, \\(\\mathbf{D}\\) is a diagonal matrix whose (diagonal) entries are the eigenvalues of \\(\\mathbf{P}\\), so that \\[ \\mathbf{D}^{t}= \\begin{bmatrix} \\lambda_{1}^{t} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_{2}^{t} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_{n}^{t} \\end{bmatrix}. \\] We factor out \\(\\lambda_{1}^{t}\\) to obtain \\[ P\\left(X\\left(t\\right)=j|X\\left(0\\right)=i\\right)= \\lambda_{1}^{t}\\left(\\mathbf{Q} \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\left(\\frac{\\lambda_{2}}{\\lambda_{1}}\\right)^{t} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\left(\\frac{\\lambda_{n}}{\\lambda_{1}}\\right)^{t} \\end{bmatrix}\\mathbf{Q}^{\\mathsf{T}}\\right)_{ij}. \\] Suppose that the eigenvalues are distinct, i.e., \\(\\left|\\lambda_{1}\\right|&gt;\\left|\\lambda_{2}\\right|&gt;\\cdots&gt;\\left|\\lambda_{n}\\right|\\), and consider the limiting behavior of this quantity. Because \\(\\lambda_{1}\\) has the largest absolute value, \\(\\lambda_{i}/\\lambda_{1}&lt;1\\) for \\(i\\in\\left\\{2,\\ldots,n\\right\\}\\). Thus, \\[ \\lim_{t\\rightarrow\\infty}P\\left(X\\left(t\\right)=j|X\\left(0\\right)=i\\right)\\approx \\lambda_{1}^{t}\\left(\\mathbf{Q} \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{bmatrix}\\mathbf{Q}^{\\mathsf{T}}\\right)_{ij}. \\] Now, \\(P\\left(X\\left(t\\right)=j|X\\left(0\\right)=i\\right)\\) is a probability, hence must be between zero and one. If \\(\\lambda_{1}&gt;1\\), as \\(t\\) increases, the expression above will become greater than one, so that \\(P\\left(X\\left(t\\right)=j|X\\left(0\\right)=i\\right)\\) will not be a valid probability. If \\(\\lambda_{1}&lt;1\\), then as \\(t\\) increases, the expression will go to zero. By assumption, \\(X\\left(t\\right)\\) is irreducible, i.e., it is possible to reach any state from any other state. If the probability of being in state \\(j\\) goes to zero, then the chain will not be irreducible (\\(j\\) is arbitrary, and the chain must be somewhere), violating the assumption. It follows that \\(\\lambda_{1}\\) cannot be less than one, which implies that \\(\\lambda_{1}=1\\). Recalling that the columns of \\(\\mathbf{Q}\\) are the eigenvectors of \\(\\mathbf{P}\\), it follows that \\[ \\mathbf{P}\\mathbf{q}_{1}=\\lambda_{1}\\mathbf{q}_{1}=\\mathbf{q}_{1}, \\] where \\(\\mathbf{q}_{i}\\) is the \\(i\\text{th}\\) eigenvector of \\(\\mathbf{Q}\\). Taking the transpose of this expression, we have \\[ \\left(\\mathbf{P}\\mathbf{q}_{1}\\right)^{\\mathsf{T}}=\\mathbf{q}_{1}^{\\mathsf{T}} \\implies \\mathbf{q}_{1}^{\\mathsf{T}}\\mathbf{P}^{\\mathsf{T}}=\\mathbf{q}_{1}^{\\mathsf{T}}, \\] By assumption, \\(\\mathbf{P}\\) is symmetric, i.e., \\(\\mathbf{P}^{\\mathsf{T}}=\\mathbf{P}\\), so that \\(\\mathbf{q}_{1}^{\\mathsf{T}}\\mathbf{P}=\\mathbf{q}_{1}^{\\mathsf{T}}\\), i.e., \\(\\mathbf{q}_{1}^{\\mathsf{T}}\\) is a left eigenvector of \\(\\mathbf{P}\\). Thus, the limiting distribution of \\(X\\left(t\\right)\\) is \\(\\mathbf{q}_{1}\\), so that \\(\\mathbf{q}_{1}=\\boldsymbol{\\pi}\\), i.e., \\(\\mathbf{q}_{1}\\) is the stationary distribution. Corollary 7.1 If \\(\\mathbf{P}\\) is a transition probability matrix for an irreducible, finite-state Markov chain \\(X\\left(t\\right)\\), then its eigenvalues have absolute value less than or equal to \\(1\\), i.e., \\(\\left|\\lambda_{i}\\right|\\leq 1\\,\\forall i\\). Further, \\(\\mathbf{P}\\) has exactly one eigenvalue equal to \\(1\\), and it corresponds to the eigenvector that is the stationary distribution of \\(X\\left(t\\right)\\). It is not difficult to find a finite-state Markov chain that does not have a unique stationary distribution. Consider a chain with four states and transition probability matrix \\[ \\begin{bmatrix} \\frac{1}{2} &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\ \\frac{1}{2} &amp; \\frac{1}{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} \\\\ 0 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} \\\\ \\end{bmatrix} \\] In this chain, states 1 and 2 do not communicate with states 3 and 4, and it is easy to see that the chain has two stationary distributions. If \\(X\\left(0\\right)\\in\\left\\{1,2\\right\\}\\), then \\(\\boldsymbol{\\pi}=\\left(1/2,1/2,0,0\\right)\\), and if \\(X\\left(0\\right)\\in\\left\\{3,4\\right\\}\\), then \\(\\boldsymbol{\\pi}=\\left(0,0,1/2,1/2\\right)\\). 7.3 Detailed balance We have said that we can use MCMC to sample from an otherwise intractable distribution if we construct a Markov chain whose stationary distribution is the target distribution. We have also proved that the limiting distribution of a finite-state Markov chain is the stationary distribution (provided it is unique). Our attention now turns to constructing such a chain. Definition 7.2 A distribution \\(\\boldsymbol{\\nu}\\) on a state space \\(S\\) is said to be in detailed balance for a Markov chain \\(X\\left(t\\right)\\) on \\(S\\) with transition probability matrix \\(\\mathbf{P}\\) if \\(\\nu\\left(i\\right)P_{ij}=\\nu\\left(j\\right)P_{ji}\\). Practically, it is difficult to produce a transition probability matrix \\(\\mathbf{P}\\) that makes \\(\\boldsymbol{\\pi}\\) stationary. It is often easier to produce a \\(\\mathbf{P}\\) such that \\(\\boldsymbol{\\pi}\\) is in detailed balance. Theorem 7.3 If \\(\\boldsymbol{\\nu}\\) is in detailed balance for a Markov chain \\(X\\left(t\\right)\\), then \\(\\boldsymbol{\\nu}\\) is a stationary distribution. Proof. We present a proof sketch of the above theorem. We need to show that \\(\\boldsymbol{\\nu}\\mathbf{P}=\\boldsymbol{\\nu}\\), or equivalently that \\(\\left(\\boldsymbol{\\nu}\\mathbf{P}\\right)_{i}=\\nu\\left(i\\right)\\). Observe that \\[ \\left(\\boldsymbol{\\nu}\\mathbf{P}\\right)_{i}= \\sum_{j=1}^{n}\\nu\\left(j\\right)P_{ji}= \\sum_{j=1}^{n}\\nu\\left(i\\right)P_{ij}= \\nu\\left(i\\right)\\sum_{j=1}^{n}P_{ij}, \\] where the second equality follows because \\(\\boldsymbol{\\nu}\\) is in detailed balance. Now, \\(\\sum_{j=1}^{n}P_{ij}\\) is a sum over a row of a transition probability matrix, which is equal to 1, hence \\(\\left(\\boldsymbol{\\nu}\\mathbf{P}\\right)_{i}=\\nu\\left(i\\right)\\). \\(i\\) was chosen arbitrarily, so it follows that \\(\\boldsymbol{\\nu}\\mathbf{P}=\\boldsymbol{\\nu}\\), and the result has been shown. We now attempt to give an intuition for detailed balance. Consider a Markov chain with two states, and suppose that \\(\\boldsymbol{\\nu}\\) is in detailed balance for the chain. Then, \\(\\nu\\left(1\\right)P_{12}=\\nu\\left(2\\right)P_{21}\\), i.e., the probability of being in state 1 and moving to state 2 is equal to the probability of being in state 2 and moving to state 1. 7.4 Metropolis-Hastings We began by considering the problem of sampling from an intractable distribution. We have established that the limiting distribution of an irreducible finite-state Markov chain \\(X\\left(t\\right)\\) is the stationary distribution, and we have seen that if a distribution is in detailed balance for the \\(X\\left(t\\right)\\), then it is a stationary distribution. We now consider an algorithm for sampling from \\(X\\left(t\\right)\\). The idea behind the Metropolis-Hastings algorithm is that we do not have, or cannot write down (or store in memory) the transition probability matrix \\(\\mathbf{P}\\). We will instead attempt to simulate \\(X\\left(t\\right)\\) by the Metropolis-Hastings algorithm. Suppose that at time \\(t\\) the chain is in state \\(s\\), i.e., \\(X\\left(t\\right)=s\\). Let \\(q\\left(s,s&#39;\\right)\\) be the probability of proposing state \\(s&#39;\\) given that the chain is in state \\(s\\). \\(q\\) is called a proposal function. Let \\(\\hat{U}\\) be a sample from a standard uniform random variable, and consider the quantity \\[ \\min\\left(1,\\frac{\\nu\\left(s&#39;\\right)q\\left(s&#39;,s\\right)}{\\nu\\left(s\\right)q\\left(s,s&#39;\\right)}\\right). \\] If \\(\\hat{U}\\) is less than this quantity, we will accept the proposal and set \\(X\\left(t+1\\right)=s&#39;\\). Otherwise, we will reject the proposal and remain in state \\(s\\), i.e., \\(X\\left(t+1\\right)=s\\). Example 7.1 Consider again the Ising model, and suppose that \\(n=3\\), so that there are 9 possible states, hence \\(2^{9}\\) possible configurations. Consider the proposal function that picks a vertex uniformly and flips its sign, taking the resulting configuration as the proposed state. Then, the probability of proposing \\(s&#39;\\) given that the chain is in state \\(s\\) is \\[ q\\left(s,s&#39;\\right)= \\begin{cases} 1/n^{2}, &amp; \\text{if }s\\text{ and }s&#39;\\text{ differ at 1 vertex}\\\\ 0, &amp; \\text{otherwise} \\end{cases}. \\] We now step through the algorithm. Having sampled \\(s&#39;\\) from the proposal, we form the Metropolis-Hastings ratio \\[ \\min\\left(1,\\frac{\\nu\\left(s&#39;\\right)q\\left(s&#39;,s\\right)}{\\nu\\left(s\\right)q\\left(s,s&#39;\\right)}\\right)= \\min\\left(1,\\frac{\\mathrm{e}^{H\\left(s&#39;\\right)}\\left(1/n^{2}\\right)}{\\mathrm{e}^{H\\left(s\\right)}\\left(1/n^{2}\\right)}\\right)= \\min\\left(1,\\mathrm{e}^{H\\left(s&#39;\\right)-H\\left(s\\right)}\\right). \\] If a sample drawn from \\(\\mathcal{U}\\left(0,1\\right)\\) is less than this quantity, we will accept the proposal and the chain will move to state \\(s&#39;\\), else we will reject it and remain in state \\(s\\). We see that if \\(H\\left(s&#39;\\right)&gt;H\\left(s\\right)\\), then \\(\\min\\left(1,\\mathrm{e}^{H\\left(s&#39;\\right)-H\\left(s\\right)}\\right)=1\\), i.e., we will accept the proposal. Intuitively, the algorithm moves to areas of higher probability, which reflects the stationary distribution. Observe also that if \\(H\\left(s&#39;\\right)&lt;H\\left(s\\right)\\), that we will sometimes accept the proposal and sometimes reject, depending on the sample from \\(\\mathcal{U}\\left(0,1\\right)\\). This reflects the fact that for the chain to fully explore the state space, it must sometimes move to a state of lower probability. This completes one step of the algorithm. Example 7.2 Let \\(X\\sim\\mathcal{N}\\left(0,1\\right)\\) be the distribution from which we wish to sample. In this case, \\(S=\\mathbb{R}\\), so that any state \\(s\\in\\mathbb{R}\\). Let the probability of being in state \\(s\\) be given by the Gaussian density, i.e., \\[ \\nu\\left(s\\right)=\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-s^{2}/2}. \\] Suppose that we propose a state by adding a sample from \\(\\mathcal{U}\\left(0,1\\right)\\) to \\(s\\) and then subtracting \\(1/2\\), i.e., \\(s&#39;=s+\\left(U-1/2\\right)\\), where \\(U\\) is a sample from the standard uniform distribution. Observe that \\(s&#39;\\) has the uniform distribution over \\(\\left[s-1/2,s+1/2\\right]\\), i.e., given that we are in state \\(s\\), the probability that we will propose \\(s&#39;\\) is uniform over an interval of length \\(1\\) centered at \\(s\\). Thus, \\(s&#39;\\) has the uniform density \\(f_{U}\\left(s\\right)=1/\\left(b-a\\right)=1/1=1\\). Observe also that the proposal is symmetric, i.e., \\(q\\left(s,s&#39;\\right)=q\\left(s&#39;,s\\right)\\). Accordingly, the Metropolis-Hastings ratio is \\[ \\min\\left(1,\\frac{\\nu\\left(s&#39;\\right)q\\left(s&#39;,s\\right)}{\\nu\\left(s\\right)q\\left(s,s&#39;\\right)}\\right)= \\min\\left(1,\\frac{\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\left(s&#39;\\right)^{2}/2}\\cdot 1}{\\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-s^{2}/2}\\cdot 1}\\right)= \\min\\left(1,\\mathrm{e}^{\\left(s^{2}-\\left(s&#39;\\right)^{2}\\right)/2}\\right). \\] Observe that Metropolis-Hastings is a general sampling algorithm, though for sampling from a \\(\\mathcal{N}\\left(0,1\\right)\\), the algorithm is slow and would not be used in practice. We will now sample from \\(X\\) using the algorithm. nu &lt;- function(x) 1 / sqrt(2 * pi) * exp(-x ^ 2 / 2) propose &lt;- function(x) x + runif(1) - 0.5 q &lt;- function(x, x_prime) 1 mh_ratio &lt;- function(x, x_prime, nu, q) { min(1, nu(x_prime) * q(x_prime, x) / (nu(x) * q(x, x_prime))) } mh_sampler &lt;- function(x0, proposal, nu, q, iter) { x &lt;- vector(mode = &quot;numeric&quot;, length = iter + 1) x[1] &lt;- x0 for (i in seq(iter)) { x_prime &lt;- proposal(x[i]) ratio &lt;- mh_ratio(x[i], x_prime, nu, q) if (runif(1) &lt; ratio) { x[i + 1] &lt;- x_prime } else { x[i + 1] &lt;- x[i] } } x } We will run our sampler for \\(T=10^{4}\\) iterations, starting with \\(X\\left(0\\right)=10\\) (we deliberately choose a starting value far from the stationary distribution to show how the algorithm moves to areas of higher probability). x &lt;- mh_sampler(x0 = 10, proposal = propose, nu = nu, q = q, iter = 1e4) We now examine the first 500 samples. We see that the distribution is far from the standard Gaussian. This is unsurprising given our choice of initial state. As the chain runs, we should begin to see convergence to the stationary distribution. We see that samples 501 to 1000 are much closer to the standard Gaussian. We now examine the remaining samples. The chain appears to have converged to the stationary distribution. For most applications, we will not know the density of the target distribution (if we did, we would not be using Metropolis-Hastings). Instead, we would perform convergence diagnostics and discard samples to account for starting “far” from the stationary distribution (burn-in) and autocorrelation (thinning). We now give a proof sketch for the Metropolis-Hastings algorithm. Recall that we cannot write down or store the transition probability matrix of the chain \\(X\\left(t\\right)\\), but if we can find a distribution \\(\\boldsymbol{\\nu}\\) that is in detailed balance for \\(X\\left(t\\right)\\), then \\(\\boldsymbol{\\nu}\\) will be a stationary distribution. Detailed balance is defined in terms of pairs of states, so the idea behind the algorithm is to make sure that every pair of states is in detailed balance. If we can do this, then \\(\\boldsymbol{\\nu}\\) will be the stationary distribution, and \\(X\\left(t\\right)\\) will converge to \\(\\boldsymbol{\\nu}\\). For the proof sketch, we will consider two states, \\(i\\) and \\(j\\), where the probability of being in state \\(i\\) is \\(\\nu\\left(i\\right)\\), and where the probability of proposing state \\(i\\) given that \\(X\\left(t\\right)\\) is in state \\(j\\) is \\(q\\left(i,j\\right)\\). Proof. We will choose transition probabilities to make \\(\\boldsymbol{\\nu}\\) the stationary distribution. For the moment, we will always accept the proposal, i.e., we will leave aside the third step of the algorithm. Then, from the diagram above, we see that the probability of moving from \\(i\\) to \\(j\\) is the probability of being in state \\(i\\) multiplied by the probability of proposing \\(j\\), i.e., \\(\\nu\\left(i\\right)q\\left(i,j\\right)\\), and similarly for moving from \\(j\\) to \\(i\\). But there is no reason that \\(\\boldsymbol{\\nu}\\) should be in detailed balance for some proposal function \\(q\\), i.e., in general we will not have \\(\\nu\\left(i\\right)q\\left(i,j\\right)=\\nu\\left(j\\right)q\\left(j,i\\right)\\). We can correct the situation by introducing \\(\\alpha,\\beta\\in\\mathbb{R}\\) such that we will accept the proposals to move to states \\(j\\) and \\(i\\) with probabilities \\(\\alpha\\) and \\(\\beta\\), respectively. I.e., the probability of accepting the proposal \\(j\\) is \\(\\alpha\\cdot q\\left(i,j\\right)\\), and similarly the probability of accepting the proposal \\(i\\) is \\(\\beta\\cdot q\\left(j,i\\right)\\). Observe then that the probability of moving from \\(i\\) to \\(j\\) is now the probability of being in \\(i\\) multiplied by the probability of proposing \\(j\\), multiplied by the probability of accepting the proposal, i.e., \\(\\nu\\left(i\\right)q\\left(i,j\\right)\\alpha\\). (Observe that the \\(\\left(i,j\\right)\\text{th}\\) entry of the transition probability matrix is \\(P_{ij}=q\\left(i,j\\right)\\alpha\\). Thus, we will specify transition probabilities without ever writing down the matrix. Observe also that \\(\\alpha=P\\left(\\mathcal{U}&lt;\\alpha\\right)\\).) Thus, we must choose \\(\\alpha\\) and \\(\\beta\\) to satisfy detailed balance, i.e., so that \\[ \\nu\\left(i\\right)q\\left(i,j\\right)\\alpha=\\nu\\left(j\\right)q\\left(j,i\\right)\\beta \\implies\\frac{\\alpha}{\\beta}=\\frac{\\nu\\left(j\\right)q\\left(j,i\\right)}{\\nu\\left(i\\right)q\\left(i,j\\right)}. \\] If we choose \\(\\beta=1\\), then \\[ \\alpha=\\frac{\\nu\\left(j\\right)q\\left(j,i\\right)}{\\nu\\left(i\\right)q\\left(i,j\\right)}, \\] but observe that if the ratio on the right is greater than \\(1\\), \\(\\alpha\\) will not be a valid probability. Thus, if the Metropolis-Hastings ratio is less than or equal to \\(1\\), we will set \\[ \\beta=1\\quad\\text{and}\\quad\\alpha=\\frac{\\nu\\left(j\\right)q\\left(j,i\\right)}{\\nu\\left(i\\right)q\\left(i,j\\right)}. \\] Similarly, if the ratio is greater than \\(1\\), we will set \\[ \\alpha=1\\quad\\text{and}\\quad\\beta=\\frac{\\nu\\left(i\\right)q\\left(i,j\\right)}{\\nu\\left(j\\right)q\\left(j,i\\right)}, \\] and observe that in this case \\(\\beta\\) is the reciprocal of the ratio. Therefore, \\[ \\alpha=\\min\\left(1,\\frac{\\nu\\left(j\\right)q\\left(j,i\\right)}{\\nu\\left(i\\right)q\\left(i,j\\right)}\\right) \\] and \\[ \\beta=\\min\\left(1,\\frac{\\nu\\left(i\\right)q\\left(i,j\\right)}{\\nu\\left(j\\right)q\\left(j,i\\right)}\\right). \\] Observe that if the MH ratio is greater than 1, we will have \\[ \\alpha=1\\quad\\text{and}\\quad\\beta=\\frac{\\nu\\left(i\\right)q\\left(i,j\\right)}{\\nu\\left(j\\right)q\\left(j,i\\right)}, \\] so that \\[ \\nu\\left(i\\right)q\\left(i,j\\right)\\alpha= \\nu\\left(i\\right)q\\left(i,j\\right)\\cdot 1= \\nu\\left(i\\right)q\\left(i,j\\right) \\] and \\[ \\nu\\left(j\\right)q\\left(j,i\\right)\\beta= \\nu\\left(j\\right)q\\left(j,i\\right)\\frac{\\nu\\left(i\\right)q\\left(i,j\\right)}{\\nu\\left(j\\right)q\\left(j,i\\right)}= \\nu\\left(i\\right)q\\left(i,j\\right), \\] so that detailed balance is satisfied. If the MH ratio is less than or equal to 1, we will have \\[ \\alpha=\\frac{\\nu\\left(j\\right)q\\left(j,i\\right)}{\\nu\\left(i\\right)q\\left(i,j\\right)}\\quad\\text{and}\\quad\\beta=1, \\] so that \\[ \\nu\\left(i\\right)q\\left(i,j\\right)\\alpha= \\nu\\left(i\\right)q\\left(i,j\\right)\\frac{\\nu\\left(j\\right)q\\left(j,i\\right)}{\\nu\\left(i\\right)q\\left(i,j\\right)}= \\nu\\left(j\\right)q\\left(j,i\\right), \\] which we see is equal to \\(\\nu\\left(j\\right)q\\left(j,i\\right)\\beta\\) when \\(\\beta=1\\), so that detailed balance is again satisfied. Thus, we have shown how to construct \\(\\alpha\\) and \\(\\beta\\), hence \\(P_{ij}\\) for an arbitrary proposal \\(q\\), such that detailed balance is satisfied for every pair of states. Theorem 7.3 implies that \\(\\boldsymbol{\\nu}\\) is the stationary distribution of \\(X\\left(t\\right)\\), and Theorem 7.2 implies that \\(X\\left(t\\right)\\) will converge to \\(\\boldsymbol{\\nu}\\). Recall that our goal is to sample from \\(\\boldsymbol{\\nu}\\). The Metropolis-Hastings algorithm gives us a way to sample from this distribution by constructing a Markov chain that has \\(\\boldsymbol{\\nu}\\) as its stationary distribution. Now, Theorem 7.2 guarantees convergence only in the case that \\(t\\rightarrow\\infty\\). In practice, we will run the chain until some large time \\(T\\), e.g., \\(T=10^{6}\\), and our sample of \\(\\boldsymbol{\\nu}\\) is the state of the chain at time \\(T\\), i.e., \\(X\\left(T\\right)\\). 7.5 Gibbs Sampling We now present Gibbs sampling, a special case of the Metropolis-Hastings algorithm that does not require us to specify a proposal. Suppose that we wish to sample from \\(\\mathbf{X}=\\left(X_{1},\\ldots,X_{n}\\right)\\). The idea behind Gibbs sampling is to set a Markov chain \\(\\mathbf{W}\\left(0\\right)=\\left(X_{1}^{\\left(0\\right)},\\ldots,X_{n}^{\\left(0\\right)}\\right)\\), where the \\(X_{i}^{\\left(0\\right)}\\) are often set at random (from possible values of each \\(X_{i}\\)). Then, the marginal density of the first coordinate is \\(f\\left(X_{1}|X_{2}^{\\left(0\\right)},\\ldots,X_{n}^{\\left(0\\right)}\\right)\\). Let \\(\\hat{X}_{1}^{\\left(1\\right)}\\) be a sample from the marginal density of \\(X_{1}\\). Setting \\(X_{1}^{\\left(1\\right)}=\\hat{X}_{1}^{\\left(1\\right)}\\), the Markov chain at \\(t=1\\) is \\(\\mathbf{W}\\left(1\\right)=\\left(X_{1}^{\\left(1\\right)},X_{2}^{\\left(0\\right)}\\ldots,X_{n}^{\\left(0\\right)}\\right)\\). For the second coordinate, let \\(f\\left(X_{2}|X_{1}^{\\left(1\\right)},X_{3}^{\\left(0\\right)},\\ldots,X_{n}^{\\left(0\\right)}\\right)\\) be the marginal density of \\(X_{2}\\). We then draw a sample \\(\\hat{X}_{2}^{\\left(1\\right)}\\) from the marginal density, and our Markov chain becomes \\(\\mathbf{W}\\left(1\\right)=\\left(X_{1}^{\\left(1\\right)},X_{2}^{\\left(1\\right)},X_{3}^{\\left(0\\right)},\\ldots,X_{n}^{\\left(0\\right)}\\right)\\). We proceed in this manner until we have drawn samples from the marginal density of each \\(X_{i}\\), which is given by \\(f\\left(X_{i}|\\mathbf{X}_{-i}\\right)\\), where \\(\\mathbf{X}_{-i}\\) is the vector whose entries are the samples drawn from the marginal densities of \\(\\left\\{X_{j}\\right\\}_{j=1}^{i-1}\\) and \\(\\left\\{X_{j}^{\\left(0\\right)}\\right\\}_{j=i+1}^{n}\\). Theorem 7.4 The limiting distribution of \\(\\mathbf{W}\\left(t\\right)\\) is \\(\\mathbf{X}\\), i.e., \\[ \\lim_{t\\rightarrow\\infty}\\mathbf{W}\\left(t\\right)\\sim\\mathbf{X}. \\] We now present a proof sketch for Gibbs sampling. Proof. We wish to sample from a distribution \\(\\nu\\left(X_{1},\\ldots,X_{n}\\right)\\). Applying conditional probability, we have \\[ \\nu\\left(X_{1},\\ldots,X_{n}\\right)=f\\left(X_{i}|\\mathbf{X}_{-i}\\right)p\\left(\\mathbf{X}_{-i}\\right). \\] Let \\(\\hat{X}_{i}\\) be the “Gibbs sampling” sample of coordinate \\(i\\). We can regard \\(\\hat{X}_{i}\\) as a proposal in the Metropolis-Hastings sense. We begin by forming the Metropolis-Hastings ratio \\[ \\min\\left(1,\\frac{\\nu\\left(\\mathbf{X}&#39;\\right)q\\left(\\mathbf{X}&#39;,\\mathbf{X}\\right)}{\\nu\\left(\\mathbf{X}\\right)q\\left(\\mathbf{X},\\mathbf{X}&#39;\\right)}\\right). \\] Noting that \\(\\nu\\left(\\mathbf{X}&#39;\\right)\\) is just \\(\\nu\\left(\\mathbf{X}\\right)\\) with \\(X_{i}\\) replaced by \\(\\hat{X}_{i}\\), the ratio becomes \\[ \\min\\left(1,\\frac{f\\left(\\hat{X}_{i}|\\mathbf{X}_{-i}\\right)p\\left(\\mathbf{X}_{-i}\\right)q\\left(x&#39;,x\\right)}{f\\left(X_{i}|\\mathbf{X}_{-i}\\right)p\\left(\\mathbf{X}_{-i}\\right)q\\left(x,x&#39;\\right)}\\right). \\] Recall that \\(q\\left(\\mathbf{X},\\mathbf{X}&#39;\\right)\\) is the probability of proposing \\(\\mathbf{X&#39;}\\) given that the chain is in state \\(\\mathbf{X}\\). Writing \\(q\\) as \\[ q\\left(\\mathbf{X},\\mathbf{X}&#39;\\right)=q\\left(\\left(X_{i},\\mathbf{X}_{-i}\\right),\\left(\\hat{X}_{i},\\mathbf{X}_{-i}\\right)\\right) \\] we see that the probability of proposing \\(\\mathbf{X}&#39;\\) is given by the marginal density of \\(\\mathbf{X}&#39;\\), \\(f\\left(\\hat{X}_{i}|\\mathbf{X}_{-i}\\right)\\), which does not depend on the current state \\(\\mathbf{X}\\). Similarly, \\(q\\left(\\mathbf{X}&#39;,\\mathbf{X}\\right)=f\\left(X_{i}|\\mathbf{X}_{-i}\\right)\\), so that the ratio becomes \\[ \\min\\left(1,\\frac{f\\left(\\hat{X}_{i}|\\mathbf{X}_{-i}\\right)p\\left(\\mathbf{X}_{-i}\\right)f\\left(X_{i}|\\mathbf{X}_{-i}\\right)}{f\\left(X_{i}|\\mathbf{X}_{-i}\\right)p\\left(\\mathbf{X}_{-i}\\right)f\\left(\\hat{X}_{i}|\\mathbf{X}_{-i}\\right)}\\right)=\\min\\left(1,1\\right)=1, \\] i.e., in Gibbs sampling, we always accept the proposal. Thus, Gibbs sampling can be viewed as a special case of Metropolis-Hastings where the proposal is always accepted. We can use Gibbs sampling to sample from \\(\\mathbf{X}\\) without specifying a proposal distribution, though we must know how to sample from the marginal densities of the \\(X_{i}\\). Gibbs sampling tends to work well when the random variable we wish to sample breaks up into coordinates. Note also that Gibbs sampling is subject to the same convergence considerations as the general Metropolis-Hastings algorithm. 7.5.1 Latent Dirichlet Allocation We return now to LDA. Recall that we wish to estimate an intractable posterior distribution. We can write the conditional distribution of each topic \\(z_{i}\\) as \\[ P\\left(z_{i}=j|\\mathbf{z}_{-i},\\mathbf{w}\\right)\\propto\\frac{n_{-i,j}^{\\left(w_{i}\\right)}+\\beta}{n_{-i,j}^{\\left(\\cdot\\right)}+W\\beta}\\frac{n_{-i,j}^{\\left(d_{i}\\right)}+\\alpha}{n_{-i}^{\\left(d_{i}\\right)}+T\\alpha}, \\] where \\(\\mathbf{z}_{-i}\\) consists of all topics except \\(z_{i}\\) \\(n_{-i,j}^{\\left(w_{i}\\right)}\\) is the number of times word \\(w_{i}\\) has been assigned to topic \\(j\\) (excluding \\(z_{i}\\)) \\(n_{-i,j}^{\\left(d_{i}\\right)}\\) is the number of times a word from document \\(d_{i}\\) has been assigned to topic \\(j\\) (again excluding \\(z_{i}\\)) \\(n_{-i,j}^{\\left(\\cdot\\right)}\\) is the number of times all words have been assigned to topic \\(j\\) (excluding \\(z_{i}\\)) \\(n_{-i}^{\\left(d_{i}\\right)}\\) is the number of times a word from document \\(d_{i}\\) has been assigned to all topics (excluding \\(z_{i}\\)) \\(W\\) is the number of words in the dictionary \\(T\\) is the number of topics and \\(\\alpha\\) and \\(\\beta\\) parameterize the prior distributions over topics and words, respectively. We can then use Gibbs sampling to sample each \\(z_{i}\\) in turn, yielding samples of the posterior distribution of assignments of word to topics \\(\\mathbf{z}\\). Gibbs sampling appears frequently in Bayesian statistics because interest often lies in the marginal distributions of various parameters. References "],
["pagerank.html", "Chapter 8 PageRank 8.1 Motivation 8.2 Computing eigenpairs 8.3 Algorithm 8.4 Considerations", " Chapter 8 PageRank 8.1 Motivation Imagine a web surfer who moves from page to page by clicking on links randomly with uniform probability. Suppose that the surfer is confined to a set of five pages defined by the following graph, where a link from page \\(i\\) to page \\(j\\) is represented by an arrow pointing from \\(i\\) to \\(j\\). If the surfer is currently on page \\(i\\), we can represent the probability that the surfer moves to page \\(j\\) by the \\(\\left(i,j\\right)\\text{th}\\) entry of the following transition probability matrix: \\[ \\mathbf{A}= \\begin{bmatrix} 0 &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\frac{1}{3} &amp; 0 &amp; \\frac{1}{3} &amp; 0 &amp; \\frac{1}{3}\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\] Proposition 8.1 Let \\(\\mathbf{A}\\) be a transition probability matrix as above. Let \\(\\boldsymbol{\\lambda}\\in\\mathbb{R}^{n}\\) be the eigenvalues of \\(\\mathbf{A}^{\\mathsf{T}}\\). Then, \\(\\max_{i\\in\\left\\{1,\\ldots,n\\right\\}}\\left |\\lambda_{i}\\right |=1\\). Let \\(\\mathbf{v}\\) be the eigenvector with eigenvalue 1. Then, \\(v_{i}\\geq 0\\). If a web surfer surfs for a long time, then \\(P\\left(\\text{surfer ends on page }i\\right)=v_{i}\\), assuming \\(\\sum_{i}v_{i}=1\\). In Google’s PageRank algorithm, described in Page et al. (1999), the page rank is determined by the values of \\(v_{i}\\) in decreasing order. Definition 8.1 For a given matrix, the eigenvalue with maximum absolute value is called the dominant eigenvalue, and the associated eigenvector is the dominant eigenvector. To obtain a page’s rank, we must compute the dominant eigenvector and eigenvalue. 8.2 Computing eigenpairs We now consider the problem of computing eigenvalues and eigenvectors. Recall that a scalar \\(\\lambda\\) is an eigenvalue of an \\(n\\times n\\) matrix \\(\\mathbf{A}\\) if and only if it satisfies the characteristic equation \\(\\det\\left(\\mathbf{A}-\\lambda\\mathbf{I}_{n}\\right)=0\\), where \\(\\mathbf{I}_{n}\\) is the \\(n\\)-dimensional identity matrix and \\(\\det\\left(\\mathbf{M}\\right)\\) is the determinant of \\(\\mathbf{M}\\). If the characteristic equation is a polynomial of degree 4 or less, then an explicit algebraic solution may be obtained. If the characteristic equation is of degree 5 or higher, than an algebraic solution is impossible, and the eigenvalues must be approximted by numerical methods. In general, the computational complexity of computing the eigenvectors of a (square) \\(n\\)-dimensional matrix, e.g., by the QR algorithm, is \\(O\\left(n^{3}\\right)\\), i.e., cubic in the dimension of the matrix. It is clear that for a network of even modest size, e.g., \\(10^{5}\\), this problem will not be tractable. If we reduce the problem to computing the dominant eigenvector rather than all eigenvectors, then we can use a more efficient algorithm. 8.3 Algorithm We now present power iteration. Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) matrix. Choose \\(\\mathbf{v}^{\\left(0\\right)}\\in\\mathbb{R}^{n}\\) such that \\(\\mathbf{v}^{\\left(0\\right)}\\neq\\mathbf{0}\\), and choose \\(\\epsilon&gt;0\\). While \\(\\left\\Vert \\mathbf{v}^{\\left(i\\right)}-\\mathbf{v}^{\\left(i-1\\right)}\\right\\Vert \\geq \\epsilon\\) \\(\\mathbf{v}^{\\left(i\\right)}\\gets\\mathbf{A}\\mathbf{v}^{\\left(i-1\\right)}\\) \\(\\mathbf{v}^{\\left(i\\right)}\\gets\\mathbf{v}^{\\left(i\\right)}/\\sum_{j}v_{j}^{\\left(i\\right)}\\) When the algorithm terminates, \\(\\mathbf{v}^{\\left(i\\right)}\\) will be (approximately) the dominant eigenvector of \\(\\mathbf{A}\\). We can then compute the dominant eigenvalue \\(\\lambda_{\\max}\\) by the Rayleigh quotient, i.e., \\[ \\lambda_{\\max}=\\dfrac{\\left(\\mathbf{v}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\mathbf{A}\\mathbf{v}^{\\left(i\\right)}}{\\left\\Vert \\mathbf{v}^{\\left(i\\right)}\\right\\Vert_{2}^{2}}. \\] Proof. We now present a proof sketch for power iteration. Let \\(\\left\\{\\lambda_{i}\\right\\}_{i=1}^{n}\\) be the eigenvalues of \\(\\mathbf{A}\\) such that \\(\\left|\\lambda_{1}\\right|&gt;\\left|\\lambda_{2}\\right|&gt;\\cdots&gt;\\left|\\lambda_{n}\\right|\\), and let \\(\\left\\{\\mathbf{w}^{\\left(i\\right)}\\right\\}_{i=1}^{n}\\) be the corresponding eigenvectors. Suppose that the \\(\\mathbf{w}^{\\left(i\\right)}\\) form a basis for \\(\\mathbb{R}^{n}\\), and suppose that we begin power iteration with some \\(\\mathbf{v}^{\\left(0\\right)}\\in\\mathbb{R}^{n}\\). Because the \\(\\mathbf{w}^{\\left(i\\right)}\\) span \\(\\mathbb{R}^{n}\\), we can express \\(\\mathbf{v}^{\\left(0\\right)}\\) as a linear combination of the \\(\\mathbf{w}^{\\left(i\\right)}\\), i.e., \\[ \\mathbf{v}^{\\left(0\\right)}=\\sum_{i=1}^{n}c_{i}\\mathbf{w}^{\\left(i\\right)} \\] where \\(\\left\\{c_{i}\\right\\}_{i=1}^{n}\\in\\mathbb{R}^{n}\\). At the first iteration of the algorithm, we left-multiply \\(\\mathbf{v}^{\\left(0\\right)}\\) by \\(\\mathbf{A}\\), which we can write as \\[ \\mathbf{v}^{\\left(1\\right)}= \\mathbf{A}\\mathbf{v}^{\\left(0\\right)}= \\sum_{i=1}^{n}c_{i}\\mathbf{A}\\mathbf{w}^{\\left(i\\right)}= \\sum_{i=1}^{n}c_{i}\\lambda_{i}\\mathbf{w}^{\\left(i\\right)} \\] where the final equality follows because \\(\\mathbf{w}^{\\left(i\\right)}\\) is an eigenvector of \\(\\mathbf{A}\\). Observe that if \\(\\mathbf{v}\\) is an eigenvector of \\(\\mathbf{A}\\), then \\(c\\mathbf{v}\\) is also an eigenvector of \\(\\mathbf{A}\\) for some \\(c\\in\\mathbb{R}\\). We have \\(c=\\sum_{j}v_{j}^{\\left(i\\right)}\\), so without loss of generality we will omit the normalization step of the algorithm. At the second iteration, we left-multiply \\(\\mathbf{v}^{\\left(1\\right)}\\) by \\(\\mathbf{A}\\), which we can write as \\[ \\mathbf{v}^{\\left(2\\right)}= \\mathbf{A}\\mathbf{v}^{\\left(1\\right)}= \\mathbf{A}\\mathbf{A}\\mathbf{v}^{\\left(0\\right)}= \\mathbf{A}^{2}\\mathbf{v}^{\\left(0\\right)}= \\sum_{i=1}^{n}c_{i}\\lambda_{i}\\mathbf{A}\\mathbf{w}^{\\left(i\\right)}= \\sum_{i=1}^{n}c_{i}\\lambda_{i}^{2}\\mathbf{w}^{\\left(i\\right)}. \\] We can see that \\(\\mathbf{v}^{\\left(M\\right)}\\) will have the form \\[ \\mathbf{v}^{\\left(M\\right)}= \\mathbf{A}^{M}\\mathbf{v}^{\\left(0\\right)}= \\sum_{i=1}^{n}c_{i}\\lambda_{i}^{M}\\mathbf{w}^{\\left(i\\right)}= c_{1}\\lambda_{1}^{M}\\mathbf{w}^{\\left(1\\right)}+ c_{2}\\lambda_{2}^{M}\\mathbf{w}^{\\left(2\\right)}+\\cdots+ c_{n}\\lambda_{n}^{M}\\mathbf{w}^{\\left(n\\right)}. \\] We can factor out \\(\\lambda_{1}^{M}\\) to give \\[ \\mathbf{v}^{\\left(M\\right)}= \\lambda_{1}^{M}\\left( c_{1}\\mathbf{w}^{\\left(1\\right)}+ c_{2}\\left(\\dfrac{\\lambda_{2}}{\\lambda_{1}}\\right)^{M}\\mathbf{w}^{\\left(2\\right)}+\\cdots+ c_{n}\\left(\\dfrac{\\lambda_{n}}{\\lambda_{1}}\\right)^{M}\\mathbf{w}^{\\left(n\\right)} \\right) \\] By assumption, \\(\\lambda_{1}\\) is the largest eigenvalue of \\(\\mathbf{A}\\), so that \\[ \\left|\\dfrac{\\lambda_{i}}{\\lambda_{1}}\\right|&lt;1,\\quad i\\in\\left\\{2,\\ldots,n\\right\\}. \\] Thus, \\[ M\\rightarrow\\infty\\implies\\left(\\dfrac{\\lambda_{i}}{\\lambda_{1}}\\right)^{M}\\rightarrow 0, \\] so that for some large but finite \\(M\\), \\[ \\mathbf{v}^{\\left(M\\right)}\\approx \\lambda_{1}^{M}\\left( c_{1}\\mathbf{w}^{\\left(1\\right)}+ c_{2}\\cdot 0\\cdot\\mathbf{w}^{\\left(2\\right)}+\\cdots+ c_{n}\\cdot 0\\cdot\\mathbf{w}^{\\left(n\\right)} \\right)= \\lambda_{1}^{M}c_{1}\\mathbf{w}^{\\left(1\\right)}. \\] Let \\(\\tilde{\\mathbf{w}}^{\\left(1\\right)}=\\lambda_{1}^{M}c_{1}\\mathbf{w}^{\\left(1\\right)}\\), so that \\(\\mathbf{v}^{\\left(M\\right)}\\approx\\tilde{\\mathbf{w}}^{\\left(1\\right)}\\). The final step of the algorithm is to normalize \\(\\mathbf{v}^{\\left(M\\right)}\\), i.e., \\[ \\mathbf{v}^{\\left(M\\right)}\\approx \\dfrac{\\tilde{\\mathbf{w}}^{\\left(1\\right)}}{\\sum_{j}\\tilde{w}_{j}^{\\left(1\\right)}}. \\] Thus, we see that \\(\\mathbf{v}^{\\left(M\\right)}\\) approximates the dominant eigenvector \\(\\mathbf{w}^{\\left(1\\right)}\\) of \\(\\mathbf{A}\\), completing the proof sketch. 8.4 Considerations 8.4.1 Connection to Markov chains We can model the web surfer’s behavior by a Markov chain with transition probability matrix \\(\\mathbf{A}\\). Suppose that \\(\\boldsymbol{\\pi}\\) is the stationary distribution of the chain, so that \\[ \\boldsymbol{\\pi}^{\\mathsf{T}}\\mathbf{A}=\\boldsymbol{\\pi}^\\mathsf{T}\\implies \\mathbf{A}^\\mathsf{T}\\boldsymbol{\\pi}=\\boldsymbol{\\pi}. \\] We can thus view power iteration as finding the stationary distribution of the Markov chain, the \\(j\\text{th}\\) element of which can be interpreted as the long-run proportion of time the chain spends in state \\(j\\). Observe also that power iteration normalizes \\(\\mathbf{v}^{\\left(i\\right)}\\) such that its components sum to 1, as required for a probability distribution. 8.4.2 Calculating the dominant eigenvalue We stated above that we can find the eigenvalue corresponding to \\(\\mathbf{w}^{\\left(1\\right)}\\) by the Rayleigh quotient. We have \\[ \\dfrac{\\left(\\mathbf{w}^{\\left(1\\right)}\\right)^{\\mathsf{T}}\\mathbf{A}\\mathbf{w}^{\\left(1\\right)}}{\\left\\Vert\\mathbf{w}^{\\left(1\\right)}\\right\\Vert_{2}^{2}}= \\dfrac{\\lambda_{1}\\left(\\mathbf{w}^{\\left(1\\right)}\\right)^{\\mathsf{T}}\\mathbf{w}^{\\left(1\\right)}}{\\left\\Vert\\mathbf{w}^{\\left(1\\right)}\\right\\Vert_{2}^{2}}= \\lambda_{1}\\dfrac{\\left\\Vert\\mathbf{w}^{\\left(1\\right)}\\right\\Vert_{2}^{2}}{\\left\\Vert\\mathbf{w}^{\\left(1\\right)}\\right\\Vert_{2}^{2}}= \\lambda_{1}, \\] as desired. 8.4.3 Computational complexity At each step of power iteration, we must compute \\(\\mathbf{A}\\mathbf{v}^{\\left(i-1\\right)}\\). The first entry of this product is obtained by summing the product of the first row of \\(\\mathbf{A}\\) with \\(\\mathbf{v}^{\\left(i-1\\right)}\\), which requires \\(n\\) multiplications. We have \\(\\mathbf{A}\\sim n\\times n\\), i.e., we must repeat this process \\(n\\) times, so that computing \\(\\mathbf{A}\\mathbf{v}^{\\left(i-1\\right)}\\) requires \\(n^{2}\\) multiplications. Thus, each step of power iteration has \\(O\\left(n^{2}\\right)\\) complexity. While this is an improvement over cubic complexity, it is still intractable for real-world problems, e.g., if we assume that the Internet has on the order of one billion web pages, i.e., \\(n=10^{9}\\), then power iteration requires \\(10^{18}\\) multiplications at each step. If we suppose that a typical laptop computer can execute on the order of one billion operations per second, then one step of power iteration on the Internet would require \\(10^{18}/10^{9}=10^{9}\\) seconds, or roughly 32 years (to say nothing of the memory requirements). Observe that the quadratic complexity of a step of power iteration arises from our assumption that each component of \\(\\mathbf{A}\\mathbf{v}^{\\left(i-1\\right)}\\) must be computed. If we knew in advance the result of certain of these multiplications, we might not have to perform them. In particular, if we knew that the \\(\\left(i,j\\right)\\text{th}\\) entry of \\(\\mathbf{A}\\) were zero, then we would also immediately know that the \\(j\\text{th}\\) summand in the product of the \\(i\\text{th}\\) row of \\(\\mathbf{A}\\) and \\(\\mathbf{v}^{\\left(i-1\\right)}\\) is zero, and we could avoid doing that multiplication. Sparse matrices enable precisely this kind of savings: they represent matrices in such a way as to avoid multiplications by zero. It remains to consider the characteristics of the network of interest, so that we might determine whether a sparse representation would be advantageous. Suppose that each web page links to on the order of 10 other pages. In this case, only 10 entries of each row of \\(\\mathbf{A}\\) are nonzero, and the remaining \\(10^{9}-10\\) entries are zero. We must perform just 10 multiplications per row, and \\(\\mathbf{A}\\) has \\(10^{9}\\) rows, so that a single step of power iteration requires just \\(10^{10}\\) multiplications, or roughly 10 seconds at \\(10^{9}\\) operations per second. 8.4.4 Convergence The approximation \\(\\mathbf{v}^{\\left(M\\right)}\\approx\\mathbf{w}^{\\left(1\\right)}\\) depends on the terms involving the other eigenvectors \\(\\left\\{\\mathbf{w}^{\\left(i\\right)}\\right\\}_{i=2}^{n}\\) shrinking toward the zero vector. The rate at which the \\(i\\text{th}\\) term converges to the zero vector is given by the ratio of \\(\\lambda_{i}\\) to \\(\\lambda_{1}\\). We assume that the eigenvalues are ordered by absolute value, hence the largest such term is \\(\\lambda_{2}/\\lambda_{1}\\), and this term will determine the rate of convergence of power iteration, e.g., if this ratio is close to 1, then the algorithm may converge slowly. For large matrices, \\(\\lambda_{2}/\\lambda_{1}\\) will be close to 1. Google dealt with the slow convergence by modifying the web surfer model. First, choose \\(p\\in\\left[0,1\\right]\\) (Google originally chose \\(p\\approx0.15\\)). Then, assume that with probability \\(p\\) the web surfer randomly, with uniform probability, jumps to any page in the network given in \\(\\mathbf{A}\\) and with probability \\(\\left(1-p\\right)\\) the surfer randomly, with uniform probability, jumps to a page with a link given in the current page. Thus, rather than surfing behavior governed exclusively by the probability transition matrix, the surfer may also make a random jump to any page in the network. We can represent this new behavior by replacing \\(\\mathbf{A}\\) by \\[ \\mathbf{M}=\\left(1-p\\right)\\mathbf{A}+p\\mathbf{B}, \\] where \\(\\mathbf{B}\\) is a matrix with all entries given by \\(1/n\\), which \\(n\\) is again the number of pages in the network. Observe that if \\(p=0\\) this is equivalent to the original model. The surfer’s behavior under this model can similarly be modeled by a Markov chain whose stationary distribution \\(\\mathbf{v}\\) satisfies \\(\\mathbf{M}^{\\mathsf{T}}\\mathbf{v}=\\mathbf{v}\\). We can again approximate the stationary distribution by power iteration. We must be careful in implementation because while \\(\\mathbf{A}\\) is typically sparse, \\(\\mathbf{M}\\) never is (\\(\\mathbf{B}\\) has all non-zero entries). It turns out that we can decompose \\(\\mathbf{M}^{\\mathsf{T}}\\mathbf{v}\\) as \\[ \\mathbf{M}^{\\mathsf{T}}\\mathbf{v}= \\left(1-p\\right)\\mathbf{A}^{\\mathsf{T}}\\mathbf{v}+\\dfrac{p}{n}\\mathbf{1}\\sum_{i=1}^{n}v_{i}, \\] where \\(\\mathbf{1}\\) is the \\(n\\)-dimensional vector of ones. This decomposition allows us to compute the dominant eigenvector of the sparse matrix \\(\\mathbf{A}^\\mathsf{T}\\) rather than the dense matrix \\(\\left(\\left(1-p\\right)\\mathbf{A}+p\\mathbf{B}\\right)^{\\mathsf{T}}\\), with the accompanying decrease in computational complexity. We now implement power iteration for this model. #&#39; Modified power iteration #&#39; #&#39; @param A adjacency matrix of original network #&#39; @param v0 starting vector #&#39; @param p probability of jumping to any page in `A` #&#39; @param tol iteration tolerance #&#39; @param niter maximum number of iterations #&#39; #&#39; @return list containing the number of iterations to converge and the #&#39; dominant eigenvector of `A` power_iter &lt;- function(A, v0, p, tol = 1e-4, niter = 1e3) { mag &lt;- function(x) sqrt(sum(x ^ 2)) # normalize v0 so that convergence can occur in a single iteration v_old &lt;- v0 / sum(v0) i &lt;- 0 delta &lt;- tol + 1 # this term does not change from iteration to iteration u &lt;- (p / length(v0)) * rep(1, length(v0)) while (i &lt; niter &amp;&amp; delta &gt; tol) { v_new &lt;- (1 - p) * crossprod(A, v_old) + sum(v_old) * u v_new &lt;- v_new / sum(v_new) delta &lt;- mag(v_new - v_old) v_old &lt;- v_new i &lt;- i + 1 } list( niter = i, v = as.vector(v_new) ) } We will test our implementation using the General Relativity network from the Stanford Network Analysis Project. path &lt;- &quot;data/ca-GrQc.txt.gz&quot; if (!fs::file_exists(path)) { download.file( &quot;http://snap.stanford.edu/data/ca-GrQc.txt.gz&quot;, destfile = path, quiet = TRUE ) } gr &lt;- readr::read_tsv(path, skip = 4, col_names = c(&quot;from&quot;, &quot;to&quot;)) gr ## # A tibble: 28,980 x 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 3466 937 ## 2 3466 5233 ## 3 3466 8579 ## 4 3466 10310 ## 5 3466 15931 ## 6 3466 17038 ## 7 3466 18720 ## 8 3466 19607 ## 9 10310 1854 ## 10 10310 3466 ## # ... with 28,970 more rows We now implement a function to perform power iteration, measure its runtime, and extract the name of the top node, i.e., \\(v_{\\max}=\\max_{j\\in\\left\\{1,\\ldots,n\\right\\}}v_{j}\\). #&#39; Power iteration runtime and top node #&#39; #&#39; @param A adjacency matrix for power iteration #&#39; @param p probability of jumping to any page in `A` #&#39; @param ... other arguments passed to `power_iter()` #&#39; #&#39; @return list containing `p`, the name of the top node, the number of #&#39; iterations required to converge, and the runtime top_node &lt;- function(A, p, ...) { runtime &lt;- system.time( pi_result &lt;- power_iter( A = A, v0 = rep(1, nrow(A)), p = p, ... ) ) # extract the (first) top-rated node v_max &lt;- (rownames(A))[which.max(pi_result$v)] list( p = p, top_node = v_max, niter = pi_result$niter, time = runtime[&quot;elapsed&quot;] ) } Finally, we implement a function to perform power iteration for multiple values of \\(p\\). Note that gr is an edgelist. #&#39; Power iteration for multiple jump probabilities with sparse matrices #&#39; #&#39; @param el data frame containing a symbolic edge list in the first two #&#39; columns; passed to `igraph::graph_from_data_frame()` #&#39; @param p vector of probabilities #&#39; @param sparse whether to use sparse matrices #&#39; #&#39; @return a tibble containing a row for each value of `p`, the name of the #&#39; top node, the number of iterations required to converge, and the runtime pagerank &lt;- function(el, p, sparse = TRUE) { graph &lt;- igraph::graph_from_data_frame(el) adj &lt;- igraph::as_adjacency_matrix(graph, sparse = sparse) purrr::map_dfr(p, top_node, A = adj) } We are now ready to examine the impact of \\(p\\). prob &lt;- c(10 ^ -(6:2), 0.15, 0.5, 0.9, 0.99) pr_dense &lt;- pagerank(gr, p = prob, sparse = FALSE) pr_dense ## # A tibble: 9 x 4 ## p top_node niter time ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000001 21012 31 0.886 ## 2 0.00001 21012 31 0.916 ## 3 0.0001 21012 31 0.960 ## 4 0.001 21012 31 0.946 ## 5 0.01 21012 31 1.01 ## 6 0.15 21012 31 0.963 ## 7 0.5 21012 31 0.972 ## 8 0.9 21012 30 0.902 ## 9 0.99 21012 4 0.117 We see that the top-rated node is 21012, and that 31 iterations were required for most values of \\(p\\). As \\(p\\) becomes large, the number of iterations required to converge decreases. We now repeat the process with sparse matrices. pr_sparse &lt;- pagerank(gr, p = prob) pr_sparse ## # A tibble: 9 x 4 ## p top_node niter time ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000001 21012 31 0.0520 ## 2 0.00001 21012 31 0.039 ## 3 0.0001 21012 31 0.037 ## 4 0.001 21012 31 0.035 ## 5 0.01 21012 31 0.035 ## 6 0.15 21012 31 0.0330 ## 7 0.5 21012 31 0.0330 ## 8 0.9 21012 30 0.035 ## 9 0.99 21012 4 0.00500 We see that the same number of iterations are required to converge as in the dense case, but that the time required is decreased by roughly two orders of magnitude. Finally, observe that node 21012 is also the node with the largest number of adjacent vertices (connections) (note that the ego graph of a vertex includes the vertex itself): graph &lt;- igraph::graph_from_data_frame(gr) igraph::neighbors(graph, v = &quot;21012&quot;) ## + 81/5242 vertices, named, from eef9b3c: ## [1] 10243 6610 22691 2980 18866 25758 11241 13597 3409 15538 570 ## [12] 8503 18719 9889 773 9341 21847 6179 1997 2741 13060 14807 ## [23] 24955 45 4511 21281 23293 9482 15003 20635 22457 19423 5134 ## [34] 3372 23452 23628 2404 22421 18894 18208 1234 25053 18543 4164 ## [45] 7956 12365 17655 25346 1653 9785 21508 14540 12781 1186 345 ## [56] 2212 231 46 19961 2952 6830 8879 11472 12496 12851 15659 ## [67] 17692 20108 20562 22887 6774 4513 25251 12503 22937 23363 5578 ## [78] 1841 16611 2450 8049 graph %&gt;% igraph::ego_size() %&gt;% max() ## [1] 82 Finally, observe that PageRank is a variant of eigenvector centrality: ec &lt;- igraph::eigen_centrality(graph) ec$vector[which.max(ec$vector)] ## 21012 ## 1 In eigenvector centrality, the score of a node is increased more by (inbound) connections from high-scoring nodes than from low-scoring nodes. There are several other important types of centrality, e.g., betweenness centrality, in which the score of a node is determined by how often it appears in the shortest path between two other nodes (how often it acts as a “bridge”). References "],
["references.html", "References", " References "]
]
