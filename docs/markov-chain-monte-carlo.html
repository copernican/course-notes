<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Master’s Companion</title>
  <meta name="description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="A Master’s Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Master’s Companion" />
  
  <meta name="twitter:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

<meta name="author" content="Sean Wilson">


<meta name="date" content="2018-11-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="em-algorithm.html">
<link rel="next" href="pagerank.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\newcommand{\coloneqq}{\mathrel{\mathop:}\mathrel{\mkern-1.2mu}=}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Master's Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="part"><span><b>I Background material</b></span></li>
<li class="chapter" data-level="2" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>2</b> Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="analysis.html"><a href="analysis.html#upper-bounds-and-suprema"><i class="fa fa-check"></i><b>2.1</b> Upper bounds and suprema</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>3</b> Probability theory</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-theory.html"><a href="probability-theory.html#background-material"><i class="fa fa-check"></i><b>3.1</b> Background material</a></li>
<li class="chapter" data-level="3.2" data-path="probability-theory.html"><a href="probability-theory.html#transformations-and-expectations"><i class="fa fa-check"></i><b>3.2</b> Transformations and expectations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-theory.html"><a href="probability-theory.html#distributions-of-functions-of-a-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Distributions of functions of a random variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-theory.html"><a href="probability-theory.html#expected-values"><i class="fa fa-check"></i><b>3.2.2</b> Expected values</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-theory.html"><a href="probability-theory.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>3.2.3</b> Moments and moment generating functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-theory.html"><a href="probability-theory.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.3</b> Multiple random variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="probability-theory.html"><a href="probability-theory.html#conditional-distributions-and-independence"><i class="fa fa-check"></i><b>3.3.1</b> Conditional distributions and independence</a></li>
<li class="chapter" data-level="3.3.2" data-path="probability-theory.html"><a href="probability-theory.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.3.2</b> Covariance and correlation</a></li>
<li class="chapter" data-level="3.3.3" data-path="probability-theory.html"><a href="probability-theory.html#multivariate-distributions"><i class="fa fa-check"></i><b>3.3.3</b> Multivariate distributions</a></li>
<li class="chapter" data-level="3.3.4" data-path="probability-theory.html"><a href="probability-theory.html#inequalities"><i class="fa fa-check"></i><b>3.3.4</b> Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-theory.html"><a href="probability-theory.html#properties-of-a-random-sample"><i class="fa fa-check"></i><b>3.4</b> Properties of a random sample</a><ul>
<li class="chapter" data-level="3.4.1" data-path="probability-theory.html"><a href="probability-theory.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i><b>3.4.1</b> Sums of random variables from a random sample</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability-theory.html"><a href="probability-theory.html#sampling-from-the-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Sampling from the normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="probability-theory.html"><a href="probability-theory.html#order-statistics"><i class="fa fa-check"></i><b>3.4.3</b> Order statistics</a></li>
<li class="chapter" data-level="3.4.4" data-path="probability-theory.html"><a href="probability-theory.html#convergence-concepts"><i class="fa fa-check"></i><b>3.4.4</b> Convergence concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Linear algebra</a></li>
<li class="part"><span><b>II Mathematical statistics</b></span></li>
<li class="chapter" data-level="5" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html"><i class="fa fa-check"></i><b>5</b> Common families of distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#defn-exp-family"><i class="fa fa-check"></i><b>5.1</b> Exponential families</a><ul>
<li class="chapter" data-level="5.1.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#natural-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Natural parameters</a></li>
<li class="chapter" data-level="5.1.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#conjugate-prior-distributions"><i class="fa fa-check"></i><b>5.1.2</b> Conjugate prior distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-and-scale-families"><i class="fa fa-check"></i><b>5.2</b> Location and scale families</a><ul>
<li class="chapter" data-level="5.2.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-families"><i class="fa fa-check"></i><b>5.2.1</b> Location families</a></li>
<li class="chapter" data-level="5.2.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#scale-families"><i class="fa fa-check"></i><b>5.2.2</b> Scale families</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>6</b> Point estimation</a><ul>
<li class="chapter" data-level="6.1" data-path="point-estimation.html"><a href="point-estimation.html#methods-of-finding-estimators"><i class="fa fa-check"></i><b>6.1</b> Methods of finding estimators</a><ul>
<li class="chapter" data-level="6.1.1" data-path="point-estimation.html"><a href="point-estimation.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>6.1.1</b> Maximum likelihood estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>7</b> Generalized linear models</a></li>
<li class="chapter" data-level="8" data-path="machine-representation.html"><a href="machine-representation.html"><i class="fa fa-check"></i><b>8</b> Machine representation</a><ul>
<li class="chapter" data-level="8.1" data-path="machine-representation.html"><a href="machine-representation.html#binary-numbers"><i class="fa fa-check"></i><b>8.1</b> Binary numbers</a></li>
<li class="chapter" data-level="8.2" data-path="machine-representation.html"><a href="machine-representation.html#integers"><i class="fa fa-check"></i><b>8.2</b> Integers</a></li>
<li class="chapter" data-level="8.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-numbers"><i class="fa fa-check"></i><b>8.3</b> Floating-point numbers</a><ul>
<li class="chapter" data-level="8.3.1" data-path="machine-representation.html"><a href="machine-representation.html#special-exponent-values"><i class="fa fa-check"></i><b>8.3.1</b> Special exponent values</a></li>
<li class="chapter" data-level="8.3.2" data-path="machine-representation.html"><a href="machine-representation.html#limitations"><i class="fa fa-check"></i><b>8.3.2</b> Limitations</a></li>
<li class="chapter" data-level="8.3.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-error"><i class="fa fa-check"></i><b>8.3.3</b> Floating-point error</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="em-algorithm.html"><a href="em-algorithm.html"><i class="fa fa-check"></i><b>9</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.1" data-path="em-algorithm.html"><a href="em-algorithm.html#motivation"><i class="fa fa-check"></i><b>9.1</b> Motivation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="em-algorithm.html"><a href="em-algorithm.html#k-means"><i class="fa fa-check"></i><b>9.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="em-algorithm.html"><a href="em-algorithm.html#em-algorithm-1"><i class="fa fa-check"></i><b>9.2</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.2.1" data-path="em-algorithm.html"><a href="em-algorithm.html#algorithmic-perspective"><i class="fa fa-check"></i><b>9.2.1</b> Algorithmic perspective</a></li>
<li class="chapter" data-level="9.2.2" data-path="em-algorithm.html"><a href="em-algorithm.html#statistical-perspective"><i class="fa fa-check"></i><b>9.2.2</b> Statistical perspective</a></li>
<li class="chapter" data-level="9.2.3" data-path="em-algorithm.html"><a href="em-algorithm.html#proof-sketch"><i class="fa fa-check"></i><b>9.2.3</b> Proof sketch</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="em-algorithm.html"><a href="em-algorithm.html#example-gaussian-mixture"><i class="fa fa-check"></i><b>9.3</b> Example: Gaussian mixture</a></li>
<li class="chapter" data-level="9.4" data-path="em-algorithm.html"><a href="em-algorithm.html#applications"><i class="fa fa-check"></i><b>9.4</b> Applications</a><ul>
<li class="chapter" data-level="9.4.1" data-path="em-algorithm.html"><a href="em-algorithm.html#factor-analysis"><i class="fa fa-check"></i><b>9.4.1</b> Factor analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>10</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="10.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#motivation-1"><i class="fa fa-check"></i><b>10.1</b> Motivation</a><ul>
<li class="chapter" data-level="10.1.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#ising-model"><i class="fa fa-check"></i><b>10.1.1</b> Ising model</a></li>
<li class="chapter" data-level="10.1.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#intractable-posterior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Intractable posterior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-is-a-sampling-technique"><i class="fa fa-check"></i><b>10.1.3</b> MCMC is a sampling technique</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>10.2</b> Markov chain</a></li>
<li class="chapter" data-level="10.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#detailed-balance"><i class="fa fa-check"></i><b>10.3</b> Detailed balance</a></li>
<li class="chapter" data-level="10.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolis-hastings"><i class="fa fa-check"></i><b>10.4</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="10.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.5</b> Gibbs Sampling</a><ul>
<li class="chapter" data-level="10.5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#latent-dirichlet-allocation"><i class="fa fa-check"></i><b>10.5.1</b> Latent Dirichlet Allocation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pagerank.html"><a href="pagerank.html"><i class="fa fa-check"></i><b>11</b> PageRank</a><ul>
<li class="chapter" data-level="11.1" data-path="pagerank.html"><a href="pagerank.html#motivation-2"><i class="fa fa-check"></i><b>11.1</b> Motivation</a></li>
<li class="chapter" data-level="11.2" data-path="pagerank.html"><a href="pagerank.html#computing-eigenpairs"><i class="fa fa-check"></i><b>11.2</b> Computing eigenpairs</a></li>
<li class="chapter" data-level="11.3" data-path="pagerank.html"><a href="pagerank.html#algorithm"><i class="fa fa-check"></i><b>11.3</b> Algorithm</a></li>
<li class="chapter" data-level="11.4" data-path="pagerank.html"><a href="pagerank.html#considerations"><i class="fa fa-check"></i><b>11.4</b> Considerations</a><ul>
<li class="chapter" data-level="11.4.1" data-path="pagerank.html"><a href="pagerank.html#connection-to-markov-chains"><i class="fa fa-check"></i><b>11.4.1</b> Connection to Markov chains</a></li>
<li class="chapter" data-level="11.4.2" data-path="pagerank.html"><a href="pagerank.html#calculating-the-dominant-eigenvalue"><i class="fa fa-check"></i><b>11.4.2</b> Calculating the dominant eigenvalue</a></li>
<li class="chapter" data-level="11.4.3" data-path="pagerank.html"><a href="pagerank.html#computational-complexity"><i class="fa fa-check"></i><b>11.4.3</b> Computational complexity</a></li>
<li class="chapter" data-level="11.4.4" data-path="pagerank.html"><a href="pagerank.html#convergence-1"><i class="fa fa-check"></i><b>11.4.4</b> Convergence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Master’s Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-chain-monte-carlo" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Markov Chain Monte Carlo</h1>
<div id="motivation-1" class="section level2">
<h2><span class="header-section-number">10.1</span> Motivation</h2>
<p>We now introduce a general sampling technique that is often used to sample from intractable distributions. Previous techniques we have seen have required information that we may not always have, such as the inverse cumulative distribution function (CDF inversion), or a distribution from which we know how to sample that envelops a target distribution (accept-reject). We begin by introducing two seemingly intractable problems.</p>
<div id="ising-model" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Ising model</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> is used in statistical mechanics to model ferromagnetism. The vertices in the graph below represent magnetic dipole moments of atomic spins, and each vertex takes one of the values <span class="math inline">\(\left\{-1,1\right\}\)</span>, with black and white representing <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>, respectively.</p>
<p><img src="course-notes_files/figure-html/ising-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Each vertex can take one of two possible values, and there are 25 such vertices, so that there are <span class="math inline">\(2^{25}\)</span> possible configurations. For an <span class="math inline">\(n\times n\)</span> grid, there are <span class="math inline">\(2^{n^{2}}\)</span> possible configurations. It is clear that even for modest <span class="math inline">\(n\)</span>, e.g., <span class="math inline">\(n=10\)</span>, the number of possible configurations will be huge. Suppose that we wish to sample uniformly from the set of all possible configurations. It is not clear that we have a procedure that will produce uniform samples, i.e., a procedure such that each of the <span class="math inline">\(2^{n^{2}}\)</span> possible configurations is equally likely.</p>
<p>Suppose further that we wish to sample according to a more complicated scheme. Let <span class="math inline">\(E\)</span> be the set of all possible configurations, so that <span class="math inline">\(\left|E\right|=2^{n^{2}}\)</span>. Let <span class="math inline">\(v\in E\)</span> be some configuration, and let <span class="math inline">\(\sigma_{ij}\)</span> be the state of vertex <span class="math inline">\(\left(i,j\right)\)</span>, so that <span class="math inline">\(\sigma_{ij}\in\left\{-1,1\right\}\)</span>, and where we have indexed the vertices as a matrix, i.e., <span class="math inline">\(i\)</span> indicates the row and <span class="math inline">\(j\)</span> the column. Let</p>
<p><span class="math display">\[
H\left(v\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{ij}\left(\sigma_{i+1,j}+\sigma_{i,j+1}+\sigma_{i-1,j}+\sigma_{i,j-1}\right),
\]</span></p>
<p>i.e., <span class="math inline">\(H\left(v\right)\)</span> is the sum of the product of the state of each vertex multiplied by its neighboring vertices (and suppose that we “wrap” vertices on an edge). Let <span class="math inline">\(X\)</span> be the random variable that samples from <span class="math inline">\(E\)</span> according to <span class="math inline">\(P\left(X=v\right)=c\cdot\mathrm{e}^{H\left(v\right)}\)</span>, where <span class="math inline">\(c\)</span> is some normalizing constant (to make this a valid probability distribution). Again, it is not clear that we have a procedure to sample from <span class="math inline">\(X\)</span>.</p>
</div>
<div id="intractable-posterior-distribution" class="section level3">
<h3><span class="header-section-number">10.1.2</span> Intractable posterior distribution</h3>
<p>Latent Dirichlet Allocation is a generative Bayesian model that describes a process for creating documents. A document is viewed as a collection of words drawn from one or more latent topics. In this model, neither grammar nor the order of words is considered. Rather, the document is viewed as a “bag of words,” with each word arising from a particular topic. Each document has a distribution over topics, and each topic has a distribution over words. We are typically interested in the posterior distribution of the assignments of words to topics</p>
<p><span class="math display">\[
P\left(\mathbf{z}|\mathbf{w}\right)=
\frac{P\left(\mathbf{w},\mathbf{z}\right)}{\sum_{\mathbf{z}}P\left(\mathbf{w},\mathbf{z}\right)},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{z}\)</span> represents the topics and <span class="math inline">\(\mathbf{w}\)</span> represents the words in the corpus. <span class="citation">Griffiths and Steyvers (<a href="#ref-Griffiths2004">2004</a>)</span> note that “this distribution cannot be computed directly, because the sum in the denominator does not factorize and involves <span class="math inline">\(T^{n}\)</span> terms, where <span class="math inline">\(n\)</span> is the total number of word instances in the corpus.” <span class="citation">Blei, Ng, and Jordan (<a href="#ref-Blei2003">2003</a>)</span> estimate the distribution using a version of the expectation-maximization algorithm. We will see that it is also possible to estimate the distribution using Markov Chain Monte Carlo methods.</p>
</div>
<div id="mcmc-is-a-sampling-technique" class="section level3">
<h3><span class="header-section-number">10.1.3</span> MCMC is a sampling technique</h3>
<p>Monte Carlo methods are a class of computational methods for estimating some quantity via sampling. The idea behind Markov Chain Monte Carlo is to build a Markov chain <span class="math inline">\(X\left(t\right)\)</span> with state space <span class="math inline">\(S=E\)</span> and stationary distribution <span class="math inline">\(\boldsymbol{\pi}\)</span> such that <span class="math inline">\(\boldsymbol{\pi}\)</span> is the distribution on <span class="math inline">\(E\)</span> from which we want to sample (<span class="math inline">\(\pi\left(i\right)=c\cdot\mathrm{e}^{H\left(i\right)}\)</span> for the Ising model). We will soon see that, under certain conditions, the limiting distribution of the Markov chain is <span class="math inline">\(\boldsymbol{\pi}\)</span>. Thus, if we can build such a chain, then we can run our sampler for a long (but finite) time, and we will be able to sample from <span class="math inline">\(\boldsymbol{\pi}\)</span>. We can therefore view MCMC as a sampling technique, where given <span class="math inline">\(\boldsymbol{\pi}\)</span>, our task is to find <span class="math inline">\(\mathbf{P}\)</span> such that <span class="math inline">\(\boldsymbol{\pi}^{\mathsf{T}}\mathbf{P}=\boldsymbol{\pi}^{\mathsf{T}}\)</span>.</p>
</div>
</div>
<div id="markov-chain" class="section level2">
<h2><span class="header-section-number">10.2</span> Markov chain</h2>
<p>We begin by setting some notation. We define a (square) matrix <span class="math inline">\(\mathbf{A}\)</span> raised to the <span class="math inline">\(k\text{th}\)</span> power as</p>
<p><span class="math display">\[
\mathbf{A}^{k}=\prod_{i=1}^{k}\mathbf{A}.
\]</span></p>
<p>Thus, <span class="math inline">\(\mathbf{A}^{2}=\mathbf{A}\mathbf{A}\)</span>, <span class="math inline">\(\mathbf{A}^{3}=\mathbf{A}\mathbf{A}\mathbf{A}\)</span>, and so on.</p>

<div class="theorem">
<p><span id="thm:mc-ptm" class="theorem"><strong>Theorem 10.1  </strong></span>Let <span class="math inline">\(X\left(t\right)\)</span> be a finite-state Markov chain with transition probability matrix <span class="math inline">\(\mathbf{P}\)</span>. Then,</p>
<p><span class="math display">\[
P\left(\left\{X\left(t\right)=j\right\}|\left\{X\left(0\right)=i\right\}\right)=\left(\mathbf{P}^{t}\right)_{ij},
\]</span></p>
i.e., the probability that the chain is in state <span class="math inline">\(j\)</span> given that it started in state <span class="math inline">\(i\)</span> is the <span class="math inline">\(\left(i,j\right)\text{th}\)</span> entry of the transition probability matrix raised to the power <span class="math inline">\(t\)</span>.
</div>

<p>We are now to consider the limiting behavior of Markov chains.</p>

<div class="definition">
<span id="def:unnamed-chunk-89" class="definition"><strong>Definition 10.1  </strong></span>A distribution <span class="math inline">\(\boldsymbol{\pi}\)</span> is said to be a <em>stationary distribution</em> for a Markov chain <span class="math inline">\(X\left(t\right)\)</span> with state space <span class="math inline">\(S\)</span> and transition probability matrix <span class="math inline">\(\mathbf{P}\)</span> if <span class="math inline">\(\boldsymbol{\pi}^{\mathsf{T}}\mathbf{P}=\boldsymbol{\pi}^{\mathsf{T}}\)</span>.
</div>

<p>Observe that <span class="math inline">\(\boldsymbol{\pi}^{\mathsf{T}}\)</span> is a left eigenvector with eigenvalue 1. We now consider how to compute <span class="math inline">\(\boldsymbol{\pi}\)</span>. One option is to solve <span class="math inline">\(\boldsymbol{\pi}^{\mathsf{T}}\mathbf{P}=\boldsymbol{\pi}^{\mathsf{T}}\)</span> as a linear algebra problem, i.e., solve</p>
<p><span class="math display">\[
\sum_{i=1}^{n}\pi\left(i\right)=1\quad\text{subject to}\quad\pi\left(i\right)\geq 0.
\]</span></p>
<p>Solving such constrained optimization problems is in general difficult, especially as the dimension of the problem increases. A second option comes from considering the limiting behavior of a finite-state Markov chain <span class="math inline">\(X\left(t\right)\)</span>. The Perron-Frobenius theorem implies that <span class="math inline">\(X\left(t\right)\)</span> has a stationary distribution.</p>

<div class="theorem">
<p><span id="thm:mc-convergence" class="theorem"><strong>Theorem 10.2  </strong></span>Let <span class="math inline">\(X\left(t\right)\)</span> be a Markov chain with finitely many states and stationary distribution <span class="math inline">\(\boldsymbol{\pi}\)</span>. If the stationary distribution is unique (equivalently, the chain is <em>irreducible</em> or <em>ergodic</em>), then</p>
<span class="math display">\[
\lim_{t\rightarrow\infty}X\left(t\right)\sim\boldsymbol{\pi}.
\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(X\left(t\right)\)</span> be a Markov chain with transition probability matrix <span class="math inline">\(\mathbf{P}\)</span>. For simplicity, we will prove the result in the case that <span class="math inline">\(\mathbf{P}\)</span> is symmetric (the result holds for non-symmetric <span class="math inline">\(\mathbf{P}\)</span>, but the proof is considerably more complicated). From Theorem <a href="markov-chain-monte-carlo.html#thm:mc-ptm">10.1</a>, we have</p>
<p><span class="math display">\[
P\left(X\left(t\right)=j|X\left(0\right)=i\right)=\left(\mathbf{P}^{t}\right)_{ij}.
\]</span></p>
<p>Now, <span class="math inline">\(\mathbf{P}\)</span> is symmetric, so it follows from Theorem <a href="linear-algebra.html#thm:unitary-decomposition">4.6</a> that <span class="math inline">\(\mathbf{P}=\mathbf{Q}\mathbf{D}\mathbf{Q}^{\mathsf{T}}\)</span>, where we have used the fact that the transpose of an orthogonal matrix is equal to its inverse. Thus,</p>
<p><span class="math display">\[
P\left(X\left(t\right)=j|X\left(0\right)=i\right)=
  \left(\mathbf{P}^{t}\right)_{ij}=
  \left(\left(\mathbf{Q}\mathbf{D}\mathbf{Q}^{\mathsf{T}}\right)^{t}\right)_{ij}.
\]</span></p>
<p>Next, observe that</p>
<p><span class="math display">\[
\left(\mathbf{Q}\mathbf{D}\mathbf{Q}^{\mathsf{T}}\right)\left(\mathbf{Q}\mathbf{D}\mathbf{Q}^{\mathsf{T}}\right)=
\mathbf{Q}\mathbf{D}\mathbf{Q}^{-1}\mathbf{Q}\mathbf{D}\mathbf{Q}^{\mathsf{T}}=
\mathbf{Q}\mathbf{D}\mathbf{I}\mathbf{D}\mathbf{Q}^{\mathsf{T}}=
\mathbf{Q}\mathbf{D}^{2}\mathbf{Q}^{\mathsf{T}}.
\]</span></p>
<p>It follows that <span class="math inline">\(\left(\mathbf{Q}\mathbf{D}\mathbf{Q}^{\mathsf{T}}\right)^{t}=\mathbf{Q}\mathbf{D}^{t}\mathbf{Q}^{\mathsf{T}}\)</span>. Now, <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix whose (diagonal) entries are the eigenvalues of <span class="math inline">\(\mathbf{P}\)</span>, so that</p>
<p><span class="math display">\[
\mathbf{D}^{t}=
\begin{bmatrix}
\lambda_{1}^{t} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_{2}^{t} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_{n}^{t}
\end{bmatrix}.
\]</span></p>
<p>We factor out <span class="math inline">\(\lambda_{1}^{t}\)</span> to obtain</p>
<p><span class="math display">\[
P\left(X\left(t\right)=j|X\left(0\right)=i\right)=
  \lambda_{1}^{t}\left(\mathbf{Q}
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \left(\frac{\lambda_{2}}{\lambda_{1}}\right)^{t} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \left(\frac{\lambda_{n}}{\lambda_{1}}\right)^{t}
\end{bmatrix}\mathbf{Q}^{\mathsf{T}}\right)_{ij}.
\]</span></p>
<p>Suppose that the eigenvalues are distinct, i.e., <span class="math inline">\(\left|\lambda_{1}\right|&gt;\left|\lambda_{2}\right|&gt;\cdots&gt;\left|\lambda_{n}\right|\)</span>, and consider the limiting behavior of this quantity. Because <span class="math inline">\(\lambda_{1}\)</span> has the largest absolute value, <span class="math inline">\(\lambda_{i}/\lambda_{1}&lt;1\)</span> for <span class="math inline">\(i\in\left\{2,\ldots,n\right\}\)</span>. Thus,</p>
<p><span class="math display">\[
\lim_{t\rightarrow\infty}P\left(X\left(t\right)=j|X\left(0\right)=i\right)\approx
  \lambda_{1}^{t}\left(\mathbf{Q}
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0
\end{bmatrix}\mathbf{Q}^{\mathsf{T}}\right)_{ij}.
\]</span></p>
<p>Now, <span class="math inline">\(P\left(X\left(t\right)=j|X\left(0\right)=i\right)\)</span> is a probability, hence must be between zero and one. If <span class="math inline">\(\lambda_{1}&gt;1\)</span>, as <span class="math inline">\(t\)</span> increases, the expression above will become greater than one, so that <span class="math inline">\(P\left(X\left(t\right)=j|X\left(0\right)=i\right)\)</span> will not be a valid probability. If <span class="math inline">\(\lambda_{1}&lt;1\)</span>, then as <span class="math inline">\(t\)</span> increases, the expression will go to zero. By assumption, <span class="math inline">\(X\left(t\right)\)</span> is irreducible, i.e., it is possible to reach any state from any other state. If the probability of being in state <span class="math inline">\(j\)</span> goes to zero, then the chain will not be irreducible (<span class="math inline">\(j\)</span> is arbitrary, and the chain must be <em>somewhere</em>), violating the assumption. It follows that <span class="math inline">\(\lambda_{1}\)</span> cannot be less than one, which implies that <span class="math inline">\(\lambda_{1}=1\)</span>.</p>
<p>Recalling that the columns of <span class="math inline">\(\mathbf{Q}\)</span> are the eigenvectors of <span class="math inline">\(\mathbf{P}\)</span>, it follows that</p>
<p><span class="math display">\[
\mathbf{P}\mathbf{q}_{1}=\lambda_{1}\mathbf{q}_{1}=\mathbf{q}_{1},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{q}_{i}\)</span> is the <span class="math inline">\(i\text{th}\)</span> eigenvector of <span class="math inline">\(\mathbf{Q}\)</span>. Taking the transpose of this expression, we have</p>
<p><span class="math display">\[
\left(\mathbf{P}\mathbf{q}_{1}\right)^{\mathsf{T}}=\mathbf{q}_{1}^{\mathsf{T}}
\implies \mathbf{q}_{1}^{\mathsf{T}}\mathbf{P}^{\mathsf{T}}=\mathbf{q}_{1}^{\mathsf{T}},
\]</span></p>
By assumption, <span class="math inline">\(\mathbf{P}\)</span> is symmetric, i.e., <span class="math inline">\(\mathbf{P}^{\mathsf{T}}=\mathbf{P}\)</span>, so that <span class="math inline">\(\mathbf{q}_{1}^{\mathsf{T}}\mathbf{P}=\mathbf{q}_{1}^{\mathsf{T}}\)</span>, i.e., <span class="math inline">\(\mathbf{q}_{1}^{\mathsf{T}}\)</span> is a left eigenvector of <span class="math inline">\(\mathbf{P}\)</span>. Thus, the limiting distribution of <span class="math inline">\(X\left(t\right)\)</span> is <span class="math inline">\(\mathbf{q}_{1}\)</span>, so that <span class="math inline">\(\mathbf{q}_{1}=\boldsymbol{\pi}\)</span>, i.e., <span class="math inline">\(\mathbf{q}_{1}\)</span> is the stationary distribution.
</div>


<div class="corollary">
<span id="cor:unnamed-chunk-91" class="corollary"><strong>Corollary 10.1  </strong></span>If <span class="math inline">\(\mathbf{P}\)</span> is a transition probability matrix for an irreducible, finite-state Markov chain <span class="math inline">\(X\left(t\right)\)</span>, then its eigenvalues have absolute value less than or equal to <span class="math inline">\(1\)</span>, i.e., <span class="math inline">\(\left|\lambda_{i}\right|\leq 1\,\forall i\)</span>. Further, <span class="math inline">\(\mathbf{P}\)</span> has exactly one eigenvalue equal to <span class="math inline">\(1\)</span>, and it corresponds to the eigenvector that is the stationary distribution of <span class="math inline">\(X\left(t\right)\)</span>.
</div>

<p>It is not difficult to find a finite-state Markov chain that does not have a unique stationary distribution. Consider a chain with four states and transition probability matrix</p>
<p><span class="math display">\[
\begin{bmatrix}
\frac{1}{2} &amp; \frac{1}{2} &amp; 0 &amp; 0 \\
\frac{1}{2} &amp; \frac{1}{2} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \frac{1}{2} &amp; \frac{1}{2} \\
0 &amp; 0 &amp; \frac{1}{2} &amp; \frac{1}{2} \\
\end{bmatrix}
\]</span></p>
<p>In this chain, states 1 and 2 do not communicate with states 3 and 4, and it is easy to see that the chain has two stationary distributions. If <span class="math inline">\(X\left(0\right)\in\left\{1,2\right\}\)</span>, then <span class="math inline">\(\boldsymbol{\pi}=\left(1/2,1/2,0,0\right)\)</span>, and if <span class="math inline">\(X\left(0\right)\in\left\{3,4\right\}\)</span>, then <span class="math inline">\(\boldsymbol{\pi}=\left(0,0,1/2,1/2\right)\)</span>.</p>
</div>
<div id="detailed-balance" class="section level2">
<h2><span class="header-section-number">10.3</span> Detailed balance</h2>
<p>We have said that we can use MCMC to sample from an otherwise intractable distribution if we construct a Markov chain whose stationary distribution is the target distribution. We have also proved that the limiting distribution of a finite-state Markov chain is the stationary distribution (provided it is unique). Our attention now turns to constructing such a chain.</p>

<div class="definition">
<span id="def:unnamed-chunk-92" class="definition"><strong>Definition 10.2  </strong></span>A distribution <span class="math inline">\(\boldsymbol{\nu}\)</span> on a state space <span class="math inline">\(S\)</span> is said to be in <em>detailed balance</em> for a Markov chain <span class="math inline">\(X\left(t\right)\)</span> on <span class="math inline">\(S\)</span> with transition probability matrix <span class="math inline">\(\mathbf{P}\)</span> if <span class="math inline">\(\nu\left(i\right)P_{ij}=\nu\left(j\right)P_{ji}\)</span>.
</div>

<p>Practically, it is difficult to produce a transition probability matrix <span class="math inline">\(\mathbf{P}\)</span> that makes <span class="math inline">\(\boldsymbol{\pi}\)</span> stationary. It is often easier to produce a <span class="math inline">\(\mathbf{P}\)</span> such that <span class="math inline">\(\boldsymbol{\pi}\)</span> is in detailed balance.</p>

<div class="theorem">
<span id="thm:detailed-balance" class="theorem"><strong>Theorem 10.3  </strong></span>If <span class="math inline">\(\boldsymbol{\nu}\)</span> is in detailed balance for a Markov chain <span class="math inline">\(X\left(t\right)\)</span>, then <span class="math inline">\(\boldsymbol{\nu}\)</span> is a stationary distribution.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We present a proof sketch of the above theorem. We need to show that <span class="math inline">\(\boldsymbol{\nu}\mathbf{P}=\boldsymbol{\nu}\)</span>, or equivalently that <span class="math inline">\(\left(\boldsymbol{\nu}\mathbf{P}\right)_{i}=\nu\left(i\right)\)</span>. Observe that</p>
<p><span class="math display">\[
\left(\boldsymbol{\nu}\mathbf{P}\right)_{i}=
\sum_{j=1}^{n}\nu\left(j\right)P_{ji}=
\sum_{j=1}^{n}\nu\left(i\right)P_{ij}=
\nu\left(i\right)\sum_{j=1}^{n}P_{ij},
\]</span></p>
where the second equality follows because <span class="math inline">\(\boldsymbol{\nu}\)</span> is in detailed balance. Now, <span class="math inline">\(\sum_{j=1}^{n}P_{ij}\)</span> is a sum over a row of a transition probability matrix, which is equal to 1, hence <span class="math inline">\(\left(\boldsymbol{\nu}\mathbf{P}\right)_{i}=\nu\left(i\right)\)</span>. <span class="math inline">\(i\)</span> was chosen arbitrarily, so it follows that <span class="math inline">\(\boldsymbol{\nu}\mathbf{P}=\boldsymbol{\nu}\)</span>, and the result has been shown.
</div>

<p>We now attempt to give an intuition for detailed balance. Consider a Markov chain with two states, and suppose that <span class="math inline">\(\boldsymbol{\nu}\)</span> is in detailed balance for the chain. Then, <span class="math inline">\(\nu\left(1\right)P_{12}=\nu\left(2\right)P_{21}\)</span>, i.e., the probability of being in state 1 and moving to state 2 is equal to the probability of being in state 2 and moving to state 1.</p>
</div>
<div id="metropolis-hastings" class="section level2">
<h2><span class="header-section-number">10.4</span> Metropolis-Hastings</h2>
<p>We began by considering the problem of sampling from an intractable distribution. We have established that the limiting distribution of an irreducible finite-state Markov chain <span class="math inline">\(X\left(t\right)\)</span> is the stationary distribution, and we have seen that if a distribution is in detailed balance for the <span class="math inline">\(X\left(t\right)\)</span>, then it is a stationary distribution. We now consider an algorithm for sampling from <span class="math inline">\(X\left(t\right)\)</span>. The idea behind the Metropolis-Hastings algorithm is that we do not have, or cannot write down (or store in memory) the transition probability matrix <span class="math inline">\(\mathbf{P}\)</span>. We will instead attempt to simulate <span class="math inline">\(X\left(t\right)\)</span> by the <em>Metropolis-Hastings algorithm</em>.</p>
<ol style="list-style-type: decimal">
<li>Suppose that at time <span class="math inline">\(t\)</span> the chain is in state <span class="math inline">\(s\)</span>, i.e., <span class="math inline">\(X\left(t\right)=s\)</span>.</li>
<li>Let <span class="math inline">\(q\left(s,s&#39;\right)\)</span> be the probability of proposing state <span class="math inline">\(s&#39;\)</span> given that the chain is in state <span class="math inline">\(s\)</span>. <span class="math inline">\(q\)</span> is called a <em>proposal function</em>.</li>
<li><p>Let <span class="math inline">\(\hat{U}\)</span> be a sample from a standard uniform random variable, and consider the quantity</p>
<p><span class="math display">\[
\min\left(1,\frac{\nu\left(s&#39;\right)q\left(s&#39;,s\right)}{\nu\left(s\right)q\left(s,s&#39;\right)}\right).
\]</span></p>
<p>If <span class="math inline">\(\hat{U}\)</span> is less than this quantity, we will accept the proposal and set <span class="math inline">\(X\left(t+1\right)=s&#39;\)</span>. Otherwise, we will reject the proposal and remain in state <span class="math inline">\(s\)</span>, i.e., <span class="math inline">\(X\left(t+1\right)=s\)</span>.</p></li>
</ol>

<div class="example">
<p><span id="exm:unnamed-chunk-94" class="example"><strong>Example 10.1  </strong></span>Consider again the Ising model, and suppose that <span class="math inline">\(n=3\)</span>, so that there are 9 possible states, hence <span class="math inline">\(2^{9}\)</span> possible configurations. Consider the proposal function that picks a vertex uniformly and flips its sign, taking the resulting configuration as the proposed state. Then, the probability of proposing <span class="math inline">\(s&#39;\)</span> given that the chain is in state <span class="math inline">\(s\)</span> is</p>
<p><span class="math display">\[
q\left(s,s&#39;\right)=
  \begin{cases}
    1/n^{2}, &amp; \text{if }s\text{ and }s&#39;\text{ differ at 1 vertex}\\
    0, &amp; \text{otherwise}
  \end{cases}.
\]</span></p>
<p>We now step through the algorithm. Having sampled <span class="math inline">\(s&#39;\)</span> from the proposal, we form the <em>Metropolis-Hastings ratio</em></p>
<p><span class="math display">\[
\min\left(1,\frac{\nu\left(s&#39;\right)q\left(s&#39;,s\right)}{\nu\left(s\right)q\left(s,s&#39;\right)}\right)=
\min\left(1,\frac{\mathrm{e}^{H\left(s&#39;\right)}\left(1/n^{2}\right)}{\mathrm{e}^{H\left(s\right)}\left(1/n^{2}\right)}\right)=
\min\left(1,\mathrm{e}^{H\left(s&#39;\right)-H\left(s\right)}\right).
\]</span></p>
If a sample drawn from <span class="math inline">\(\mathcal{U}\left(0,1\right)\)</span> is less than this quantity, we will accept the proposal and the chain will move to state <span class="math inline">\(s&#39;\)</span>, else we will reject it and remain in state <span class="math inline">\(s\)</span>. We see that if <span class="math inline">\(H\left(s&#39;\right)&gt;H\left(s\right)\)</span>, then <span class="math inline">\(\min\left(1,\mathrm{e}^{H\left(s&#39;\right)-H\left(s\right)}\right)=1\)</span>, i.e., we will accept the proposal. Intuitively, the algorithm moves to areas of higher probability, which reflects the stationary distribution. Observe also that if <span class="math inline">\(H\left(s&#39;\right)&lt;H\left(s\right)\)</span>, that we will sometimes accept the proposal and sometimes reject, depending on the sample from <span class="math inline">\(\mathcal{U}\left(0,1\right)\)</span>. This reflects the fact that for the chain to fully explore the state space, it must sometimes move to a state of lower probability. This completes one step of the algorithm.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-95" class="example"><strong>Example 10.2  </strong></span>Let <span class="math inline">\(X\sim\mathcal{N}\left(0,1\right)\)</span> be the distribution from which we wish to sample. In this case, <span class="math inline">\(S=\mathbb{R}\)</span>, so that any state <span class="math inline">\(s\in\mathbb{R}\)</span>. Let the probability of being in state <span class="math inline">\(s\)</span> be given by the Gaussian density, i.e.,</p>
<p><span class="math display">\[
\nu\left(s\right)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-s^{2}/2}.
\]</span></p>
<p>Suppose that we propose a state by adding a sample from <span class="math inline">\(\mathcal{U}\left(0,1\right)\)</span> to <span class="math inline">\(s\)</span> and then subtracting <span class="math inline">\(1/2\)</span>, i.e., <span class="math inline">\(s&#39;=s+\left(U-1/2\right)\)</span>, where <span class="math inline">\(U\)</span> is a sample from the standard uniform distribution. Observe that <span class="math inline">\(s&#39;\)</span> has the uniform distribution over <span class="math inline">\(\left[s-1/2,s+1/2\right]\)</span>, i.e., given that we are in state <span class="math inline">\(s\)</span>, the probability that we will propose <span class="math inline">\(s&#39;\)</span> is uniform over an interval of length <span class="math inline">\(1\)</span> centered at <span class="math inline">\(s\)</span>. Thus, <span class="math inline">\(s&#39;\)</span> has the uniform density <span class="math inline">\(f_{U}\left(s\right)=1/\left(b-a\right)=1/1=1\)</span>. Observe also that the proposal is symmetric, i.e., <span class="math inline">\(q\left(s,s&#39;\right)=q\left(s&#39;,s\right)\)</span>. Accordingly, the Metropolis-Hastings ratio is</p>
<p><span class="math display">\[
\min\left(1,\frac{\nu\left(s&#39;\right)q\left(s&#39;,s\right)}{\nu\left(s\right)q\left(s,s&#39;\right)}\right)=
\min\left(1,\frac{\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\left(s&#39;\right)^{2}/2}\cdot 1}{\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-s^{2}/2}\cdot 1}\right)=
\min\left(1,\mathrm{e}^{\left(s^{2}-\left(s&#39;\right)^{2}\right)/2}\right).
\]</span></p>
Observe that Metropolis-Hastings is a general sampling algorithm, though for sampling from a <span class="math inline">\(\mathcal{N}\left(0,1\right)\)</span>, the algorithm is slow and would not be used in practice. We will now sample from <span class="math inline">\(X\)</span> using the algorithm.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nu &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)

propose &lt;-<span class="st"> </span><span class="cf">function</span>(x) x <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span>

q &lt;-<span class="st"> </span><span class="cf">function</span>(x, x_prime) <span class="dv">1</span>

mh_ratio &lt;-<span class="st"> </span><span class="cf">function</span>(x, x_prime, nu, q) {
  <span class="kw">min</span>(<span class="dv">1</span>, <span class="kw">nu</span>(x_prime) <span class="op">*</span><span class="st"> </span><span class="kw">q</span>(x_prime, x) <span class="op">/</span><span class="st"> </span>(<span class="kw">nu</span>(x) <span class="op">*</span><span class="st"> </span><span class="kw">q</span>(x, x_prime)))
}

mh_sampler &lt;-<span class="st"> </span><span class="cf">function</span>(x0, proposal, nu, q, iter) {
  x &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;numeric&quot;</span>, <span class="dt">length =</span> iter <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)
  x[<span class="dv">1</span>] &lt;-<span class="st"> </span>x0
  
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq</span>(iter)) {
    x_prime &lt;-<span class="st"> </span><span class="kw">proposal</span>(x[i])
    ratio &lt;-<span class="st"> </span><span class="kw">mh_ratio</span>(x[i], x_prime, nu, q)
    <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span>ratio) {
      x[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>x_prime
    } <span class="cf">else</span> {
      x[i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>x[i]
    }
  }
  x
}</code></pre></div>
<p>We will run our sampler for <span class="math inline">\(T=10^{4}\)</span> iterations, starting with <span class="math inline">\(X\left(0\right)=10\)</span> (we deliberately choose a starting value far from the stationary distribution to show how the algorithm moves to areas of higher probability).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">mh_sampler</span>(<span class="dt">x0 =</span> <span class="dv">10</span>, <span class="dt">proposal =</span> propose, <span class="dt">nu =</span> nu, <span class="dt">q =</span> q, <span class="dt">iter =</span> <span class="fl">1e4</span>)</code></pre></div>
<p>We now examine the first 500 samples.</p>
<p><img src="course-notes_files/figure-html/mh-gaussian-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that the distribution is far from the standard Gaussian. This is unsurprising given our choice of initial state. As the chain runs, we should begin to see convergence to the stationary distribution.</p>
<p><img src="course-notes_files/figure-html/mh-gaussian-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that samples 501 to 1000 are much closer to the standard Gaussian. We now examine the remaining samples.</p>
<p><img src="course-notes_files/figure-html/mh-gaussian-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The chain appears to have converged to the stationary distribution. For most applications, we will not know the density of the target distribution (if we did, we would not be using Metropolis-Hastings). Instead, we would perform <em>convergence diagnostics</em> and discard samples to account for starting “far” from the stationary distribution (burn-in) and autocorrelation (thinning).</p>
<p>We now give a proof sketch for the Metropolis-Hastings algorithm. Recall that we cannot write down or store the transition probability matrix of the chain <span class="math inline">\(X\left(t\right)\)</span>, but if we can find a distribution <span class="math inline">\(\boldsymbol{\nu}\)</span> that is in detailed balance for <span class="math inline">\(X\left(t\right)\)</span>, then <span class="math inline">\(\boldsymbol{\nu}\)</span> will be a stationary distribution. Detailed balance is defined in terms of pairs of states, so the idea behind the algorithm is to make sure that every pair of states is in detailed balance. If we can do this, then <span class="math inline">\(\boldsymbol{\nu}\)</span> will be the stationary distribution, and <span class="math inline">\(X\left(t\right)\)</span> will converge to <span class="math inline">\(\boldsymbol{\nu}\)</span>. For the proof sketch, we will consider two states, <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, where the probability of being in state <span class="math inline">\(i\)</span> is <span class="math inline">\(\nu\left(i\right)\)</span>, and where the probability of proposing state <span class="math inline">\(i\)</span> given that <span class="math inline">\(X\left(t\right)\)</span> is in state <span class="math inline">\(j\)</span> is <span class="math inline">\(q\left(i,j\right)\)</span>.</p>
<p><img src="course-notes_files/figure-html/mh-proof-1.png" width="384" style="display: block; margin: auto;" /></p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We will choose transition probabilities to make <span class="math inline">\(\boldsymbol{\nu}\)</span> the stationary distribution. For the moment, we will always accept the proposal, i.e., we will leave aside the third step of the algorithm.</p>
<p>Then, from the diagram above, we see that the probability of moving from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> is the probability of being in state <span class="math inline">\(i\)</span> multiplied by the probability of proposing <span class="math inline">\(j\)</span>, i.e., <span class="math inline">\(\nu\left(i\right)q\left(i,j\right)\)</span>, and similarly for moving from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span>. But there is no reason that <span class="math inline">\(\boldsymbol{\nu}\)</span> should be in detailed balance for some proposal function <span class="math inline">\(q\)</span>, i.e., in general we will not have <span class="math inline">\(\nu\left(i\right)q\left(i,j\right)=\nu\left(j\right)q\left(j,i\right)\)</span>. We can correct the situation by introducing <span class="math inline">\(\alpha,\beta\in\mathbb{R}\)</span> such that we will accept the proposals to move to states <span class="math inline">\(j\)</span> and <span class="math inline">\(i\)</span> with probabilities <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, respectively. I.e., the probability of accepting the proposal <span class="math inline">\(j\)</span> is <span class="math inline">\(\alpha\cdot q\left(i,j\right)\)</span>, and similarly the probability of accepting the proposal <span class="math inline">\(i\)</span> is <span class="math inline">\(\beta\cdot q\left(j,i\right)\)</span>.</p>
<p>Observe then that the probability of moving from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> is now the probability of being in <span class="math inline">\(i\)</span> multiplied by the probability of proposing <span class="math inline">\(j\)</span>, multiplied by the probability of accepting the proposal, i.e., <span class="math inline">\(\nu\left(i\right)q\left(i,j\right)\alpha\)</span>. (Observe that the <span class="math inline">\(\left(i,j\right)\text{th}\)</span> entry of the transition probability matrix is <span class="math inline">\(P_{ij}=q\left(i,j\right)\alpha\)</span>. Thus, we will specify transition probabilities without ever writing down the matrix. Observe also that <span class="math inline">\(\alpha=P\left(\mathcal{U}&lt;\alpha\right)\)</span>.) Thus, we must choose <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to satisfy detailed balance, i.e., so that</p>
<p><span class="math display">\[
\nu\left(i\right)q\left(i,j\right)\alpha=\nu\left(j\right)q\left(j,i\right)\beta
\implies\frac{\alpha}{\beta}=\frac{\nu\left(j\right)q\left(j,i\right)}{\nu\left(i\right)q\left(i,j\right)}.
\]</span></p>
<p>If we choose <span class="math inline">\(\beta=1\)</span>, then</p>
<p><span class="math display">\[
\alpha=\frac{\nu\left(j\right)q\left(j,i\right)}{\nu\left(i\right)q\left(i,j\right)},
\]</span></p>
<p>but observe that if the ratio on the right is greater than <span class="math inline">\(1\)</span>, <span class="math inline">\(\alpha\)</span> will not be a valid probability. Thus, if the Metropolis-Hastings ratio is less than or equal to <span class="math inline">\(1\)</span>, we will set</p>
<p><span class="math display">\[
\beta=1\quad\text{and}\quad\alpha=\frac{\nu\left(j\right)q\left(j,i\right)}{\nu\left(i\right)q\left(i,j\right)}.
\]</span></p>
<p>Similarly, if the ratio is greater than <span class="math inline">\(1\)</span>, we will set</p>
<p><span class="math display">\[
\alpha=1\quad\text{and}\quad\beta=\frac{\nu\left(i\right)q\left(i,j\right)}{\nu\left(j\right)q\left(j,i\right)},
\]</span></p>
<p>and observe that in this case <span class="math inline">\(\beta\)</span> is the reciprocal of the ratio. Therefore,</p>
<p><span class="math display">\[
\alpha=\min\left(1,\frac{\nu\left(j\right)q\left(j,i\right)}{\nu\left(i\right)q\left(i,j\right)}\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\beta=\min\left(1,\frac{\nu\left(i\right)q\left(i,j\right)}{\nu\left(j\right)q\left(j,i\right)}\right).
\]</span></p>
<p>Observe that if the MH ratio is greater than 1, we will have</p>
<p><span class="math display">\[
\alpha=1\quad\text{and}\quad\beta=\frac{\nu\left(i\right)q\left(i,j\right)}{\nu\left(j\right)q\left(j,i\right)},
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\nu\left(i\right)q\left(i,j\right)\alpha=
\nu\left(i\right)q\left(i,j\right)\cdot 1=
\nu\left(i\right)q\left(i,j\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\nu\left(j\right)q\left(j,i\right)\beta=
\nu\left(j\right)q\left(j,i\right)\frac{\nu\left(i\right)q\left(i,j\right)}{\nu\left(j\right)q\left(j,i\right)}=
\nu\left(i\right)q\left(i,j\right),
\]</span></p>
<p>so that detailed balance is satisfied. If the MH ratio is less than or equal to 1, we will have</p>
<p><span class="math display">\[
\alpha=\frac{\nu\left(j\right)q\left(j,i\right)}{\nu\left(i\right)q\left(i,j\right)}\quad\text{and}\quad\beta=1,
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\nu\left(i\right)q\left(i,j\right)\alpha=
\nu\left(i\right)q\left(i,j\right)\frac{\nu\left(j\right)q\left(j,i\right)}{\nu\left(i\right)q\left(i,j\right)}=
\nu\left(j\right)q\left(j,i\right),
\]</span></p>
which we see is equal to <span class="math inline">\(\nu\left(j\right)q\left(j,i\right)\beta\)</span> when <span class="math inline">\(\beta=1\)</span>, so that detailed balance is again satisfied. Thus, we have shown how to construct <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, hence <span class="math inline">\(P_{ij}\)</span> for an arbitrary proposal <span class="math inline">\(q\)</span>, such that detailed balance is satisfied for every pair of states. Theorem <a href="markov-chain-monte-carlo.html#thm:detailed-balance">10.3</a> implies that <span class="math inline">\(\boldsymbol{\nu}\)</span> is the stationary distribution of <span class="math inline">\(X\left(t\right)\)</span>, and Theorem <a href="markov-chain-monte-carlo.html#thm:mc-convergence">10.2</a> implies that <span class="math inline">\(X\left(t\right)\)</span> will converge to <span class="math inline">\(\boldsymbol{\nu}\)</span>.
</div>

<p>Recall that our goal is to sample from <span class="math inline">\(\boldsymbol{\nu}\)</span>. The Metropolis-Hastings algorithm gives us a way to sample from this distribution by constructing a Markov chain that has <span class="math inline">\(\boldsymbol{\nu}\)</span> as its stationary distribution. Now, Theorem <a href="markov-chain-monte-carlo.html#thm:mc-convergence">10.2</a> guarantees convergence only in the case that <span class="math inline">\(t\rightarrow\infty\)</span>. In practice, we will run the chain until some large time <span class="math inline">\(T\)</span>, e.g., <span class="math inline">\(T=10^{6}\)</span>, and our sample of <span class="math inline">\(\boldsymbol{\nu}\)</span> is the state of the chain at time <span class="math inline">\(T\)</span>, i.e., <span class="math inline">\(X\left(T\right)\)</span>.</p>
</div>
<div id="gibbs-sampling" class="section level2">
<h2><span class="header-section-number">10.5</span> Gibbs Sampling</h2>
<p>We now present Gibbs sampling, a special case of the Metropolis-Hastings algorithm that does not require us to specify a proposal. Suppose that we wish to sample from <span class="math inline">\(\mathbf{X}=\left(X_{1},\ldots,X_{n}\right)\)</span>. The idea behind Gibbs sampling is to set a Markov chain <span class="math inline">\(\mathbf{W}\left(0\right)=\left(X_{1}^{\left(0\right)},\ldots,X_{n}^{\left(0\right)}\right)\)</span>, where the <span class="math inline">\(X_{i}^{\left(0\right)}\)</span> are often set at random (from possible values of each <span class="math inline">\(X_{i}\)</span>). Then, the marginal density of the first coordinate is <span class="math inline">\(f\left(X_{1}|X_{2}^{\left(0\right)},\ldots,X_{n}^{\left(0\right)}\right)\)</span>. Let <span class="math inline">\(\hat{X}_{1}^{\left(1\right)}\)</span> be a sample from the marginal density of <span class="math inline">\(X_{1}\)</span>. Setting <span class="math inline">\(X_{1}^{\left(1\right)}=\hat{X}_{1}^{\left(1\right)}\)</span>, the Markov chain at <span class="math inline">\(t=1\)</span> is <span class="math inline">\(\mathbf{W}\left(1\right)=\left(X_{1}^{\left(1\right)},X_{2}^{\left(0\right)}\ldots,X_{n}^{\left(0\right)}\right)\)</span>. For the second coordinate, let <span class="math inline">\(f\left(X_{2}|X_{1}^{\left(1\right)},X_{3}^{\left(0\right)},\ldots,X_{n}^{\left(0\right)}\right)\)</span> be the marginal density of <span class="math inline">\(X_{2}\)</span>. We then draw a sample <span class="math inline">\(\hat{X}_{2}^{\left(1\right)}\)</span> from the marginal density, and our Markov chain becomes <span class="math inline">\(\mathbf{W}\left(1\right)=\left(X_{1}^{\left(1\right)},X_{2}^{\left(1\right)},X_{3}^{\left(0\right)},\ldots,X_{n}^{\left(0\right)}\right)\)</span>. We proceed in this manner until we have drawn samples from the marginal density of each <span class="math inline">\(X_{i}\)</span>, which is given by <span class="math inline">\(f\left(X_{i}|\mathbf{X}_{-i}\right)\)</span>, where <span class="math inline">\(\mathbf{X}_{-i}\)</span> is the vector whose entries are the samples drawn from the marginal densities of <span class="math inline">\(\left\{X_{j}\right\}_{j=1}^{i-1}\)</span> and <span class="math inline">\(\left\{X_{j}^{\left(0\right)}\right\}_{j=i+1}^{n}\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-99" class="theorem"><strong>Theorem 10.4  </strong></span>The limiting distribution of <span class="math inline">\(\mathbf{W}\left(t\right)\)</span> is <span class="math inline">\(\mathbf{X}\)</span>, i.e., <span class="math display">\[
  \lim_{t\rightarrow\infty}\mathbf{W}\left(t\right)\sim\mathbf{X}.
\]</span>
</div>

<p>We now present a proof sketch for Gibbs sampling.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We wish to sample from a distribution <span class="math inline">\(\nu\left(X_{1},\ldots,X_{n}\right)\)</span>. Applying conditional probability, we have</p>
<p><span class="math display">\[
  \nu\left(X_{1},\ldots,X_{n}\right)=f\left(X_{i}|\mathbf{X}_{-i}\right)p\left(\mathbf{X}_{-i}\right).
\]</span></p>
<p>Let <span class="math inline">\(\hat{X}_{i}\)</span> be the “Gibbs sampling” sample of coordinate <span class="math inline">\(i\)</span>. We can regard <span class="math inline">\(\hat{X}_{i}\)</span> as a proposal in the Metropolis-Hastings sense. We begin by forming the Metropolis-Hastings ratio</p>
<p><span class="math display">\[
  \min\left(1,\frac{\nu\left(\mathbf{X}&#39;\right)q\left(\mathbf{X}&#39;,\mathbf{X}\right)}{\nu\left(\mathbf{X}\right)q\left(\mathbf{X},\mathbf{X}&#39;\right)}\right).
\]</span></p>
<p>Noting that <span class="math inline">\(\nu\left(\mathbf{X}&#39;\right)\)</span> is just <span class="math inline">\(\nu\left(\mathbf{X}\right)\)</span> with <span class="math inline">\(X_{i}\)</span> replaced by <span class="math inline">\(\hat{X}_{i}\)</span>, the ratio becomes</p>
<p><span class="math display">\[
\min\left(1,\frac{f\left(\hat{X}_{i}|\mathbf{X}_{-i}\right)p\left(\mathbf{X}_{-i}\right)q\left(x&#39;,x\right)}{f\left(X_{i}|\mathbf{X}_{-i}\right)p\left(\mathbf{X}_{-i}\right)q\left(x,x&#39;\right)}\right).
\]</span></p>
<p>Recall that <span class="math inline">\(q\left(\mathbf{X},\mathbf{X}&#39;\right)\)</span> is the probability of proposing <span class="math inline">\(\mathbf{X&#39;}\)</span> given that the chain is in state <span class="math inline">\(\mathbf{X}\)</span>. Writing <span class="math inline">\(q\)</span> as</p>
<p><span class="math display">\[
q\left(\mathbf{X},\mathbf{X}&#39;\right)=q\left(\left(X_{i},\mathbf{X}_{-i}\right),\left(\hat{X}_{i},\mathbf{X}_{-i}\right)\right)
\]</span></p>
<p>we see that the probability of proposing <span class="math inline">\(\mathbf{X}&#39;\)</span> is given by the marginal density of <span class="math inline">\(\mathbf{X}&#39;\)</span>, <span class="math inline">\(f\left(\hat{X}_{i}|\mathbf{X}_{-i}\right)\)</span>, which does not depend on the current state <span class="math inline">\(\mathbf{X}\)</span>. Similarly, <span class="math inline">\(q\left(\mathbf{X}&#39;,\mathbf{X}\right)=f\left(X_{i}|\mathbf{X}_{-i}\right)\)</span>, so that the ratio becomes</p>
<p><span class="math display">\[
\min\left(1,\frac{f\left(\hat{X}_{i}|\mathbf{X}_{-i}\right)p\left(\mathbf{X}_{-i}\right)f\left(X_{i}|\mathbf{X}_{-i}\right)}{f\left(X_{i}|\mathbf{X}_{-i}\right)p\left(\mathbf{X}_{-i}\right)f\left(\hat{X}_{i}|\mathbf{X}_{-i}\right)}\right)=\min\left(1,1\right)=1,
\]</span></p>
i.e., in Gibbs sampling, we always accept the proposal. Thus, Gibbs sampling can be viewed as a special case of Metropolis-Hastings where the proposal is always accepted.
</div>

<p>We can use Gibbs sampling to sample from <span class="math inline">\(\mathbf{X}\)</span> without specifying a proposal distribution, though we must know how to sample from the marginal densities of the <span class="math inline">\(X_{i}\)</span>. Gibbs sampling tends to work well when the random variable we wish to sample breaks up into coordinates. Note also that Gibbs sampling is subject to the same convergence considerations as the general Metropolis-Hastings algorithm.</p>
<div id="latent-dirichlet-allocation" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Latent Dirichlet Allocation</h3>
<p>We return now to LDA. Recall that we wish to estimate an intractable posterior distribution. We can write the conditional distribution of each topic <span class="math inline">\(z_{i}\)</span> as</p>
<p><span class="math display">\[
P\left(z_{i}=j|\mathbf{z}_{-i},\mathbf{w}\right)\propto\frac{n_{-i,j}^{\left(w_{i}\right)}+\beta}{n_{-i,j}^{\left(\cdot\right)}+W\beta}\frac{n_{-i,j}^{\left(d_{i}\right)}+\alpha}{n_{-i}^{\left(d_{i}\right)}+T\alpha},
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{z}_{-i}\)</span> consists of all topics except <span class="math inline">\(z_{i}\)</span></li>
<li><span class="math inline">\(n_{-i,j}^{\left(w_{i}\right)}\)</span> is the number of times word <span class="math inline">\(w_{i}\)</span> has been assigned to topic <span class="math inline">\(j\)</span> (excluding <span class="math inline">\(z_{i}\)</span>)</li>
<li><span class="math inline">\(n_{-i,j}^{\left(d_{i}\right)}\)</span> is the number of times a word from document <span class="math inline">\(d_{i}\)</span> has been assigned to topic <span class="math inline">\(j\)</span> (again excluding <span class="math inline">\(z_{i}\)</span>)</li>
<li><span class="math inline">\(n_{-i,j}^{\left(\cdot\right)}\)</span> is the number of times all words have been assigned to topic <span class="math inline">\(j\)</span> (excluding <span class="math inline">\(z_{i}\)</span>)</li>
<li><span class="math inline">\(n_{-i}^{\left(d_{i}\right)}\)</span> is the number of times a word from document <span class="math inline">\(d_{i}\)</span> has been assigned to all topics (excluding <span class="math inline">\(z_{i}\)</span>)</li>
<li><span class="math inline">\(W\)</span> is the number of words in the dictionary</li>
<li><span class="math inline">\(T\)</span> is the number of topics</li>
<li>and <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> parameterize the prior distributions over topics and words, respectively.</li>
</ul>
<p>We can then use Gibbs sampling to sample each <span class="math inline">\(z_{i}\)</span> in turn, yielding samples of the posterior distribution of assignments of word to topics <span class="math inline">\(\mathbf{z}\)</span>.</p>
<p>Gibbs sampling appears frequently in Bayesian statistics because interest often lies in the marginal distributions of various parameters.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Griffiths2004">
<p>Griffiths, Thomas L, and Mark Steyvers. 2004. “Finding scientific topics.” <em>Proceedings of the National Academy of Sciences</em> 101 (Supplement 1). National Academy of Sciences: 5228–35. doi:<a href="https://doi.org/10.1073/pnas.0307752101">10.1073/pnas.0307752101</a>.</p>
</div>
<div id="ref-Blei2003">
<p>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3: 993–1022. doi:<a href="https://doi.org/10.1162/jmlr.2003.3.4-5.993">10.1162/jmlr.2003.3.4-5.993</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="em-algorithm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pagerank.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["course-notes.pdf", "course-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
