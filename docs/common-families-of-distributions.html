<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Common families of distributions | A Master’s Companion</title>
  <meta name="description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Common families of distributions | A Master’s Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Common families of distributions | A Master’s Companion" />
  
  <meta name="twitter:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

<meta name="author" content="Sean Wilson" />


<meta name="date" content="2019-05-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-algebra.html">
<link rel="next" href="point-estimation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\newcommand{\coloneqq}{\mathrel{\mathop:}\mathrel{\mkern-1.2mu}=}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Master's Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="part"><span><b>I Background material</b></span></li>
<li class="chapter" data-level="2" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>2</b> Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="analysis.html"><a href="analysis.html#upper-bounds-and-suprema"><i class="fa fa-check"></i><b>2.1</b> Upper bounds and suprema</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>3</b> Probability theory</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-theory.html"><a href="probability-theory.html#background-material"><i class="fa fa-check"></i><b>3.1</b> Background material</a></li>
<li class="chapter" data-level="3.2" data-path="probability-theory.html"><a href="probability-theory.html#transformations-and-expectations"><i class="fa fa-check"></i><b>3.2</b> Transformations and expectations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-theory.html"><a href="probability-theory.html#distributions-of-functions-of-a-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Distributions of functions of a random variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-theory.html"><a href="probability-theory.html#expected-values"><i class="fa fa-check"></i><b>3.2.2</b> Expected values</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-theory.html"><a href="probability-theory.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>3.2.3</b> Moments and moment generating functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-theory.html"><a href="probability-theory.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.3</b> Multiple random variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="probability-theory.html"><a href="probability-theory.html#conditional-distributions-and-independence"><i class="fa fa-check"></i><b>3.3.1</b> Conditional distributions and independence</a></li>
<li class="chapter" data-level="3.3.2" data-path="probability-theory.html"><a href="probability-theory.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.3.2</b> Covariance and correlation</a></li>
<li class="chapter" data-level="3.3.3" data-path="probability-theory.html"><a href="probability-theory.html#multivariate-distributions"><i class="fa fa-check"></i><b>3.3.3</b> Multivariate distributions</a></li>
<li class="chapter" data-level="3.3.4" data-path="probability-theory.html"><a href="probability-theory.html#inequalities"><i class="fa fa-check"></i><b>3.3.4</b> Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-theory.html"><a href="probability-theory.html#properties-of-a-random-sample"><i class="fa fa-check"></i><b>3.4</b> Properties of a random sample</a><ul>
<li class="chapter" data-level="3.4.1" data-path="probability-theory.html"><a href="probability-theory.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i><b>3.4.1</b> Sums of random variables from a random sample</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability-theory.html"><a href="probability-theory.html#sampling-from-the-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Sampling from the normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="probability-theory.html"><a href="probability-theory.html#order-statistics"><i class="fa fa-check"></i><b>3.4.3</b> Order statistics</a></li>
<li class="chapter" data-level="3.4.4" data-path="probability-theory.html"><a href="probability-theory.html#convergence-concepts"><i class="fa fa-check"></i><b>3.4.4</b> Convergence concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Linear algebra</a></li>
<li class="part"><span><b>II Mathematical statistics</b></span></li>
<li class="chapter" data-level="5" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html"><i class="fa fa-check"></i><b>5</b> Common families of distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#defn-exp-family"><i class="fa fa-check"></i><b>5.1</b> Exponential families</a><ul>
<li class="chapter" data-level="5.1.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#natural-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Natural parameters</a></li>
<li class="chapter" data-level="5.1.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#conjugate-prior-distributions"><i class="fa fa-check"></i><b>5.1.2</b> Conjugate prior distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-and-scale-families"><i class="fa fa-check"></i><b>5.2</b> Location and scale families</a><ul>
<li class="chapter" data-level="5.2.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-families"><i class="fa fa-check"></i><b>5.2.1</b> Location families</a></li>
<li class="chapter" data-level="5.2.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#scale-families"><i class="fa fa-check"></i><b>5.2.2</b> Scale families</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>6</b> Point estimation</a><ul>
<li class="chapter" data-level="6.1" data-path="point-estimation.html"><a href="point-estimation.html#methods-of-finding-estimators"><i class="fa fa-check"></i><b>6.1</b> Methods of finding estimators</a><ul>
<li class="chapter" data-level="6.1.1" data-path="point-estimation.html"><a href="point-estimation.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>6.1.1</b> Maximum likelihood estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>7</b> Generalized linear models</a><ul>
<li class="chapter" data-level="7.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interaction-terms"><i class="fa fa-check"></i><b>7.1</b> Interaction terms</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="machine-representation.html"><a href="machine-representation.html"><i class="fa fa-check"></i><b>8</b> Machine representation</a><ul>
<li class="chapter" data-level="8.1" data-path="machine-representation.html"><a href="machine-representation.html#binary-numbers"><i class="fa fa-check"></i><b>8.1</b> Binary numbers</a></li>
<li class="chapter" data-level="8.2" data-path="machine-representation.html"><a href="machine-representation.html#integers"><i class="fa fa-check"></i><b>8.2</b> Integers</a></li>
<li class="chapter" data-level="8.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-numbers"><i class="fa fa-check"></i><b>8.3</b> Floating-point numbers</a><ul>
<li class="chapter" data-level="8.3.1" data-path="machine-representation.html"><a href="machine-representation.html#special-exponent-values"><i class="fa fa-check"></i><b>8.3.1</b> Special exponent values</a></li>
<li class="chapter" data-level="8.3.2" data-path="machine-representation.html"><a href="machine-representation.html#limitations"><i class="fa fa-check"></i><b>8.3.2</b> Limitations</a></li>
<li class="chapter" data-level="8.3.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-error"><i class="fa fa-check"></i><b>8.3.3</b> Floating-point error</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="em-algorithm.html"><a href="em-algorithm.html"><i class="fa fa-check"></i><b>9</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.1" data-path="em-algorithm.html"><a href="em-algorithm.html#motivation"><i class="fa fa-check"></i><b>9.1</b> Motivation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="em-algorithm.html"><a href="em-algorithm.html#k-means"><i class="fa fa-check"></i><b>9.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="em-algorithm.html"><a href="em-algorithm.html#em-algorithm-1"><i class="fa fa-check"></i><b>9.2</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.2.1" data-path="em-algorithm.html"><a href="em-algorithm.html#algorithmic-perspective"><i class="fa fa-check"></i><b>9.2.1</b> Algorithmic perspective</a></li>
<li class="chapter" data-level="9.2.2" data-path="em-algorithm.html"><a href="em-algorithm.html#statistical-perspective"><i class="fa fa-check"></i><b>9.2.2</b> Statistical perspective</a></li>
<li class="chapter" data-level="9.2.3" data-path="em-algorithm.html"><a href="em-algorithm.html#proof-sketch"><i class="fa fa-check"></i><b>9.2.3</b> Proof sketch</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="em-algorithm.html"><a href="em-algorithm.html#example-gaussian-mixture"><i class="fa fa-check"></i><b>9.3</b> Example: Gaussian mixture</a></li>
<li class="chapter" data-level="9.4" data-path="em-algorithm.html"><a href="em-algorithm.html#applications"><i class="fa fa-check"></i><b>9.4</b> Applications</a><ul>
<li class="chapter" data-level="9.4.1" data-path="em-algorithm.html"><a href="em-algorithm.html#factor-analysis"><i class="fa fa-check"></i><b>9.4.1</b> Factor analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>10</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="10.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#motivation-1"><i class="fa fa-check"></i><b>10.1</b> Motivation</a><ul>
<li class="chapter" data-level="10.1.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#ising-model"><i class="fa fa-check"></i><b>10.1.1</b> Ising model</a></li>
<li class="chapter" data-level="10.1.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#intractable-posterior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Intractable posterior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-is-a-sampling-technique"><i class="fa fa-check"></i><b>10.1.3</b> MCMC is a sampling technique</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>10.2</b> Markov chain</a></li>
<li class="chapter" data-level="10.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#detailed-balance"><i class="fa fa-check"></i><b>10.3</b> Detailed balance</a></li>
<li class="chapter" data-level="10.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolis-hastings"><i class="fa fa-check"></i><b>10.4</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="10.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.5</b> Gibbs Sampling</a><ul>
<li class="chapter" data-level="10.5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#latent-dirichlet-allocation"><i class="fa fa-check"></i><b>10.5.1</b> Latent Dirichlet Allocation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pagerank.html"><a href="pagerank.html"><i class="fa fa-check"></i><b>11</b> PageRank</a><ul>
<li class="chapter" data-level="11.1" data-path="pagerank.html"><a href="pagerank.html#motivation-2"><i class="fa fa-check"></i><b>11.1</b> Motivation</a></li>
<li class="chapter" data-level="11.2" data-path="pagerank.html"><a href="pagerank.html#computing-eigenpairs"><i class="fa fa-check"></i><b>11.2</b> Computing eigenpairs</a></li>
<li class="chapter" data-level="11.3" data-path="pagerank.html"><a href="pagerank.html#algorithm"><i class="fa fa-check"></i><b>11.3</b> Algorithm</a></li>
<li class="chapter" data-level="11.4" data-path="pagerank.html"><a href="pagerank.html#considerations"><i class="fa fa-check"></i><b>11.4</b> Considerations</a><ul>
<li class="chapter" data-level="11.4.1" data-path="pagerank.html"><a href="pagerank.html#connection-to-markov-chains"><i class="fa fa-check"></i><b>11.4.1</b> Connection to Markov chains</a></li>
<li class="chapter" data-level="11.4.2" data-path="pagerank.html"><a href="pagerank.html#calculating-the-dominant-eigenvalue"><i class="fa fa-check"></i><b>11.4.2</b> Calculating the dominant eigenvalue</a></li>
<li class="chapter" data-level="11.4.3" data-path="pagerank.html"><a href="pagerank.html#computational-complexity"><i class="fa fa-check"></i><b>11.4.3</b> Computational complexity</a></li>
<li class="chapter" data-level="11.4.4" data-path="pagerank.html"><a href="pagerank.html#convergence-1"><i class="fa fa-check"></i><b>11.4.4</b> Convergence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Master’s Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="common-families-of-distributions" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Common families of distributions</h1>
<div id="defn-exp-family" class="section level2">
<h2><span class="header-section-number">5.1</span> Exponential families</h2>
<p>A family of pdfs (or pmfs) indexed by a parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> is called a <span class="math inline">\(k\)</span>-parameter exponential family if it can be expressed as</p>
<p><span class="math display">\[
f\left(x|\boldsymbol{\theta}\right)=h\left(x\right)c\left(\boldsymbol{\theta}\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\boldsymbol{\theta}\right)t_{j}\left(x\right)\right\}
\]</span></p>
<p>where <span class="math inline">\(h\left(x\right)\geq 0\)</span>, <span class="math inline">\(c\left(\boldsymbol{\theta}\right)\geq 0\)</span>, and <span class="math inline">\(\left\{t_{j}\left(x\right)\right\}_{j=1}^{k}\)</span> are real-valued functions of <span class="math inline">\(x\)</span>, and where <span class="math inline">\(\left\{\omega_{j}\left(\boldsymbol{\theta}\right)\right\}_{j=1}^{k}\)</span> are real-valued functions of the possibly vector-valued parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>. I.e., <span class="math inline">\(f\left(x|\boldsymbol{\theta}\right)\)</span> can be expressed in three parts:</p>
<ul>
<li>a part that depends only on the random variable(s)</li>
<li>a part that depends only on the parameter(s)</li>
<li>a part that depends on both the random variable(s) and the parameter(s).</li>
</ul>
<p>Most of the parametric models you may have studied are exponential families, e.g., normal, gamma, beta, binomial, negative binomial, Poisson, and multinomial. The uniform distribution is not an exponential family (see Example <a href="common-families-of-distributions.html#exm:exp-family-uniform">5.4</a> below).</p>

<div class="example">
<p><span id="exm:exp-family-binomial" class="example"><strong>Example 5.1  (Binomial random variables)  </strong></span>Let <span class="math inline">\(X\sim\mathcal{B}\left(n,p\right)\)</span>, where <span class="math inline">\(n\)</span> is known and <span class="math inline">\(p\in\left(0,1\right)\)</span>. Recall that <span class="math inline">\(X\)</span> represents the number of successes in <span class="math inline">\(n\)</span> i.i.d. Bernoulli trials and its pmf is given by</p>
<p><span class="math display">\[
f\left(x|p\right)   =\binom{n}{x}p^{x}\left(1-p\right)^{n-x}
\]</span></p>
<p>for <span class="math inline">\(x=0,1,\ldots,n\)</span> and <span class="math inline">\(f\left(x|p\right)=0\)</span> otherwise. We can write <span class="math inline">\(f\)</span> as</p>
<span class="math display">\[\begin{align*}
f\left(x|p\right)   &amp; = \binom{n}{x}p^{x}\left(1-p\right)^{n-x} \\
    &amp; = \binom{n}{x}p^{x}\left(1-p\right)^{n}\left(1-p\right)^{-x} \\
    &amp; = \binom{n}{x}\left(1-p\right)^{n}\left(\frac{p^{x}}{\left(1-p\right)^{x}}\right) \\
    &amp; = \binom{n}{x}\left(1-p\right)^{n}\left(\frac{p}{1-p}\right)^{x} \\
    &amp; = \binom{n}{x}\left(1-p\right)^{n}\exp\left\{ \log\left(\frac{p}{1-p}\right)^{x}\right\} \\
  &amp; = \binom{n}{x}\left(1-p\right)^{n}\exp\left\{ x\log\frac{p}{1-p}\right\}.
\end{align*}\]</span>
<p>Now, <span class="math inline">\(n\)</span> is known, so the only parameter is <span class="math inline">\(p\)</span>. Thus, we see that the binomial distribution is an exponential family, with</p>
<p><span class="math display">\[
h\left(x\right)=\binom{n}{x},\quad c\left(p\right)=\left(1-p\right)^{n},\quad t\left(x\right)=x,\quad\ \omega\left(p\right)=\log\frac{p}{1-p}.
\]</span></p>
Observe that when <span class="math inline">\(n=1\)</span>, <span class="math inline">\(X\)</span> is a Bernoulli random variable, so it follows that the Bernoulli distribution is also an exponential family.
</div>


<div class="example">
<p><span id="exm:exp-family-poisson" class="example"><strong>Example 5.2  (Poisson random variables)  </strong></span>Let <span class="math inline">\(X\sim\text{Poisson}\left(\lambda\right)\)</span>, where <span class="math inline">\(\lambda&gt;0\)</span>. Recall that <span class="math inline">\(X\)</span> represents the frequency with which a specified event occurs given some fixed dimension, such as space or time, and its pmf is given by</p>
<p><span class="math display">\[
f\left(x|\lambda\right)=\frac{\mathrm{e}^{-\lambda}\lambda^{x}}{x!}
\]</span></p>
<p>for <span class="math inline">\(x=0,1,2,\ldots\)</span> and <span class="math inline">\(f\left(x|\lambda\right)=0\)</span> otherwise. We can write <span class="math inline">\(f\)</span> as</p>
<p><span class="math display">\[
f\left(x|\lambda\right)
  =\frac{\mathrm{e}^{-\lambda}\lambda^{x}}{x!}
  =\frac{1}{x!}\mathrm{e}^{-\lambda}\exp\left\{ \log\left(\lambda^{x}\right)\right\}   
  =\frac{1}{x!}\mathrm{e}^{-\lambda}\exp\left\{ x\log\lambda\right\}.
\]</span></p>
<p>It follows that the Poisson distribution is an exponential family, with</p>
<span class="math display">\[
h\left(x\right)=\frac{1}{x!},\quad c\left(\lambda\right)=\mathrm{e}^{-\lambda},\quad t\left(x\right)=x,\quad \omega\left(\lambda\right)=\log\lambda. 
\]</span>
</div>


<div class="example">
<p><span id="exm:exp-family-normal" class="example"><strong>Example 5.3  (Normal random variables)  </strong></span>Let <span class="math inline">\(X\sim\mathcal{N}\left(\mu,\sigma^{2}\right)\)</span>, where <span class="math inline">\(\mu\in\mathbb{R}\)</span> and <span class="math inline">\(\sigma^{2}&gt;0\)</span>. A pdf for <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
f\left(x|\mu,\sigma^{2}\right)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right\}
\]</span></p>
<p>for <span class="math inline">\(x\in\mathbb{R}\)</span>. We begin with the case that <span class="math inline">\(\sigma^{2}\)</span> is known, so that <span class="math inline">\(\theta=\mu\)</span>. We can write <span class="math inline">\(f\)</span> as</p>
<p><span class="math display">\[
\begin{align*}
f\left(x|\mu\right) &amp; =\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{x^{2}-2\mu x+\mu^{2}}{2\sigma^{2}}\right\} \\
    &amp; =\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{x^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} \exp\left\{ x\frac{\mu}{\sigma^{2}}\right\},
\end{align*}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
h\left(x\right)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{-\frac{x^{2}}{2\sigma^{2}}\right\},\quad c\left(\mu\right)=\exp\left\{-\frac{\mu^{2}}{2\sigma^{2}}\right\},\quad t\left(x\right)=x,\quad \omega\left(\mu\right)=\frac{\mu}{\sigma^{2}},
\]</span></p>
<p>hence the normal distribution with known variance is an exponential family. Now suppose that <span class="math inline">\(\sigma^{2}\)</span> is unknown, so that <span class="math inline">\(\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)\)</span>. We can write <span class="math inline">\(f\)</span> as</p>
<p><span class="math display">\[
\begin{align*}
f\left(x|\mu,\sigma^{2}\right) &amp; =\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right\} \\
    &amp; =\frac{1}{\sqrt{2\pi}}\left(\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{x^{2}-2\mu x+\mu^{2}}{2\sigma^{2}}\right\} \\
    &amp; =\frac{1}{\sqrt{2\pi}}\exp\left\{ \log\left(\sigma^{2}\right)^{-1/2}\right\} \exp\left\{ -\frac{x^{2}}{2\sigma^{2}}+\frac{\mu x}{\sigma^{2}}\right\} \exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} \\
    &amp; =\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{1}{2}\left(\frac{\mu^{2}}{\sigma^{2}}+\log\sigma^{2}\right)\right\} \exp\left\{ -\frac{x^{2}}{2\sigma^{2}}+\frac{\mu x}{\sigma^{2}}\right\},
\end{align*}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\begin{align*}
  h\left(x\right) &amp;= \frac{1}{\sqrt{2\pi}}, &amp; c\left(\boldsymbol{\theta}\right) &amp;= \exp\left\{-\frac{1}{2}\left(\frac{\mu^{2}}{\sigma^{2}}+\log\sigma^{2}\right)\right\}, \\
  t_{1}\left(x\right) &amp;= -\frac{x^{2}}{2}, &amp; \omega_{1}\left(\boldsymbol{\theta}\right) &amp;= \frac{1}{\sigma^{2}}, \\ 
  t_{2}\left(x\right) &amp;= x, &amp; \omega_{2}\left(\boldsymbol{\theta}\right) &amp;=\frac{\mu}{\sigma^{2}}.
\end{align*}
\]</span></p>
Thus, the normal distribution with unknown variance is also an exponential family (and the first exponential family we have seen where <span class="math inline">\(k&gt;1\)</span>).
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-54" class="definition"><strong>Definition 5.1  </strong></span>The indicator function of a set <span class="math inline">\(\mathcal{A}\)</span>, denoted by <span class="math inline">\(I_{\mathcal{A}}\left(x\right)\)</span>, is the function</p>
<span class="math display">\[
I_{\mathcal{A}}\left(x\right)=
  \begin{cases}
    1, &amp; x\in \mathcal{A}\\
    0, &amp; x\notin \mathcal{A}
\end{cases}.
\]</span>
</div>


<div class="example">
<p><span id="exm:exp-family-uniform" class="example"><strong>Example 5.4  (Uniform random variables)  </strong></span>Let <span class="math inline">\(X\sim\mathcal{U}\left(a,b\right)\)</span>, where <span class="math inline">\(-\infty&lt;a&lt;b&lt;\infty\)</span>. A pdf for <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
f\left(x|a,b\right)=\frac{1}{b-a}
\]</span></p>
<p>for <span class="math inline">\(x\in\left[a,b\right]\)</span> and <span class="math inline">\(f\left(x|a,b\right)=0\)</span> otherwise. Let <span class="math inline">\(\mathcal{A}=\left\{ x:x\in\left[a,b\right]\right\}\)</span> and let <span class="math inline">\(I_{\mathcal{A}}\)</span> be the indicator function of <span class="math inline">\(\mathcal{A}\)</span>. Then, we can write <span class="math inline">\(f\)</span> as</p>
<p><span class="math display">\[
f\left(x|a,b\right)=
  \frac{1}{b-a}I_{\mathcal{A}}\left(x\right)=
  \frac{1}{b-a}I_{\left[a,b\right]}\left(x\right).
\]</span></p>
Notice that <span class="math inline">\(I_{\left[a,b\right]}\left(x\right)\)</span> is not a function of <span class="math inline">\(x\)</span> exclusively, not a function of <span class="math inline">\(\boldsymbol{\theta}=\left(a,b\right)\)</span> exclusively, and cannot be written as an exponential. Because the entire pdf must be incorporated into <span class="math inline">\(h\left(x\right)\)</span>, <span class="math inline">\(c\left(\boldsymbol{\theta}\right)\)</span>, <span class="math inline">\(t_{j}\left(x\right)\)</span>, and <span class="math inline">\(\omega_{j}\left(\boldsymbol{\theta}\right)\)</span>, it follows that the uniform distribution is not an exponential family.
</div>


<div class="example">
<p><span id="exm:3-param-exp-family" class="example"><strong>Example 5.5  (Three-parameter exponential family distribution)  </strong></span>Consider the family of distributions with densities</p>
<p><span class="math display">\[
f\left(x|\theta\right)  =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right]
\]</span></p>
<p>for <span class="math inline">\(x\in\mathbb{R}\)</span>. Express <span class="math inline">\(f\left(x|\theta\right)\)</span> in exponential family form.</p>
<p>Recall that the binomial theorem states that</p>
<p><span class="math display">\[
\left(x+y\right)^{n}=\sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k},
\]</span></p>
<p>so we have</p>
<span class="math display">\[\begin{align*}
f\left(x|\theta\right)  &amp; =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right] \\
    &amp; =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\sum_{k=0}^{4}\binom{4}{k}x^{k}\left(-\theta\right)^{4-k}\right\} \\
    &amp; =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\left[1\cdot1\cdot\theta^{4}-4x\theta^{3}+6x^{2}\theta^{2}-4x^{3}\theta+1\cdot x^{4}\cdot1\right]\right\},
\end{align*}\]</span>
<p>so that</p>
<p><span class="math display">\[
\begin{align*}
  h\left(x\right) &amp; = \frac{2}{\Gamma\left(1/4\right)}\exp\left\{-x^{4}\right\}, &amp; c\left(\theta\right) &amp; = \exp\left\{-\theta^{4}\right\}, \\
  t_{1}\left(x\right) &amp; = 4x^{3}, &amp; \omega_{1}\left(\theta\right) &amp; = \theta, \\
  t_{2}\left(x\right) &amp; = -6x^{2}, &amp; \omega_{2}\left(\theta\right) &amp; = \theta^{2}, \\
  t_{3}\left(x\right) &amp; = 4x, &amp; \omega_{3}\left(\theta\right) &amp; = \theta^{3}.
\end{align*}
\]</span></p>
Thus, this family of distributions is a 3-parameter exponential family.
</div>


<div class="theorem">
<span id="thm:unnamed-chunk-55" class="theorem"><strong>Theorem 5.1  </strong></span>Random samples from <span class="math inline">\(k\)</span>-parameter exponential families have joint distributions which are <span class="math inline">\(k\)</span>-parameter exponential families.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Suppose that a random variable <span class="math inline">\(X\)</span> has a pdf <span class="math inline">\(f\left(x|\theta\right)\)</span>, and that <span class="math inline">\(f\)</span> is part of an exponential family, so that <span class="math inline">\(f\)</span> can be written as</p>
<p><span class="math display">\[
f=h\left(x\right)c\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}t_{j}\left(x\right)\omega_{j}\left(\theta\right)\right\} .
\]</span></p>
<p>Now suppose that <span class="math inline">\(X_{1},X_{2},\ldots,X_{n}\)</span> is a random sample from a population having the distribution of <span class="math inline">\(X\)</span>. It follows that the <span class="math inline">\(X_{i}\text{&#39;s}\)</span> are independent and identically distributed, and that each <span class="math inline">\(X_{i}\)</span> has the same cdf as <span class="math inline">\(X\)</span>, and therefore that <span class="math inline">\(f\left(x|\theta\right)\)</span> is a pdf for each <span class="math inline">\(X_{i}\)</span>. Then, the joint pdf of the <span class="math inline">\(X_{i}\text{&#39;s}\)</span> is given by</p>
<span class="math display">\[\begin{align*}
f\left(\mathbf{x}|\theta\right) &amp; =\prod_{i=1}^{n}f\left(x_{i}|\theta\right) \\
    &amp; =\prod_{i=1}^{n}\left[h\left(x_{i}\right)c\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \right] \\
    &amp; =\left[\prod_{i=1}^{n}h\left(x_{i}\right)\right]\left[c\left(\theta\right)\right]^{n}\exp\left\{ \sum_{j=1}^{k}\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\}
\end{align*}\]</span>
<p>Then, let</p>
<p><span class="math display">\[
h^{*}\left(x\right)=\prod_{i=1}^{n}h\left(x_{i}\right)\quad\text{and}\quad c^{*}\left(\theta\right)=\left[c\left(\theta\right)\right]^{n},
\]</span></p>
<p>so that we have</p>
<span class="math display">\[\begin{align*}
f\left(\mathbf{x}|\theta\right) &amp; =\left[\prod_{i=1}^{n}h\left(x_{i}\right)\right]\left[c\left(\theta\right)\right]^{n}\exp\left\{ \sum_{j=1}^{k}\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \\
    &amp; =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\left(\omega_{j}\left(\theta\right)\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\right)\right\} .
\end{align*}\]</span>
<p>Now, let <span class="math inline">\(T_{j}\left(x\right) =\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\)</span>, so that</p>
<p><span class="math display">\[
f\left(\mathbf{x}|\theta\right) =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} .
\]</span></p>
Thus, the joint pdf <span class="math inline">\(f\left(\mathbf{x}|\theta\right)\)</span> is a <span class="math inline">\(k\)</span>-parameter exponential family.
</div>

<div id="natural-parameters" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Natural parameters</h3>
<p>An exponential family is sometimes reparametrized as</p>
<p><span class="math display">\[
f\left(x|\boldsymbol{\eta}\right)   =h\left(x\right)c^{*}\left(\boldsymbol{\eta}\right)\exp\left\{\sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\},
\]</span></p>
<p>where the natural parameters are defined by <span class="math inline">\(\eta_{j}=\omega_{j}\left(\theta\right)\)</span> and the natural parameter space is</p>
<p><span class="math display">\[
\mathcal{H}=\left\{ \boldsymbol{\eta}=\left(\eta_{1},\ldots,\eta_{k}\right):\int h\left(x\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} \dif x&lt;\infty\right\}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
c^{*}\left(\boldsymbol{\eta}\right)=\frac{1}{\int h\left(x\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\dif x\right\} },
\]</span></p>
<p>which ensures that the pdf integrates to 1.</p>

<div class="example">
<p><span id="exm:natural-param-binomial" class="example"><strong>Example 5.6  (Binomial random variables)  </strong></span>Express the pmf of <span class="math inline">\(X\sim\mathcal{B}\left(n,p\right)\)</span> using a natural parameterization.</p>
<p>From Example <a href="common-families-of-distributions.html#exm:exp-family-binomial">5.1</a>, the pmf of <span class="math inline">\(X\)</span> can be written as</p>
<p><span class="math display">\[
f\left(x|p\right)   =\binom{n}{x}\left(1-p\right)^{n}\exp\left\{ x\log\frac{p}{1-p}\right\} ,
\]</span></p>
<p>where <span class="math inline">\(k=1\)</span> and</p>
<p><span class="math display">\[
\omega\left(p\right)    =\log\frac{p}{1-p}.
\]</span></p>
<p>Then, let <span class="math inline">\(\eta=\omega\left(p\right)\)</span>, so that</p>
<p><span class="math display">\[
\mathrm{e}^{\eta}=\frac{p}{1-p}\implies p=\mathrm{e}^{\eta}\left(1-p\right)=\mathrm{e}^{\eta}-\mathrm{e}^{\eta}p\implies\mathrm{e}^{\eta}=p\left(1+\mathrm{e}^{\eta}\right)\implies p=\frac{\mathrm{e}^{\eta}}{1+\mathrm{e}^{\eta}}.
\]</span></p>
<p>Then, we have</p>
<p><span class="math display">\[
c\left(p\right)=\left(1-p\right)^{n}\implies c^{\ast}\left(\eta\right)=\left(1-\frac{\mathrm{e}^{\eta}}{1+\mathrm{e}^{\eta}}\right)^{n}=\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}
\]</span></p>
<p>and</p>
<span class="math display">\[
f\left(x|\eta\right)    =\binom{n}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}\mathrm{e}^{x\eta}.
\]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-57" class="example"><strong>Example 5.7  (Poisson random variables)  </strong></span>Express the pmf of <span class="math inline">\(X\sim\text{Poisson}\left(\lambda\right)\)</span> using a natural parameterization.</p>
<p>From Example <a href="common-families-of-distributions.html#exm:exp-family-poisson">5.2</a>, the pmf of <span class="math inline">\(X\)</span> can be written as</p>
<p><span class="math display">\[
f\left(x|\lambda\right) =\frac{1}{x!}\mathrm{e}^{-\lambda}\exp\left\{ x\log\lambda\right\} ,
\]</span></p>
<p>where <span class="math inline">\(k=1\)</span> and <span class="math inline">\(\omega\left(\lambda\right) =\log\lambda\)</span>. Then, let <span class="math inline">\(\eta=\omega\left(\lambda\right)\)</span>, so that</p>
<p><span class="math display">\[
\eta=\log\lambda\implies\mathrm{e}^{\eta}=\mathrm{e}^{\log\lambda}\implies\mathrm{e}^{\eta}=\lambda.
\]</span></p>
<p>Then, we have <span class="math inline">\(c\left(\lambda\right)=\mathrm{e}^{-\lambda}\implies c^{\ast}\left(\eta\right)=\exp\left\{-\mathrm{e}^{\eta}\right\}\)</span> and</p>
<span class="math display">\[
f\left(x|\eta\right)    =\frac{1}{x!}\exp\left\{-\mathrm{e}^{\eta}\right\}\mathrm{e}^{x\eta}.
\]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-58" class="example"><strong>Example 5.8  (Bernoulli random variables)  </strong></span>Express the pmf of <span class="math inline">\(X\sim\text{Bernoulli}\left(p\right)\)</span> using a natural parameterization.</p>
<p>Noting that <span class="math inline">\(X\sim\mathcal{B}\left(1,p\right)\)</span>, from Example <a href="common-families-of-distributions.html#exm:natural-param-binomial">5.6</a>, we have</p>
<span class="math display">\[
f\left(x|\eta\right)    = \binom{n}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}\mathrm{e}^{x\eta}
  = \binom{1}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)\mathrm{e}^{x\eta}
  = \frac{\mathrm{e}^{x\eta}}{1+\mathrm{e}^{\eta}}.
\]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-59" class="example"><strong>Example 5.9  (Normal random variables)  </strong></span>Express the pdf of <span class="math inline">\(X\sim\mathcal{N}\left(\mu,\sigma^{2}\right)\)</span> using a natural parameterization, where <span class="math inline">\(\sigma&gt;0\)</span> is unknown.</p>
<p>From Example <a href="common-families-of-distributions.html#exm:exp-family-normal">5.3</a>, we have</p>
<p><span class="math display">\[
f\left(x|\boldsymbol{\theta}\right) = \frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{1}{2}\left(\frac{\mu^{2}}{\sigma^{2}}+\log\sigma^{2}\right)\right\} \exp\left\{ -\frac{x^{2}}{2\sigma^{2}}+\frac{\mu x}{\sigma^{2}}\right\}.
\]</span></p>
<p>Then, let <span class="math inline">\(\eta_{1}=\omega_{1}\left(\boldsymbol{\theta}\right)\)</span>, so that</p>
<p><span class="math display">\[
\eta_{1}=\frac{1}{\sigma^{2}}\implies\sigma^{2}=\frac{1}{\eta_{1}}
\]</span></p>
<p>and let <span class="math inline">\(\eta_{2}=\omega_{2}\left(\boldsymbol{\theta}\right)\)</span>, so that</p>
<p><span class="math display">\[
\eta_{2}=\frac{\mu}{\sigma^{2}}\implies\mu=\sigma^{2}\eta_{2}=\frac{\eta_{2}}{\eta_{1}}.
\]</span></p>
<p>Then, we have</p>
<p><span class="math display">\[
\begin{align*}
  c\left(\boldsymbol{\theta}\right) &amp; = \exp\left\{ -\frac{1}{2}\left(\frac{\mu^{2}}{\sigma^{2}}+\log\sigma^{2}\right)\right\} \\
\implies c^{\ast}\left(\boldsymbol{\eta}\right) &amp; = \exp\left\{-\frac{1}{2}\left(\frac{\left(\eta_{2}/\eta_{1}\right)^{2}}{1/\eta_{1}}+\log\frac{1}{\eta_{1}}\right)\right\} \\
  &amp; = \exp\left\{-\frac{1}{2}\left(\frac{\eta_{2}^{2}}{\eta_{1}}-\log\eta_{1}\right)\right\},
\end{align*}
\]</span></p>
<p>so that</p>
<span class="math display">\[
f\left(x|\boldsymbol{\eta}\right)   =\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{1}{2}\left(\frac{\eta_{2}^{2}}{\eta_{1}}-\log\eta_{1}\right)\right\} \exp\left\{ -\frac{\eta_{1}x^{2}}{2}+\eta_{2}x\right\}.
\]</span>
</div>


<div class="theorem">
<p><span id="thm:expected-value-exp-family" class="theorem"><strong>Theorem 5.2  </strong></span>Let <span class="math inline">\(X\)</span> have density in an exponential family. Then,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\E\left[t_{j}\left(X\right)\right]=-\dfrac{\partial}{\partial\eta_{j}}\log c^{*}\left(\boldsymbol{\eta}\right)\)</span>,</li>
<li><span class="math inline">\(\Var\left(t_{j}\left(X\right)\right)=-\dfrac{\partial^{2}}{\partial\eta_{j}^{2}}\log c^{*}\left(\boldsymbol{\eta}\right)\)</span>,</li>
</ol>
<p>and the moment-generating function for <span class="math inline">\(\left(X_{1},\ldots,X_{k}\right)\)</span> is</p>
<span class="math display">\[
M_{\left(X_{1},\ldots,X_{k}\right)}\left(s_{1},\ldots,s_{k}\right)=\E\left[\exp\left\{\sum_{j=1}^{k}s_{j}X_{j}\right\}\right].
\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We begin with the pdf of an exponential family, i.e.,</p>
<span class="math display">\[\begin{align*}
1   &amp; =\int f\left(x|\theta\right)\dif x \\
    &amp; =\int h\left(x\right)c\left(\theta\right)\exp\left(\sum_{i=1}^{k}\omega_{i}\left(\theta\right)t_{i}\left(x\right)\right)\dif x \\
    &amp; =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x,
\end{align*}\]</span>
<p>where the second equality follows because <span class="math inline">\(f\)</span> is in an exponential family, and where the third equality is the natural parameterization of <span class="math inline">\(f\)</span>. Taking the derivative of both sides with repect to <span class="math inline">\(\eta_{j}\)</span> gives</p>
<span class="math display">\[\begin{align*}
\frac{\partial}{\partial\eta_{j}}1 &amp; =\frac{\partial}{\partial\eta_{j}}\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
\implies0   &amp; =\int\frac{\partial}{\partial\eta_{j}}\left[h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\right]\dif x \\
    &amp; =\int\left[h\left(x\right)\left[\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)+c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\right]\right]\dif x \\
    &amp; =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; \quad+\int h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; \quad+\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right],
\end{align*}\]</span>
<p>where the final equality follows from the definition of expected value. Observe that for some differentiable function <span class="math inline">\(g\left(x\right)\)</span>, we have</p>
<p><span class="math display">\[
g&#39;\left(x\right)=\frac{g\left(x\right)}{g\left(x\right)}g&#39;\left(x\right)=g\left(x\right)\frac{\dif}{\dif x}\log\left(g\left(x\right)\right),
\]</span></p>
<p>which leads to</p>
<span class="math display">\[\begin{align*}
0   &amp; =\int h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right] \\
    &amp; =\frac{\partial}{\partial\eta_{j}}\left(\log c^{*}\left(\eta\right)\right)\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right] \\
    &amp; =\frac{\partial}{\partial\eta_{j}}\left(\log c^{*}\left(\eta\right)\right)\cdot1+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right],
\end{align*}\]</span>
<p>where the final equality follows from the fact that the integral of a pdf over its range of positivity is equal to <span class="math inline">\(1\)</span>. Then,</p>
<span class="math display">\[\begin{align*}
-\frac{\partial}{\partial\eta_{j}}\log c^{*}\left(\eta\right)   &amp; =\E\left[\frac{\partial}{\partial\eta_{j}}\eta_{1}t_{1}\left(X\right)+\ldots+\frac{\partial}{\partial\eta_{j}}\eta_{j}t_{j}\left(X\right)+\ldots+\frac{\partial}{\partial\eta_{j}}\eta_{k}t_{k}\left(X\right)\right] \\
    &amp; =\E\left[0\cdot t_{1}\left(X\right)+\ldots+1\cdot t_{j}\left(X\right)+\ldots+0\cdot t_{k}\left(X\right)\right] \\
    &amp; =\E\left[t_{j}\left(X\right)\right],
\end{align*}\]</span>
<p>proving the first claim. Then,</p>
<span class="math display">\[\begin{align*}
-\frac{\partial^{2}}{\partial\eta_{j}^{2}}\log c^{*}\left(\eta\right)   &amp; =\frac{\partial}{\partial\eta_{j}}\left(-\frac{\partial}{\partial\eta_{j}}\log c^{*}\left(\eta\right)\right) \\
    &amp; =\frac{\partial}{\partial\eta_{j}}\E\left[t_{j}\left(X\right)\right] \\
    &amp; =\frac{\partial}{\partial\eta_{j}}\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\int t_{j}\left(x\right)h\left(x\right)\frac{\partial}{\partial\eta_{j}}c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\int t_{j}\left(x\right)h\left(x\right)\left[\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)+c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\right]\dif x \\
    &amp; =\int t_{j}\left(x\right)h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; \quad+\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x.
\end{align*}\]</span>
<p>The first summand becomes</p>
<span class="math display">\[\begin{align*}
  &amp; \quad\,\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\E\left[t_{j}\left(X\right)\right] \\
    &amp; =\left(-\E\left[t_{j}\left(X\right)\right]\right)\E\left[t_{j}\left(X\right)\right] \\
    &amp; =-\left(\E\left[t_{j}\left(X\right)\right]\right)^{2},
\end{align*}\]</span>
<p>where the penultimate equality follows from the first part of the proof. The second summand becomes</p>
<span class="math display">\[\begin{align*}
  &amp; \quad\,\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(\frac{\partial}{\partial\eta_{1}}n_{1}t_{1}\left(x\right)+\cdots+\frac{\partial}{\partial\eta_{k}}n_{k}t_{k}\left(x\right)\right)\dif x \\
    &amp; =\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(0+\cdots+1\cdot t_{j}\left(x\right)+\cdots+0\right)\dif x \\
    &amp; =\int\left(t_{j}\left(x\right)\right)^{2}h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x \\
    &amp; =\E\left[\left(t_{j}\left(X\right)\right)^{2}\right].
\end{align*}\]</span>
<p>For some random variable <span class="math inline">\(Y\)</span> with defined second central moment, we have</p>
<span class="math display">\[\begin{align*}
\Var\left(Y\right) &amp; =\E\left[\left(Y-\E\left[Y\right]\right)^{2}\right] \\
    &amp; =\E\left[Y^{2}-2Y\E\left[Y\right]+\left(\E\left[Y\right]\right)^{2}\right] \\
    &amp; =\E\left[Y^{2}\right]-2\E\left[Y\E\left[Y\right]\right]+\E\left[\left(\E\left[Y\right]\right)^{2}\right] \\
    &amp; =\E\left[Y^{2}\right]-2\E\left[Y\right]\E\left[Y\right]+\left(\E\left[Y\right]\right)^{2}\tag{$\E\left[Y\right]$ is constant} \\
    &amp; =\E\left[Y^{2}\right]-2\left(\E\left[Y\right]\right)^{2}+\left(\E\left[Y\right]\right)^{2} \\
    &amp; =\E\left[Y^{2}\right]-\left(\E\left[Y\right]\right)^{2}.
\end{align*}\]</span>
<p>It follows that</p>
<p><span class="math display">\[
-\frac{\partial^{2}}{\partial\eta_{j}^{2}}\log c^{*}\left(\eta\right)=-\left(\E\left[t_{j}\left(X\right)\right]\right)^{2}+\E\left[\left(t_{j}\left(X\right)\right)^{2}\right]=\Var\left(t_{j}\left(X\right)\right),
\]</span></p>
proving the second claim.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-61" class="example"><strong>Example 5.10  (Expected value of a binomial random variable)  </strong></span>Find the expected value of <span class="math inline">\(X\sim\mathcal{B}\left(n,p\right)\)</span>.</p>
<p>We will find <span class="math inline">\(\E\left[X\right]\)</span> by applying Theorem <a href="common-families-of-distributions.html#thm:expected-value-exp-family">5.2</a>. From Example <a href="common-families-of-distributions.html#exm:natural-param-binomial">5.6</a>, the pmf of <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
f\left(x|\eta\right)    =\binom{n}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}\mathrm{e}^{x\eta},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\eta=\log\frac{p}{1-p}\implies p=\frac{1}{1+\mathrm{e}^{\eta}}.
\]</span></p>
<p>From the general form of a natural parameterization, we have <span class="math inline">\(k=1\)</span>, <span class="math inline">\(t\left(x\right)=x\)</span>, and <span class="math inline">\(c^{*}\left(\eta\right)=\left(1/\left(1+\mathrm{e}^{\eta}\right)\right)^{n}\)</span>. Then, we have</p>
<span class="math display">\[\begin{align*}
\E\left[X\right] &amp; =\E\left[t\left(X\right)\right] \\
    &amp; =-\frac{\partial}{\partial\eta}\log\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n} \\
    &amp; =-\frac{\partial}{\partial\eta}\log\left(1+\mathrm{e}^{\eta}\right)^{-n} \\
    &amp; =-\frac{\partial}{\partial\eta}\left(-n\log\left(1+\mathrm{e}^{\eta}\right)\right) \\
    &amp; =n\frac{\partial}{\partial\eta}\log\left(1+\mathrm{e}^{\eta}\right) \\
    &amp; =n\frac{\mathrm{e}^{\eta}}{1+\mathrm{e}^{\eta}} \\
    &amp; =np.
\end{align*}\]</span>
</div>


<div class="theorem">
<p><span id="thm:mgf-natural-param" class="theorem"><strong>Theorem 5.3  </strong></span>If <span class="math inline">\(X\)</span> has a <span class="math inline">\(k\)</span>-parameter exponential family distribution indexed by the natural parameters, then for any <span class="math inline">\(\eta\)</span> on the interior of the natural parameter space, the mgf of <span class="math inline">\(\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)\)</span> exists and is given by</p>
<p><span class="math display">\[
M_{\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)}\left(s_{1},\ldots,s_{k}\right)  =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}
\]</span></p>
where <span class="math inline">\(\eta+s\)</span> is the vector <span class="math inline">\(\left(\eta_{1}+s_{1},\ldots,\eta_{k}+s_{k}\right)\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Suppose that <span class="math inline">\(X\)</span> is a <span class="math inline">\(k\)</span>-parameter exponential family distribution indexed by the natural parameters. Then, from Section <a href="common-families-of-distributions.html#natural-parameters">5.1.1</a>, it has a pdf given by</p>
<p><span class="math display">\[
f\left(x|\eta\right)    =h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} .
\]</span></p>
<p>It follows from Theorem <a href="common-families-of-distributions.html#thm:expected-value-exp-family">5.2</a> that</p>
<p><span class="math display">\[
M_{\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)}\left(s_{1},\ldots,s_{k}\right)=\E\left[\mathrm{e}^{\sum_{j=1}^{k}s_{j}t_{j}\left(X\right)}\right],
\]</span></p>
<p>with <span class="math inline">\(X_{i}\)</span> replaced by <span class="math inline">\(t_{i}\left(X\right)\)</span>. Then, we have</p>
<span class="math display">\[\begin{align*}
\E\left[\mathrm{e}^{\sum_{j=1}^{k}s_{j}t_{j}\left(X\right)}\right] &amp; =\int\exp\left\{ \sum_{j=1}^{k}s_{j}t_{j}\left(x\right)\right\} h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} \dif x \\
    &amp; =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}s_{j}t_{j}\left(x\right)+\sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} \dif x \\
    &amp; =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right\} \dif x \\
    &amp; =\frac{c^{*}\left(\eta+s\right)}{c^{*}\left(\eta+s\right)}\int h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right\} \dif x \\
    &amp; =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\int h\left(x\right)c^{*}\left(\eta+s\right)\exp\left\{ \sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right\} \dif x \\
    &amp; =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\cdot\int f\left(x|\eta+s\right)\dif x \\
    &amp; =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\cdot1 \\
    &amp; =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)},
\end{align*}\]</span>
establishing the claim.
</div>


<div class="definition">
<span id="def:unnamed-chunk-63" class="definition"><strong>Definition 5.2  </strong></span>A <em>curved exponential family</em> is a family of densities of the form given in Section <a href="common-families-of-distributions.html#defn-exp-family">5.1</a> for which the dimension of the vector <span class="math inline">\(\boldsymbol{\theta}\)</span> is equal to <span class="math inline">\(d&lt;k\)</span>, where <span class="math inline">\(k\)</span> is the number of terms in the sum in the exponent. If <span class="math inline">\(d=k\)</span>, the family is a <em>full exponential family</em>.
</div>

</div>
<div id="conjugate-prior-distributions" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Conjugate prior distributions</h3>
<p>MOVE THIS TO THE (EVENTUAL) BAYESIAN SECTION (WITH CONSISTENT NOTATION)</p>
<p>Sampling models from exponential families all have conjugate priors. Recall that a one-parameter exponential family model is any model with density that can be expressed as</p>
<p><span class="math display">\[
p\left(x|\theta\right)=h\left(x\right)g\left(\theta\right)\exp\left\{ \phi\left(\theta\right)t\left(x\right)\right\} ,
\]</span></p>
<p>where <span class="math inline">\(\phi\left(\theta\right)\)</span> is called the natural parameter and <span class="math inline">\(t\left(x\right)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>. The conjugate prior has the form</p>
<p><span class="math display">\[
p\left(\theta\right)\propto g\left(\theta\right)^{n_{0}}\exp\left\{ \phi\left(\theta\right)\nu\right\} ,
\]</span></p>
<p>where <span class="math inline">\(\nu\)</span> represents the prior expected value of <span class="math inline">\(t\left(X\right)\)</span> and <span class="math inline">\(n_{0}/\left(n_{0}+n\right)\)</span> represents how informative the prior is relative to the data. With observed iid data <span class="math inline">\(x_{1},\ldots,x_{n}\)</span>, the likelihood for <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[
\mathcal{L}\left(\theta\right)=\prod_{i=1}^{n}p\left(x_{i}|\theta\right)=\left(\prod_{i=1}^{n}h\left(x_{i}\right)\right)g\left(\theta\right)^{n}\exp\left\{ \phi\left(\theta\right)\sum_{i=1}^{n}t\left(x_{i}\right)\right\} .
\]</span></p>
<p>The posterior distribution then becomes</p>
<p><span class="math display">\[
p\left(\theta|x\right)\propto g\left(\theta\right)^{n_{0}+n}\exp\left\{ \phi\left(\theta\right)\left(\nu+\sum_{i=1}^{n}t\left(x_{i}\right)\right)\right\} .
\]</span></p>

<div class="example">
<span id="exm:unnamed-chunk-64" class="example"><strong>Example 5.11  (Binomial conjugate prior)  </strong></span>
</div>

<p>Suppose <span class="math inline">\(Y\sim\mathcal{B}\left(n,\theta\right)\)</span>. Then, <a href="common-families-of-distributions.html#exm:exp-family-binomial">5.1</a> implies that</p>
<p><span class="math display">\[
p\left(y|\theta\right)=\binom{n}{y}\left(1-\theta\right)^{n}\exp\left\{ y\log\frac{\theta}{1-\theta}\right\} ,
\]</span></p>
<p>so that <span class="math inline">\(g\left(\theta\right)=1-\theta\)</span>, <span class="math inline">\(\phi\left(\theta\right)=\log\left(\theta/\left(1-\theta\right)\right)\)</span> is the natural parameter, and <span class="math inline">\(t\left(y\right)=y\)</span> is a sufficient statistic. Then, the conjugate prior for <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[
\begin{align*}
p\left(\theta\right) &amp; \propto g\left(\theta\right)^{n_{0}}\exp\left\{ \phi\left(\theta\right)\nu\right\} \\
    &amp; =\left(1-\theta\right)^{n_{0}}\exp\left\{ \nu\log\frac{\theta}{1-\theta}\right\} \\
    &amp; =\left(1-\theta\right)^{n_{0}}\left[\exp\left\{ \log\frac{\theta}{1-\theta}\right\} \right]^{\nu} \\
    &amp; =\left(1-\theta\right)^{n_{0}}\left(\frac{\theta}{1-\theta}\right)^{\nu} \\
    &amp; =\theta^{\nu}\left(1-\theta\right)^{n_{0}}\left(1-\theta\right)^{-\nu} \\
    &amp; =\theta^{\nu}\left(1-\theta\right)^{n_{0}-\nu},
\end{align*}
\]</span><br />
which we recognize as the kernel of a <span class="math inline">\(\text{Beta}\left(\nu+1,n_{0}-\nu+1\right)\)</span> distribution, i.e., the conjugate prior for <span class="math inline">\(\theta\)</span> is a beta distribution.</p>
</div>
</div>
<div id="location-and-scale-families" class="section level2">
<h2><span class="header-section-number">5.2</span> Location and scale families</h2>
<p>Location families, scale families, and location-scale families are constructed by specifying a single pdf, <span class="math inline">\(f\left(x\right)\)</span>, called the standard pdf for the family. Then, all other pdfs in the family are generated by transforming the standard pdf in a prescribed way.</p>
<div id="location-families" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Location families</h3>

<div class="definition">
<span id="def:unnamed-chunk-65" class="definition"><strong>Definition 5.3  </strong></span>Let <span class="math inline">\(f\left(x\right)\)</span> be any pdf. Then, the family of pdfs indexed by <span class="math inline">\(\mu\)</span>, <span class="math inline">\(f\left(x-\mu\right)\)</span>, is called the <em>location family</em> with respect to the standard pdf <span class="math inline">\(f\)</span>, and <span class="math inline">\(\mu\)</span> is called the location parameter.
</div>

<p>For example, <span class="math inline">\(f\left(x\right)\sim\mathcal{N}\left(0,1^{2}\right)\)</span>, <span class="math inline">\(\mathcal{N}\left(\mu,1^{2}\right)\)</span> is a location family. The location parameter <span class="math inline">\(\mu\)</span> simply shifts the pdf <span class="math inline">\(f\left(x\right)\)</span> so that the shape of the graph is unchanged but the point on the graph that was above <span class="math inline">\(x=0\)</span> under <span class="math inline">\(f\left(x\right)\)</span> is above <span class="math inline">\(x=\mu\)</span> for <span class="math inline">\(f\left(x-\mu\right)\)</span>, thus</p>
<p><span class="math display">\[
P\left(\left\{ -1\leq X\leq2|X\sim f\left(x\right)\right\} \right)  =P\left(\left\{ \mu-1\leq X\leq\mu+2|X\sim f\left(x-\mu\right)\right\} \right).
\]</span></p>
<p>Figure <a href="common-families-of-distributions.html#fig:ex-of-location-normal">5.1</a> shows the normal distribution with <span class="math inline">\(\sigma^{2}=1^{2}\)</span> and <span class="math inline">\(\mu\in\left\{-2,0,2\right\}\)</span> in green, blue, and red, respectively.</p>
<div class="figure" style="text-align: center"><span id="fig:ex-of-location-normal"></span>
<img src="course-notes_files/figure-html/ex-of-location-normal-1.png" alt="example of a normal location family" width="70%" />
<p class="caption">
Figure 5.1: example of a normal location family
</p>
</div>
</div>
<div id="scale-families" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Scale families</h3>

<div class="definition">
<span id="def:unnamed-chunk-66" class="definition"><strong>Definition 5.4  </strong></span>Let <span class="math inline">\(f\left(x\right)\)</span> be any pdf. Then, for any <span class="math inline">\(\sigma&gt;0\)</span>, the family of pdfs <span class="math inline">\(\left(1/\sigma\right)f\left(x/\sigma\right)\)</span>, indexed by the parameter <span class="math inline">\(\sigma\)</span>, is called the <em>scale family</em> with standard pdf <span class="math inline">\(f\left(x\right)\)</span> and <span class="math inline">\(\sigma\)</span> is called the <em>scale parameter</em> of the family.
</div>

<p>For example, <span class="math inline">\(f\left(x\right)\sim\mathcal{N}\left(0,1^{2}\right)\)</span>, <span class="math inline">\(\mathcal{N}\left(0,\sigma^{2}\right)\)</span> is a scale family. The effect of introducing the scale parameter <span class="math inline">\(\sigma\)</span> is either to stretch (<span class="math inline">\(\sigma&gt;1\)</span>) or to contract (<span class="math inline">\(\sigma&lt;1\)</span>) the graph of <span class="math inline">\(f\left(x\right)\)</span> while still maintaining the same basic shape of the graph. Figure <a href="common-families-of-distributions.html#fig:ex-of-scale-normal">5.2</a> shows the normal distribution with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^{2}\in\left\{0.75^{2},1^{2},1.5^{2}\right\}=1^{2}\)</span> green, red, and blue, respectively.</p>
<div class="figure" style="text-align: center"><span id="fig:ex-of-scale-normal"></span>
<img src="course-notes_files/figure-html/ex-of-scale-normal-1.png" alt="example of a normal scale family" width="70%" />
<p class="caption">
Figure 5.2: example of a normal scale family
</p>
</div>

<div class="theorem">
<p><span id="thm:location-scale-family" class="theorem"><strong>Theorem 5.4  </strong></span>Let <span class="math inline">\(f\left(x\right)\)</span> be any pdf. Let <span class="math inline">\(\mu\)</span> be any real number, and let <span class="math inline">\(\sigma\)</span> be any positive real number. Then <span class="math inline">\(X\)</span> is a random variable with pdf <span class="math inline">\(\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)\)</span> if and only if there exists a random variable <span class="math inline">\(Z\)</span> with pdf <span class="math inline">\(f\left(z\right)\)</span> and <span class="math inline">\(X=\sigma Z+\mu\)</span>.</p>
(This is Theorem 3.5.6 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is based on one given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> To prove the “if” part, define <span class="math inline">\(g\left(z\right)=\sigma z+\mu\)</span>, so that</p>
<p><span class="math display">\[
X=g\left(Z\right)=\sigma Z +\mu\implies Z=\frac{X-\mu}{\sigma}.
\]</span></p>
<p>We have</p>
<p><span class="math display">\[
g^{-1}\left(x\right) = g^{-1}\left(g\left(z\right)\right) = z = \frac{x-\mu}{\sigma}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\left|\frac{\dif}{\dif x}g^{-1}\left(x\right)\right|=\left|\frac{1}{\sigma}\right|=\frac{1}{\sigma},
\]</span></p>
<p>which is continuous on <span class="math inline">\(\mathbb{R}\)</span>. Noting that <span class="math inline">\(g\)</span> is monotone, Theorem <a href="probability-theory.html#thm:pdf-of-function-of-rv">3.5</a> implies that the pdf of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
f_{X}\left(x\right) = f_{Z}\left(g^{-1}\left(x\right)\right)\left|\frac{\dif}{\dif x}g^{-1}\left(x\right)\right|
  = f_{Z}\left(z\right)\frac{1}{\sigma}
  = \frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right).
\]</span></p>
<p>To prove the “only if” part, define <span class="math inline">\(g\left(x\right)=\left(x-\mu\right)/\sigma\)</span>, and let</p>
<p><span class="math display">\[
Z=g\left(X\right)=\frac{X-\mu}{\sigma}\implies X=\sigma Z+\mu.
\]</span></p>
<p>We have</p>
<p><span class="math display">\[
g^{-1}\left(z\right)=g^{-1}\left(g\left(x\right)\right) = x = \sigma z+\mu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\left|\frac{\dif}{\dif z}g^{-1}\left(z\right)\right|=\left|\sigma\right|=\sigma,
\]</span></p>
<p>which is continuous on <span class="math inline">\(\mathbb{R}\)</span>. Noting that <span class="math inline">\(g\)</span> is monotone, Theorem <a href="probability-theory.html#thm:pdf-of-function-of-rv">3.5</a> implies that the pdf of <span class="math inline">\(Z\)</span> is</p>
<span class="math display">\[
f_{Z}\left(z\right) = f_{X}\left(g^{-1}\left(z\right)\right)\left|\frac{\dif}{\dif z}g^{-1}\left(z\right)\right|
  = f_{X}\left(x\right)\sigma
  = \frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)\sigma
  = f\left(z\right).
\]</span>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-68" class="theorem"><strong>Theorem 5.5  </strong></span>Let <span class="math inline">\(Z\)</span> be a random variable with pdf <span class="math inline">\(f\left(z\right).\)</span> Suppose <span class="math inline">\(\E\left[Z\right]\)</span> and <span class="math inline">\(\Var\left(Z\right)\)</span> exist. If <span class="math inline">\(X\)</span> is a random variable with pdf <span class="math inline">\(\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)\)</span>, then</p>
<p><span class="math display">\[
\E\left[X\right]=\sigma\E\left[Z\right]+\mu\quad\text{and}\quad\Var\left(X\right)=\sigma^{2}\Var\left(Z\right).
\]</span></p>
(This is Theorem 3.5.7 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
</div>

<p>By Theorem <a href="common-families-of-distributions.html#thm:location-scale-family">5.4</a>, there is a random variable <span class="math inline">\(Z^{\ast}\)</span> with pdf <span class="math inline">\(f\left(z\right)\)</span> and <span class="math inline">\(X=\sigma Z^{\ast}+\mu\)</span>. So</p>
<p><span class="math display">\[
\begin{align*}
  \E\left[X\right] &amp; =\E\left[\sigma Z^{\ast}+\mu\right] \\
  &amp; = \sigma\E\left[Z^{\ast}\right]+\mu \\
  &amp; = \sigma\int z^{\ast}\cdot f\left(z^{\ast}\right)\dif z^{\ast}+\mu \\
  &amp; = \sigma\int z\cdot f\left(z\right)\dif z+\mu \\
  &amp; =\sigma\E\left[Z\right]+\mu,
\end{align*}
\]</span></p>
<p>where the penultimate equality follows because <span class="math inline">\(Z^{\ast}\)</span> has the same pdf as <span class="math inline">\(Z\)</span>. Next,</p>
<p><span class="math display">\[
\begin{align*}
  \Var\left(X\right) &amp; = \E\left[\left(X-\E\left[X\right]\right)^{2}\right] \\
  &amp; = \E\left[\left(\sigma Z^{\ast}+\mu-\E\left[\sigma Z^{\ast}+\mu\right]\right)^{2}\right] \\
  &amp; = \E\left[\left(\sigma Z^{\ast}+\mu-\left(\sigma\E\left[Z^{\ast}\right]+\mu\right)\right)^{2}\right] \\
  &amp; = \E\left[\left(\sigma\left(Z^{\ast}-\E\left[Z^{\ast}\right]\right)\right)^{2}\right] \\
  &amp; = \E\left[\sigma^{2}\left(Z^{\ast}-\E\left[Z^{\ast}\right]\right)^{2}\right] \\
  &amp; = \sigma^{2}\E\left[\left(Z^{\ast}-\E\left[Z^{\ast}\right]\right)^{2}\right] \\
  &amp; = \sigma^{2}\Var\left(Z^{\ast}\right) \\
  &amp; = \sigma^{2}\Var\left(Z\right),
\end{align*}
\]</span></p>
<p>where the final equality follows because <span class="math inline">\(Z^{\ast}\)</span> has the same pdf as <span class="math inline">\(Z\)</span>, and the result has been shown.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-casella2002statistical">
<p>Casella, G., and R.L. Berger. 2002. <em>Statistical Inference</em>. Duxbury Advanced Series in Statistics and Decision Sciences. Thomson Learning. <a href="https://books.google.com/books?id=0x\_vAAAAMAAJ" class="uri">https://books.google.com/books?id=0x\_vAAAAMAAJ</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-algebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="point-estimation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["course-notes.pdf", "course-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
