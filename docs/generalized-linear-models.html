<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Generalized linear models | A Master’s Companion</title>
  <meta name="description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Generalized linear models | A Master’s Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Generalized linear models | A Master’s Companion" />
  
  <meta name="twitter:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

<meta name="author" content="Sean Wilson" />


<meta name="date" content="2019-05-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="point-estimation.html">
<link rel="next" href="machine-representation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\newcommand{\coloneqq}{\mathrel{\mathop:}\mathrel{\mkern-1.2mu}=}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Master's Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="part"><span><b>I Background material</b></span></li>
<li class="chapter" data-level="2" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>2</b> Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="analysis.html"><a href="analysis.html#upper-bounds-and-suprema"><i class="fa fa-check"></i><b>2.1</b> Upper bounds and suprema</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>3</b> Probability theory</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-theory.html"><a href="probability-theory.html#background-material"><i class="fa fa-check"></i><b>3.1</b> Background material</a></li>
<li class="chapter" data-level="3.2" data-path="probability-theory.html"><a href="probability-theory.html#transformations-and-expectations"><i class="fa fa-check"></i><b>3.2</b> Transformations and expectations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-theory.html"><a href="probability-theory.html#distributions-of-functions-of-a-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Distributions of functions of a random variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-theory.html"><a href="probability-theory.html#expected-values"><i class="fa fa-check"></i><b>3.2.2</b> Expected values</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-theory.html"><a href="probability-theory.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>3.2.3</b> Moments and moment generating functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-theory.html"><a href="probability-theory.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.3</b> Multiple random variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="probability-theory.html"><a href="probability-theory.html#conditional-distributions-and-independence"><i class="fa fa-check"></i><b>3.3.1</b> Conditional distributions and independence</a></li>
<li class="chapter" data-level="3.3.2" data-path="probability-theory.html"><a href="probability-theory.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.3.2</b> Covariance and correlation</a></li>
<li class="chapter" data-level="3.3.3" data-path="probability-theory.html"><a href="probability-theory.html#multivariate-distributions"><i class="fa fa-check"></i><b>3.3.3</b> Multivariate distributions</a></li>
<li class="chapter" data-level="3.3.4" data-path="probability-theory.html"><a href="probability-theory.html#inequalities"><i class="fa fa-check"></i><b>3.3.4</b> Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-theory.html"><a href="probability-theory.html#properties-of-a-random-sample"><i class="fa fa-check"></i><b>3.4</b> Properties of a random sample</a><ul>
<li class="chapter" data-level="3.4.1" data-path="probability-theory.html"><a href="probability-theory.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i><b>3.4.1</b> Sums of random variables from a random sample</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability-theory.html"><a href="probability-theory.html#sampling-from-the-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Sampling from the normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="probability-theory.html"><a href="probability-theory.html#order-statistics"><i class="fa fa-check"></i><b>3.4.3</b> Order statistics</a></li>
<li class="chapter" data-level="3.4.4" data-path="probability-theory.html"><a href="probability-theory.html#convergence-concepts"><i class="fa fa-check"></i><b>3.4.4</b> Convergence concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Linear algebra</a></li>
<li class="part"><span><b>II Mathematical statistics</b></span></li>
<li class="chapter" data-level="5" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html"><i class="fa fa-check"></i><b>5</b> Common families of distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#defn-exp-family"><i class="fa fa-check"></i><b>5.1</b> Exponential families</a><ul>
<li class="chapter" data-level="5.1.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#natural-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Natural parameters</a></li>
<li class="chapter" data-level="5.1.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#conjugate-prior-distributions"><i class="fa fa-check"></i><b>5.1.2</b> Conjugate prior distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-and-scale-families"><i class="fa fa-check"></i><b>5.2</b> Location and scale families</a><ul>
<li class="chapter" data-level="5.2.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-families"><i class="fa fa-check"></i><b>5.2.1</b> Location families</a></li>
<li class="chapter" data-level="5.2.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#scale-families"><i class="fa fa-check"></i><b>5.2.2</b> Scale families</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>6</b> Point estimation</a><ul>
<li class="chapter" data-level="6.1" data-path="point-estimation.html"><a href="point-estimation.html#methods-of-finding-estimators"><i class="fa fa-check"></i><b>6.1</b> Methods of finding estimators</a><ul>
<li class="chapter" data-level="6.1.1" data-path="point-estimation.html"><a href="point-estimation.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>6.1.1</b> Maximum likelihood estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>7</b> Generalized linear models</a><ul>
<li class="chapter" data-level="7.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interaction-terms"><i class="fa fa-check"></i><b>7.1</b> Interaction terms</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="machine-representation.html"><a href="machine-representation.html"><i class="fa fa-check"></i><b>8</b> Machine representation</a><ul>
<li class="chapter" data-level="8.1" data-path="machine-representation.html"><a href="machine-representation.html#binary-numbers"><i class="fa fa-check"></i><b>8.1</b> Binary numbers</a></li>
<li class="chapter" data-level="8.2" data-path="machine-representation.html"><a href="machine-representation.html#integers"><i class="fa fa-check"></i><b>8.2</b> Integers</a></li>
<li class="chapter" data-level="8.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-numbers"><i class="fa fa-check"></i><b>8.3</b> Floating-point numbers</a><ul>
<li class="chapter" data-level="8.3.1" data-path="machine-representation.html"><a href="machine-representation.html#special-exponent-values"><i class="fa fa-check"></i><b>8.3.1</b> Special exponent values</a></li>
<li class="chapter" data-level="8.3.2" data-path="machine-representation.html"><a href="machine-representation.html#limitations"><i class="fa fa-check"></i><b>8.3.2</b> Limitations</a></li>
<li class="chapter" data-level="8.3.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-error"><i class="fa fa-check"></i><b>8.3.3</b> Floating-point error</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="em-algorithm.html"><a href="em-algorithm.html"><i class="fa fa-check"></i><b>9</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.1" data-path="em-algorithm.html"><a href="em-algorithm.html#motivation"><i class="fa fa-check"></i><b>9.1</b> Motivation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="em-algorithm.html"><a href="em-algorithm.html#k-means"><i class="fa fa-check"></i><b>9.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="em-algorithm.html"><a href="em-algorithm.html#em-algorithm-1"><i class="fa fa-check"></i><b>9.2</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.2.1" data-path="em-algorithm.html"><a href="em-algorithm.html#algorithmic-perspective"><i class="fa fa-check"></i><b>9.2.1</b> Algorithmic perspective</a></li>
<li class="chapter" data-level="9.2.2" data-path="em-algorithm.html"><a href="em-algorithm.html#statistical-perspective"><i class="fa fa-check"></i><b>9.2.2</b> Statistical perspective</a></li>
<li class="chapter" data-level="9.2.3" data-path="em-algorithm.html"><a href="em-algorithm.html#proof-sketch"><i class="fa fa-check"></i><b>9.2.3</b> Proof sketch</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="em-algorithm.html"><a href="em-algorithm.html#example-gaussian-mixture"><i class="fa fa-check"></i><b>9.3</b> Example: Gaussian mixture</a></li>
<li class="chapter" data-level="9.4" data-path="em-algorithm.html"><a href="em-algorithm.html#applications"><i class="fa fa-check"></i><b>9.4</b> Applications</a><ul>
<li class="chapter" data-level="9.4.1" data-path="em-algorithm.html"><a href="em-algorithm.html#factor-analysis"><i class="fa fa-check"></i><b>9.4.1</b> Factor analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>10</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="10.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#motivation-1"><i class="fa fa-check"></i><b>10.1</b> Motivation</a><ul>
<li class="chapter" data-level="10.1.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#ising-model"><i class="fa fa-check"></i><b>10.1.1</b> Ising model</a></li>
<li class="chapter" data-level="10.1.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#intractable-posterior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Intractable posterior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-is-a-sampling-technique"><i class="fa fa-check"></i><b>10.1.3</b> MCMC is a sampling technique</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>10.2</b> Markov chain</a></li>
<li class="chapter" data-level="10.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#detailed-balance"><i class="fa fa-check"></i><b>10.3</b> Detailed balance</a></li>
<li class="chapter" data-level="10.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolis-hastings"><i class="fa fa-check"></i><b>10.4</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="10.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.5</b> Gibbs Sampling</a><ul>
<li class="chapter" data-level="10.5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#latent-dirichlet-allocation"><i class="fa fa-check"></i><b>10.5.1</b> Latent Dirichlet Allocation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pagerank.html"><a href="pagerank.html"><i class="fa fa-check"></i><b>11</b> PageRank</a><ul>
<li class="chapter" data-level="11.1" data-path="pagerank.html"><a href="pagerank.html#motivation-2"><i class="fa fa-check"></i><b>11.1</b> Motivation</a></li>
<li class="chapter" data-level="11.2" data-path="pagerank.html"><a href="pagerank.html#computing-eigenpairs"><i class="fa fa-check"></i><b>11.2</b> Computing eigenpairs</a></li>
<li class="chapter" data-level="11.3" data-path="pagerank.html"><a href="pagerank.html#algorithm"><i class="fa fa-check"></i><b>11.3</b> Algorithm</a></li>
<li class="chapter" data-level="11.4" data-path="pagerank.html"><a href="pagerank.html#considerations"><i class="fa fa-check"></i><b>11.4</b> Considerations</a><ul>
<li class="chapter" data-level="11.4.1" data-path="pagerank.html"><a href="pagerank.html#connection-to-markov-chains"><i class="fa fa-check"></i><b>11.4.1</b> Connection to Markov chains</a></li>
<li class="chapter" data-level="11.4.2" data-path="pagerank.html"><a href="pagerank.html#calculating-the-dominant-eigenvalue"><i class="fa fa-check"></i><b>11.4.2</b> Calculating the dominant eigenvalue</a></li>
<li class="chapter" data-level="11.4.3" data-path="pagerank.html"><a href="pagerank.html#computational-complexity"><i class="fa fa-check"></i><b>11.4.3</b> Computational complexity</a></li>
<li class="chapter" data-level="11.4.4" data-path="pagerank.html"><a href="pagerank.html#convergence-1"><i class="fa fa-check"></i><b>11.4.4</b> Convergence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Master’s Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-models" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Generalized linear models</h1>
<p>In ordinary least squares with a single predictor, we have the relationship <span class="math inline">\(\E\left[Y_{i}\right]=\alpha+\beta x_{i}\)</span>. This model asserts that the mean response</p>
<ul>
<li>has a “baseline” (intercept) of <span class="math inline">\(\alpha\)</span></li>
<li>will change by <span class="math inline">\(\beta\)</span> (slope) for a one-unit increase in <span class="math inline">\(x_{i}\)</span></li>
</ul>
<p>Because the mean response is <em>linear</em> in the regression coefficients, we refer to OLS as a linear model. OLS assumes that the response <span class="math inline">\(Y\)</span> is continuous, e.g., height. But, analytical interest often lies in responses that are not continuous, and OLS does not model these discrete responses well. In such cases, we can extend the model by assuming other distributions for <span class="math inline">\(Y\)</span>.</p>
<p>Following <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>, a <em>generalized linear model</em> “describes a relationship between the mean of a response variable <span class="math inline">\(Y\)</span> and an independent variable <span class="math inline">\(x\)</span>.” A GLM consists of three components.</p>
<ol style="list-style-type: decimal">
<li><p>The <em>random component</em> or <em>distributional assumption</em> consists of the response variables <span class="math inline">\(Y_{1},\ldots,Y_{n}\)</span>, which are assumed to be independent random variables from the same exponential family (though they are not assumed to be identically distributed). <span class="citation">Fitzmaurice, Laird, and Ware (<a href="#ref-fitzmaurice2012applied">2012</a>)</span> describe the random component as “a probabilistic mechanism by which the responses are assumed to be generated.”</p></li>
<li><p>The <em>systematic component</em> is the linear regression model, i.e., a function of the predictor variables <span class="math inline">\(X_{i}\)</span> that is linear in the parameters <span class="math inline">\(\beta_{i}\)</span> and related to the mean of <span class="math inline">\(Y_{i}\)</span>.</p></li>
<li><p>The <em>link function</em> <span class="math inline">\(g\left(\mu\right)\)</span> links the random and systematic components by asserting that <span class="math inline">\(g\left(\mu_{i}\right)=\mathbf{X}_{i}^{\mathsf{T}}\boldsymbol{\beta}\)</span>, where <span class="math inline">\(\mu_{i}=\E\left[Y_{i}\right]\)</span> and <span class="math inline">\(\mathbf{X}_{i}^{\mathsf{T}}\boldsymbol{\beta}=\sum_{k=1}^{p}\beta_{k}X_{ik}\)</span> is the systematic component.</p></li>
</ol>

<div class="example">
<p><span id="exm:logistic-regression" class="example"><strong>Example 7.1  (Logistic regression)  </strong></span>For <span class="math inline">\(Y_{1},Y_{2},\ldots,Y_{n}\)</span>, let <span class="math inline">\(Y_{i}\sim\text{Bernoulli}\left(p\right)\)</span>, i.e.,</p>
<p><span class="math display">\[
Y_{i}=\begin{cases}
0, &amp; \text{if no event}\\
1, &amp; \text{if event}
\end{cases}.
\]</span></p>
<p>The <span class="math inline">\(Y_{i}\)</span> are the random component. Suppose that we believe that variables <span class="math inline">\(X_{1},\ldots,X_{p}\)</span> are related to the response <span class="math inline">\(Y\)</span>, so that the systematic component is <span class="math inline">\(\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\)</span>. We now consider the link function. The expected value of a Bernoulli random variable is its parameter <span class="math inline">\(p\)</span>, hence <span class="math inline">\(\mu_{i}=\E\left[Y_{i}\right]=p_{i}=P\left(Y_{i}=1\right)\)</span>.</p>
<p>If we use the <em>identity link</em> <span class="math inline">\(g\left(\mu\right)=\mu\)</span>, then our model is <span class="math inline">\(\mu=\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\)</span>. Depending on our predictors, we may obtain values for <span class="math inline">\(\mu\)</span> that lie outside <span class="math inline">\(\left[0,1\right]\)</span>. Because <span class="math inline">\(p\in\left[0,1\right]\)</span>, it is not clear how to interpret such values. Accordingly, we would like to transform <span class="math inline">\(\mu\)</span> such that it always lies in <span class="math inline">\(\left[0,1\right]\)</span>.</p>
<p>The standard logistic function is</p>
<p><span class="math display">\[
\sigma\left(t\right)=\frac{1}{1+\mathrm{e}^{-t}},\quad x\in\mathbb{R}.
\]</span></p>
<p>Observe that</p>
<p><span class="math display">\[
\lim_{t\rightarrow\infty} \sigma\left(t\right)=\frac{1}{\lim_{t\rightarrow\infty}\left(1+\mathrm{e}^{-t}\right)}
  = \frac{1}{1+\lim_{t\rightarrow\infty}\mathrm{e}^{-t}}=\frac{1}{1+0}=1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\lim_{t\rightarrow -\infty}\sigma\left(t\right)=\frac{1}{1+\lim_{t\rightarrow -\infty}\mathrm{e}^{-t}}=\frac{1}{1+\infty}=0.
\]</span></p>
<p>For any <span class="math inline">\(t\in\left(-\infty,\infty\right)\)</span>, we have <span class="math inline">\(\mathrm{e}^{-t}&gt;0\)</span>, so that <span class="math inline">\(1&lt;1+\mathrm{e}^{-t}\)</span>, hence <span class="math inline">\(\sigma\left(t\right)\)</span> is bounded below by 0 and above by 1. The standard logistic function would thus seem to be a good candidate for our link. If we let <span class="math inline">\(t=\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\)</span>, then <span class="math inline">\(\sigma\)</span> will map the model to <span class="math inline">\(\left[0,1\right]\)</span>, i.e.,</p>
<p><span class="math display">\[
p\left(\mathbf{X}\right)=\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}},
\]</span></p>
<p>where we have used the notation <span class="math inline">\(p\)</span> to reflect that we are modeling the probability of a success (the event of interest occurs). We have mapped the model to an interval appropriate for the mean response, but have some work left to do to put it into the correct form for a GLM. The inverse of the logistic function is the <em>logit</em> function, given by</p>
<p><span class="math display">\[
\text{logit}\left(t\right)=\log\frac{t}{1-t}.
\]</span></p>
<p>Observe that</p>
<p><span class="math display">\[
\begin{align*}
  \text{logit}\left(p\left(\mathbf{X}\right)\right) &amp; = \log\frac{p\left(\mathbf{X}\right)}{1-p\left(\mathbf{X}\right)} \\
  &amp; = \log p\left(\mathbf{X}\right)-\log\left(1-p\left(\mathbf{X}\right)\right) \\
  &amp; = \log\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}-\log\left(1-\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}\right) \\
  &amp; = \log 1-\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\log\left(\frac{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}-\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}\right) \\
  &amp; = -\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\log\left(\frac{\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}\right) \\
  &amp; = -\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\left[\log\left(\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)\right] \\
  &amp; = \mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}.
\end{align*}
\]</span></p>
<p>Thus, applying the logit function to the transformed mean response <span class="math inline">\(p\left(\mathbf{X}\right)\)</span> results in an appropriate form for the systematic component of the model. Accordingly, we will take the logit as the link function, i.e., <span class="math inline">\(g\left(\mu\right)=\text{logit}\left(\mu\right)\)</span>, so that the logistic regression model is</p>
<p><span class="math display">\[
\text{logit}\left(\mu\right) = \text{logit}\left(p\right) = \log\left(\frac{p}{1-p}\right)=\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}.
\]</span></p>
<p>We refer to <span class="math inline">\(p/\left(1-p\right)\)</span> as the <em>odds</em> of the event (how likely versus not). We see that logistic regression is linear in the <em>log-odds</em> of <span class="math inline">\(Y\)</span>. When we exponentiate both sides of the above equation, we can interpret the coefficient <span class="math inline">\(\beta_{i}\)</span> as the multiplicative change in the odds of success associated with a one-unit change in <span class="math inline">\(X_{i}\)</span>.</p>
Finally, recall from <a href="common-families-of-distributions.html#exm:natural-param-binomial">5.6</a> that <span class="math inline">\(\log\left(p/\left(1-p\right)\right)\)</span> is the natural parameter of the binomial exponential family. When the natural parameter is used as the link function in a GLM, it is called the <em>canonical link</em>.
</div>

<p>We now consider a GLM suitable for count data.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-70" class="example"><strong>Example 7.2  (Poisson regression)  </strong></span>For <span class="math inline">\(Y_{1},Y_{2},\ldots,Y_{n}\)</span>, let <span class="math inline">\(Y_{i}\sim\text{Poisson}\left(p\right)\)</span>.</p>
In a Poisson regression, we have <span class="math inline">\(\log\left(\lambda\right)=\beta_{0}+\beta_{1}X_{1}+\ldots+\beta_{k}X_{k}\)</span>.
</div>

<div id="interaction-terms" class="section level2">
<h2><span class="header-section-number">7.1</span> Interaction terms</h2>
<p>THIS DOESN’T QUITE BELONG HERE, EVENTUALLY REARRANGE IT</p>
<p>The relationship among the predictors in the models specified above is additive: the effect on the response variable of two predictors <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> for <span class="math inline">\(i\neq j\)</span> is their sum, weighted by their respective coefficients, i.e., <span class="math inline">\(\beta_{i}X_{i}+\beta_{j}X_{j}\)</span>. Suppose that we believe that the effect on the reponse of <span class="math inline">\(X_{j}\)</span> is not independent of the value <span class="math inline">\(X_{i}\)</span>, i.e., the relationship between <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> is <em>not</em> additive. We can test this hypothesis by introducing an <em>interaction term</em>.</p>
<p>We will begin by considering interactions in the context of the German Credit data set from <span class="citation">Dheeru and Karra Taniskidou (<a href="#ref-dua2017">2017</a>)</span>, which is bundled in the <strong><code>caret</code></strong> package. This data set gives the creditworthiness (<code>Good</code> or <code>Bad</code>) of 1000 customers, along with related attributes. We will model the credit class as a function of the customer’s age, whether the customer is a foreign worker, and whether the customer has a telephone number registered under his or her name. (While the data set contains many other predictors, we will use a simple model that allows us to examine a two-way interaction of binary variables.) The response variable (<code>Class</code>) has two levels, so (binary) logistic regression is suitable. We begin by fitting the (additive) model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(
  Class <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>ForeignWorker <span class="op">+</span><span class="st"> </span>Telephone, 
  <span class="dt">family =</span> binomial,
  <span class="dt">data =</span> GermanCredit
)

<span class="kw">summary</span>(glm_fit)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Class ~ Age + ForeignWorker + Telephone, family = binomial, 
##     data = GermanCredit)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2389  -1.4298   0.8033   0.8835   0.9851  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    1.629875   0.594810   2.740  0.00614 **
## Age            0.017587   0.006495   2.708  0.00677 **
## ForeignWorker -1.347619   0.536163  -2.513  0.01196 * 
## Telephone     -0.145706   0.144578  -1.008  0.31355   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1221.7  on 999  degrees of freedom
## Residual deviance: 1203.9  on 996  degrees of freedom
## AIC: 1211.9
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We can exponentiate the coefficients to obtain odds ratios. Recall that the odds of a success are given by</p>
<p><span class="math display">\[
\frac{p}{1-p}=\exp\left\{\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}.
\]</span></p>
<p>for <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n+1}\)</span>. Let <span class="math inline">\(\text{odds}\left(x_{i}\right)\)</span> be the odds of a success when <span class="math inline">\(X_{i}=x_{i}\)</span> and all other predictors are held fixed. Then, the <em>odds ratio</em> associated with a one-unit increase in <span class="math inline">\(X_{i}\)</span> is</p>
<p><span class="math display">\[
\begin{align*}
  \frac{\text{odds}\left(x_{i}+1\right)}{\text{odds}\left(x_{i}\right)} &amp; = \frac{\exp\left\{\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{i}\left(x_{i}+1\right)+\cdots+\beta_{n}x_{n}\right\}}{\exp\left\{\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{i}x_{i}+\cdots+\beta_{n}x_{n}\right\}} \\
  &amp; = \exp\left\{\beta_{i}\left(x_{i}+1\right)\right\}\exp\left\{-\beta_{i}x_{i}\right\} \\
  &amp; = \mathrm{e}^{\beta_{i}}.
\end{align*}
\]</span></p>
<p>Observe that for some <span class="math inline">\(k\in\mathbb{N}^{+}\)</span>, we have</p>
<p><span class="math display">\[
\begin{align*}
  \text{odds}\left(x_{i}+k\right) &amp; = \exp\left\{\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{i}\left(x_{i}+k\right)+\cdots+\beta_{n}x_{n}\right\} \\
  &amp; = \exp\left\{\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{i}x_{i}+\cdots+\beta_{n}x_{n}\right\}\exp\left\{k\beta_{i}\right\} \\
  &amp; = \mathrm{e}^{k\beta_{i}}\cdot\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta} \\
  &amp; = \left(\prod_{i=1}^{k}\mathrm{e}^{\beta_{i}}\right)\text{odds}\left(x_{i}\right).
\end{align*}
\]</span></p>
<p>We thus see that the odds of a success are multiplied by <span class="math inline">\(\mathrm{e}^{\beta_{i}}\)</span> <span class="math inline">\(k\)</span> times for a <span class="math inline">\(k\)</span>-unit increase in <span class="math inline">\(X_{i}\)</span> (again, when all other predictors are held fixed). We could also divide this expression by <span class="math inline">\(\text{odds}\left(x_{i}\right)\)</span> to obtain the odds ratio associated with a <span class="math inline">\(k\)</span>-unit increase in <span class="math inline">\(X_{i}\)</span>. We now consider the odds ratios for our model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cbind</span>(
  <span class="dt">coef =</span> <span class="kw">coef</span>(glm_fit),
  <span class="dt">coef_exp =</span> <span class="kw">exp</span>(<span class="kw">coef</span>(glm_fit))
)</code></pre></div>
<pre><code>##                      coef  coef_exp
## (Intercept)    1.62987542 5.1032389
## Age            0.01758655 1.0177421
## ForeignWorker -1.34761869 0.2598583
## Telephone     -0.14570579 0.8644120</code></pre>
<p>We see that the customer’s <code>Age</code> and status as a <code>ForeignWorker</code> are statistically significant predictors of credit class at the customary <span class="math inline">\(\alpha=0.05\)</span>. A one-unit change in <code>Age</code> is associated with a change in odds of <span class="math inline">\(\left(1.02-1\right)\times 100\%=2\%\)</span>, i.e., for every additional year older, the odds of <code>Good</code> credit increase by a (multiplicative) factor of <span class="math inline">\(2\%\)</span>. Similarly, a one-unit change in <code>ForeignWorker</code> is associated with a change in odds of <span class="math inline">\(\left(0.26-1\right)\times 100\%=-74\%\)</span>, i.e., if the customer is a foreign worker (versus not), the odds of <code>Good</code> credit decrease by <span class="math inline">\(74\%\)</span>.</p>
<p>We observe that that whether the customer has a registered telephone number is not a significant predictor of <code>Good</code> credit. Now, it is plausible that having a registered telephone number is a proxy for how long one has been a resident of the country, hence that workers from abroad might tend to have telephone numbers at a rate different from that of domestic workers. We can test whether this interaction is significant by including the appropriate model term.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_interact_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(
  Class <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>ForeignWorker <span class="op">*</span><span class="st"> </span>Telephone, 
  <span class="dt">family =</span> binomial,
  <span class="dt">data =</span> GermanCredit
)

<span class="kw">summary</span>(glm_interact_fit)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Class ~ Age + ForeignWorker * Telephone, family = binomial, 
##     data = GermanCredit)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3440  -1.4251   0.8021   0.8835   1.0674  
## 
## Coefficients:
##                          Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)             -0.187238   0.940721  -0.199  0.84223   
## Age                      0.017371   0.006503   2.671  0.00755 **
## ForeignWorker            0.496322   0.921310   0.539  0.59008   
## Telephone                2.294990   1.170523   1.961  0.04992 * 
## ForeignWorker:Telephone -2.472636   1.179491  -2.096  0.03605 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1221.7  on 999  degrees of freedom
## Residual deviance: 1199.8  on 995  degrees of freedom
## AIC: 1209.8
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>We see that the interaction term <code>ForeignWorker:Telephone</code> is significant, so we conclude that the effect of having a <code>Telephone</code> number is <em>not</em> independent of whether the customer is a <code>ForeignWorker</code>. The interpretation of the <em>main effects</em> for <code>ForeignWorker</code> and <code>Telephone</code> is very different in the presence of the interaction term. One way to understand the effects is to consider all possible combinations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdata &lt;-<span class="st"> </span><span class="kw">with</span>(
  GermanCredit,
  <span class="kw">expand.grid</span>(
    <span class="dt">ForeignWorker =</span> <span class="kw">unique</span>(ForeignWorker),
    <span class="dt">Telephone =</span> <span class="kw">unique</span>(Telephone),
    <span class="dt">Age =</span> <span class="kw">mean</span>(Age)
  )
)

pred &lt;-<span class="st"> </span><span class="kw">predict</span>(glm_interact_fit, <span class="dt">newdata =</span> newdata)

pred_odds &lt;-<span class="st"> </span><span class="kw">cbind</span>(
  newdata,
  <span class="dt">log_odds =</span> pred,
  <span class="dt">odds =</span> <span class="kw">exp</span>(pred)
)

pred_odds</code></pre></div>
<pre><code>##   ForeignWorker Telephone    Age  log_odds      odds
## 1             1         0 35.546 0.9265514  2.525784
## 2             0         0 35.546 0.4302298  1.537611
## 3             1         1 35.546 0.7489063  2.114686
## 4             0         1 35.546 2.7252202 15.259774</code></pre>
<p>These results give the odds of having <code>Good</code> versus <code>Bad</code> credit for each combination of <code>ForeignWorker</code> and <code>Telephone</code>, holding <code>Age</code> constant (at its mean). Observe that the log-odds is a linear combination of the coefficients for each set of predictors. For example, the first row represents a customer who is a foreign worker, is of mean age, and does not have a telephone. <code>Telephone</code> is 0, so that the interaction <span class="math inline">\(\text{ForeignWorker}\times\text{Telephone}\)</span> is 0, hence the log-odds are the sum of the coefficients for the intercept, <code>ForeignWorker</code>, and <code>Age</code> (multiplied by the mean age), i.e.,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(
  <span class="kw">coef</span>(glm_interact_fit)[<span class="kw">c</span>(<span class="st">&quot;(Intercept)&quot;</span>, <span class="st">&quot;ForeignWorker&quot;</span>)],
  <span class="kw">coef</span>(glm_interact_fit)[<span class="st">&quot;Age&quot;</span>] <span class="op">*</span><span class="st"> </span>pred_odds[<span class="dv">1</span>, <span class="st">&quot;Age&quot;</span>]
)</code></pre></div>
<pre><code>## [1] 0.9265514</code></pre>
<p>Geometrically, including an interaction term is equivalent to testing whether the <em>slope</em> (the coefficient) of the <code>Telephone</code> predictor is different if the customer is a <code>ForeignWorker</code>; else we assume the slope is the same regardless of <code>ForeignWorker</code> status. Also notice that we can no longer interpret the coefficient for <code>Telephone</code> as the unique effect of having a telephone on credit class. The interaction is significant, i.e., <em>the effect of having a telephone depends on whether the customer is a foreign worker</em>, so we must consider the interaction term in addition to the main effect. It also follows that we cannot simply exponentiate the main effect for <code>Telephone</code> to obtain the change in odds of <code>Good</code> credit associated with having a telephone. If we wish to consider an odds ratio in this case, we must first specify whether the customer is a <code>ForeignWorker</code>, i.e., fix the value of that predictor. Then, we can compute the odds in both cases (telephone versus no telephone) and divide to obtain an odds ratio. If we set <code>ForeignWorker</code> to 0, then having a telephone is associated with an increase in the odds of <code>Good</code> credit by a factor of nearly 10.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_odds[<span class="dv">4</span>, <span class="dv">5</span>] <span class="op">/</span><span class="st"> </span>pred_odds[<span class="dv">2</span>, <span class="dv">5</span>]</code></pre></div>
<pre><code>## [1] 9.924342</code></pre>
<p>If we set <code>ForeignWorker</code> to 1, however, having a telephone is associated with a slight decrease in the odds of <code>Good</code> credit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_odds[<span class="dv">3</span>, <span class="dv">5</span>] <span class="op">/</span><span class="st"> </span>pred_odds[<span class="dv">1</span>, <span class="dv">5</span>]</code></pre></div>
<pre><code>## [1] 0.8372396</code></pre>
<p>These very different odds ratios are consistent with our earlier finding that the interaction is significant.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-casella2002statistical">
<p>Casella, G., and R.L. Berger. 2002. <em>Statistical Inference</em>. Duxbury Advanced Series in Statistics and Decision Sciences. Thomson Learning. <a href="https://books.google.com/books?id=0x\_vAAAAMAAJ" class="uri">https://books.google.com/books?id=0x\_vAAAAMAAJ</a>.</p>
</div>
<div id="ref-dua2017">
<p>Dheeru, Dua, and Efi Karra Taniskidou. 2017. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. <a href="http://archive.ics.uci.edu/ml" class="uri">http://archive.ics.uci.edu/ml</a>.</p>
</div>
<div id="ref-fitzmaurice2012applied">
<p>Fitzmaurice, Garrett M, Nan M Laird, and James H Ware. 2012. <em>Applied Longitudinal Analysis</em>. Vol. 998. John Wiley &amp; Sons.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="point-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="machine-representation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["course-notes.pdf", "course-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
