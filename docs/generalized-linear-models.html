<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Master’s Companion</title>
  <meta name="description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="A Master’s Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Master’s Companion" />
  
  <meta name="twitter:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

<meta name="author" content="Sean Wilson">


<meta name="date" content="2018-11-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="point-estimation.html">
<link rel="next" href="machine-representation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\newcommand{\coloneqq}{\mathrel{\mathop:}\mathrel{\mkern-1.2mu}=}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Master's Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="part"><span><b>I Background material</b></span></li>
<li class="chapter" data-level="2" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>2</b> Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="analysis.html"><a href="analysis.html#upper-bounds-and-suprema"><i class="fa fa-check"></i><b>2.1</b> Upper bounds and suprema</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>3</b> Probability theory</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-theory.html"><a href="probability-theory.html#background-material"><i class="fa fa-check"></i><b>3.1</b> Background material</a></li>
<li class="chapter" data-level="3.2" data-path="probability-theory.html"><a href="probability-theory.html#transformations-and-expectations"><i class="fa fa-check"></i><b>3.2</b> Transformations and expectations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-theory.html"><a href="probability-theory.html#distributions-of-functions-of-a-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Distributions of functions of a random variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-theory.html"><a href="probability-theory.html#expected-values"><i class="fa fa-check"></i><b>3.2.2</b> Expected values</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-theory.html"><a href="probability-theory.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>3.2.3</b> Moments and moment generating functions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-theory.html"><a href="probability-theory.html#multiple-random-variables"><i class="fa fa-check"></i><b>3.3</b> Multiple random variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="probability-theory.html"><a href="probability-theory.html#conditional-distributions-and-independence"><i class="fa fa-check"></i><b>3.3.1</b> Conditional distributions and independence</a></li>
<li class="chapter" data-level="3.3.2" data-path="probability-theory.html"><a href="probability-theory.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.3.2</b> Covariance and correlation</a></li>
<li class="chapter" data-level="3.3.3" data-path="probability-theory.html"><a href="probability-theory.html#multivariate-distributions"><i class="fa fa-check"></i><b>3.3.3</b> Multivariate distributions</a></li>
<li class="chapter" data-level="3.3.4" data-path="probability-theory.html"><a href="probability-theory.html#inequalities"><i class="fa fa-check"></i><b>3.3.4</b> Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-theory.html"><a href="probability-theory.html#properties-of-a-random-sample"><i class="fa fa-check"></i><b>3.4</b> Properties of a random sample</a><ul>
<li class="chapter" data-level="3.4.1" data-path="probability-theory.html"><a href="probability-theory.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i><b>3.4.1</b> Sums of random variables from a random sample</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability-theory.html"><a href="probability-theory.html#sampling-from-the-normal-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Sampling from the normal distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="probability-theory.html"><a href="probability-theory.html#order-statistics"><i class="fa fa-check"></i><b>3.4.3</b> Order statistics</a></li>
<li class="chapter" data-level="3.4.4" data-path="probability-theory.html"><a href="probability-theory.html#convergence-concepts"><i class="fa fa-check"></i><b>3.4.4</b> Convergence concepts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Linear algebra</a></li>
<li class="part"><span><b>II Mathematical statistics</b></span></li>
<li class="chapter" data-level="5" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html"><i class="fa fa-check"></i><b>5</b> Common families of distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#defn-exp-family"><i class="fa fa-check"></i><b>5.1</b> Exponential families</a><ul>
<li class="chapter" data-level="5.1.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#natural-parameters"><i class="fa fa-check"></i><b>5.1.1</b> Natural parameters</a></li>
<li class="chapter" data-level="5.1.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#conjugate-prior-distributions"><i class="fa fa-check"></i><b>5.1.2</b> Conjugate prior distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-and-scale-families"><i class="fa fa-check"></i><b>5.2</b> Location and scale families</a><ul>
<li class="chapter" data-level="5.2.1" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#location-families"><i class="fa fa-check"></i><b>5.2.1</b> Location families</a></li>
<li class="chapter" data-level="5.2.2" data-path="common-families-of-distributions.html"><a href="common-families-of-distributions.html#scale-families"><i class="fa fa-check"></i><b>5.2.2</b> Scale families</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>6</b> Point estimation</a><ul>
<li class="chapter" data-level="6.1" data-path="point-estimation.html"><a href="point-estimation.html#methods-of-finding-estimators"><i class="fa fa-check"></i><b>6.1</b> Methods of finding estimators</a><ul>
<li class="chapter" data-level="6.1.1" data-path="point-estimation.html"><a href="point-estimation.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>6.1.1</b> Maximum likelihood estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>7</b> Generalized linear models</a></li>
<li class="chapter" data-level="8" data-path="machine-representation.html"><a href="machine-representation.html"><i class="fa fa-check"></i><b>8</b> Machine representation</a><ul>
<li class="chapter" data-level="8.1" data-path="machine-representation.html"><a href="machine-representation.html#binary-numbers"><i class="fa fa-check"></i><b>8.1</b> Binary numbers</a></li>
<li class="chapter" data-level="8.2" data-path="machine-representation.html"><a href="machine-representation.html#integers"><i class="fa fa-check"></i><b>8.2</b> Integers</a></li>
<li class="chapter" data-level="8.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-numbers"><i class="fa fa-check"></i><b>8.3</b> Floating-point numbers</a><ul>
<li class="chapter" data-level="8.3.1" data-path="machine-representation.html"><a href="machine-representation.html#special-exponent-values"><i class="fa fa-check"></i><b>8.3.1</b> Special exponent values</a></li>
<li class="chapter" data-level="8.3.2" data-path="machine-representation.html"><a href="machine-representation.html#limitations"><i class="fa fa-check"></i><b>8.3.2</b> Limitations</a></li>
<li class="chapter" data-level="8.3.3" data-path="machine-representation.html"><a href="machine-representation.html#floating-point-error"><i class="fa fa-check"></i><b>8.3.3</b> Floating-point error</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="em-algorithm.html"><a href="em-algorithm.html"><i class="fa fa-check"></i><b>9</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.1" data-path="em-algorithm.html"><a href="em-algorithm.html#motivation"><i class="fa fa-check"></i><b>9.1</b> Motivation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="em-algorithm.html"><a href="em-algorithm.html#k-means"><i class="fa fa-check"></i><b>9.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="em-algorithm.html"><a href="em-algorithm.html#em-algorithm-1"><i class="fa fa-check"></i><b>9.2</b> EM algorithm</a><ul>
<li class="chapter" data-level="9.2.1" data-path="em-algorithm.html"><a href="em-algorithm.html#algorithmic-perspective"><i class="fa fa-check"></i><b>9.2.1</b> Algorithmic perspective</a></li>
<li class="chapter" data-level="9.2.2" data-path="em-algorithm.html"><a href="em-algorithm.html#statistical-perspective"><i class="fa fa-check"></i><b>9.2.2</b> Statistical perspective</a></li>
<li class="chapter" data-level="9.2.3" data-path="em-algorithm.html"><a href="em-algorithm.html#proof-sketch"><i class="fa fa-check"></i><b>9.2.3</b> Proof sketch</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="em-algorithm.html"><a href="em-algorithm.html#example-gaussian-mixture"><i class="fa fa-check"></i><b>9.3</b> Example: Gaussian mixture</a></li>
<li class="chapter" data-level="9.4" data-path="em-algorithm.html"><a href="em-algorithm.html#applications"><i class="fa fa-check"></i><b>9.4</b> Applications</a><ul>
<li class="chapter" data-level="9.4.1" data-path="em-algorithm.html"><a href="em-algorithm.html#factor-analysis"><i class="fa fa-check"></i><b>9.4.1</b> Factor analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>10</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="10.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#motivation-1"><i class="fa fa-check"></i><b>10.1</b> Motivation</a><ul>
<li class="chapter" data-level="10.1.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#ising-model"><i class="fa fa-check"></i><b>10.1.1</b> Ising model</a></li>
<li class="chapter" data-level="10.1.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#intractable-posterior-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Intractable posterior distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-is-a-sampling-technique"><i class="fa fa-check"></i><b>10.1.3</b> MCMC is a sampling technique</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#markov-chain"><i class="fa fa-check"></i><b>10.2</b> Markov chain</a></li>
<li class="chapter" data-level="10.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#detailed-balance"><i class="fa fa-check"></i><b>10.3</b> Detailed balance</a></li>
<li class="chapter" data-level="10.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolis-hastings"><i class="fa fa-check"></i><b>10.4</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="10.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.5</b> Gibbs Sampling</a><ul>
<li class="chapter" data-level="10.5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#latent-dirichlet-allocation"><i class="fa fa-check"></i><b>10.5.1</b> Latent Dirichlet Allocation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pagerank.html"><a href="pagerank.html"><i class="fa fa-check"></i><b>11</b> PageRank</a><ul>
<li class="chapter" data-level="11.1" data-path="pagerank.html"><a href="pagerank.html#motivation-2"><i class="fa fa-check"></i><b>11.1</b> Motivation</a></li>
<li class="chapter" data-level="11.2" data-path="pagerank.html"><a href="pagerank.html#computing-eigenpairs"><i class="fa fa-check"></i><b>11.2</b> Computing eigenpairs</a></li>
<li class="chapter" data-level="11.3" data-path="pagerank.html"><a href="pagerank.html#algorithm"><i class="fa fa-check"></i><b>11.3</b> Algorithm</a></li>
<li class="chapter" data-level="11.4" data-path="pagerank.html"><a href="pagerank.html#considerations"><i class="fa fa-check"></i><b>11.4</b> Considerations</a><ul>
<li class="chapter" data-level="11.4.1" data-path="pagerank.html"><a href="pagerank.html#connection-to-markov-chains"><i class="fa fa-check"></i><b>11.4.1</b> Connection to Markov chains</a></li>
<li class="chapter" data-level="11.4.2" data-path="pagerank.html"><a href="pagerank.html#calculating-the-dominant-eigenvalue"><i class="fa fa-check"></i><b>11.4.2</b> Calculating the dominant eigenvalue</a></li>
<li class="chapter" data-level="11.4.3" data-path="pagerank.html"><a href="pagerank.html#computational-complexity"><i class="fa fa-check"></i><b>11.4.3</b> Computational complexity</a></li>
<li class="chapter" data-level="11.4.4" data-path="pagerank.html"><a href="pagerank.html#convergence-1"><i class="fa fa-check"></i><b>11.4.4</b> Convergence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Master’s Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-models" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Generalized linear models</h1>
<p>In ordinary least squares with a single predictor, we have the relationship <span class="math inline">\(\E\left[Y_{i}\right]=\alpha+\beta x_{i}\)</span>. This model asserts that the mean response</p>
<ul>
<li>has a “baseline” (intercept) of <span class="math inline">\(\alpha\)</span></li>
<li>will change by <span class="math inline">\(\beta\)</span> (slope) for a one-unit increase in <span class="math inline">\(x_{i}\)</span></li>
</ul>
<p>Because the mean response is <em>linear</em> in the regression coefficients, we refer to OLS as a linear model. OLS assumes that the response <span class="math inline">\(Y\)</span> is continuous, e.g., height. But, analytical interest often lies in responses that are not continuous, and OLS does not model these discrete responses well. In such cases, we can extend the model by assuming other distributions for <span class="math inline">\(Y\)</span>.</p>
<p>Following <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>, a <em>generalized linear model</em> “describes a relationship between the mean of a response variable <span class="math inline">\(Y\)</span> and an independent variable <span class="math inline">\(x\)</span>.” A GLM consists of three components.</p>
<ol style="list-style-type: decimal">
<li><p>The <em>random component</em> or <em>distributional assumption</em> consists of the response variables <span class="math inline">\(Y_{1},\ldots,Y_{n}\)</span>, which are assumed to be independent random variables from the same exponential family (though they are not assumed to be identically distributed). <span class="citation">Fitzmaurice, Laird, and Ware (<a href="#ref-fitzmaurice2012applied">2012</a>)</span> describe the random component as “a probabilistic mechanism by which the responses are assumed to be generated.”</p></li>
<li><p>The <em>systematic component</em> is the linear regression model, i.e., a function of the predictor variables <span class="math inline">\(X_{i}\)</span> that is linear in the parameters <span class="math inline">\(\beta_{i}\)</span> and related to the mean of <span class="math inline">\(Y_{i}\)</span>.</p></li>
<li><p>The <em>link function</em> <span class="math inline">\(g\left(\mu\right)\)</span> links the random and systematic components by asserting that <span class="math inline">\(g\left(\mu_{i}\right)=\mathbf{X}_{i}^{\mathsf{T}}\boldsymbol{\beta}\)</span>, where <span class="math inline">\(\mu_{i}=\E\left[Y_{i}\right]\)</span> and <span class="math inline">\(\mathbf{X}_{i}^{\mathsf{T}}\boldsymbol{\beta}=\sum_{k=1}^{p}\beta_{k}X_{ik}\)</span> is the systematic component.</p></li>
</ol>

<div class="example">
<p><span id="exm:logistic-regression" class="example"><strong>Example 7.1  (Logistic regression)  </strong></span>For <span class="math inline">\(Y_{1},Y_{2},\ldots,Y_{n}\)</span>, let <span class="math inline">\(Y_{i}\sim\text{Bernoulli}\left(p\right)\)</span>, i.e.,</p>
<p><span class="math display">\[
Y_{i}=\begin{cases}
0, &amp; \text{if no event}\\
1, &amp; \text{if event}
\end{cases}.
\]</span></p>
<p>The <span class="math inline">\(Y_{i}\)</span> are the random component. Suppose that we believe that variables <span class="math inline">\(X_{1},\ldots,X_{p}\)</span> are related to the response <span class="math inline">\(Y\)</span>, so that the systematic component is <span class="math inline">\(\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\)</span>. We now consider the link function. The expected value of a Bernoulli random variable is its parameter <span class="math inline">\(p\)</span>, hence <span class="math inline">\(\mu_{i}=\E\left[Y_{i}\right]=p_{i}=P\left(Y_{i}=1\right)\)</span>.</p>
<p>If we use the <em>identity link</em> <span class="math inline">\(g\left(\mu\right)=\mu\)</span>, then our model is <span class="math inline">\(\mu=\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\)</span>. Depending on our predictors, we may obtain values for <span class="math inline">\(\mu\)</span> that lie outside <span class="math inline">\(\left[0,1\right]\)</span>. Because <span class="math inline">\(p\in\left[0,1\right]\)</span>, it is not clear how to interpret such values. Accordingly, we would like to transform <span class="math inline">\(\mu\)</span> such that it always lies in <span class="math inline">\(\left[0,1\right]\)</span>.</p>
<p>The standard logistic function is</p>
<p><span class="math display">\[
\sigma\left(t\right)=\frac{1}{1+\mathrm{e}^{-t}},\quad x\in\mathbb{R}.
\]</span></p>
<p>Observe that</p>
<p><span class="math display">\[
\lim_{t\rightarrow\infty} \sigma\left(t\right)=\frac{1}{\lim_{t\rightarrow\infty}\left(1+\mathrm{e}^{-t}\right)}
  = \frac{1}{1+\lim_{t\rightarrow\infty}\mathrm{e}^{-t}}=\frac{1}{1+0}=1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\lim_{t\rightarrow -\infty}\sigma\left(t\right)=\frac{1}{1+\lim_{t\rightarrow -\infty}\mathrm{e}^{-t}}=\frac{1}{1+\infty}=0.
\]</span></p>
<p>For any <span class="math inline">\(t\in\left(-\infty,\infty\right)\)</span>, we have <span class="math inline">\(\mathrm{e}^{-t}&gt;0\)</span>, so that <span class="math inline">\(1&lt;1+\mathrm{e}^{-t}\)</span>, hence <span class="math inline">\(\sigma\left(t\right)\)</span> is bounded below by 0 and above by 1. The standard logistic function would thus seem to be a good candidate for our link. If we let <span class="math inline">\(t=\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\)</span>, then <span class="math inline">\(\sigma\)</span> will map the model to <span class="math inline">\(\left[0,1\right]\)</span>, i.e.,</p>
<p><span class="math display">\[
p\left(\mathbf{X}\right)=\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}},
\]</span></p>
<p>where we have used the notation <span class="math inline">\(p\)</span> to reflect that we are modeling the probability of a success (the event of interest occurs). We have mapped the model to an interval appropriate for the mean response, but have some work left to do to put it into the correct form for a GLM. The inverse of the logistic function is the <em>logit</em> function, given by</p>
<p><span class="math display">\[
\text{logit}\left(t\right)=\log\frac{t}{1-t}.
\]</span></p>
<p>Observe that</p>
<p><span class="math display">\[
\begin{align*}
  \text{logit}\left(p\left(\mathbf{X}\right)\right) &amp; = \log\frac{p\left(\mathbf{X}\right)}{1-p\left(\mathbf{X}\right)} \\
  &amp; = \log p\left(\mathbf{X}\right)-\log\left(1-p\left(\mathbf{X}\right)\right) \\
  &amp; = \log\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}-\log\left(1-\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}\right) \\
  &amp; = \log 1-\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\log\left(\frac{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}-\frac{1}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}\right) \\
  &amp; = -\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\log\left(\frac{\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}{1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}}\right) \\
  &amp; = -\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\left[\log\left(\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)-\log\left(1+\exp\left\{-\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}\right\}\right)\right] \\
  &amp; = \mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}.
\end{align*}
\]</span></p>
<p>Thus, applying the logit function to the transformed mean response <span class="math inline">\(p\left(\mathbf{X}\right)\)</span> results in an appropriate form for the systematic component of the model. Accordingly, we will take the logit as the link function, i.e., <span class="math inline">\(g\left(\mu\right)=\text{logit}\left(\mu\right)\)</span>, so that the logistic regression model is</p>
<p><span class="math display">\[
\text{logit}\left(\mu\right) = \text{logit}\left(p\right) = \log\left(\frac{p}{1-p}\right)=\mathbf{X}^{\mathsf{T}}\boldsymbol{\beta}.
\]</span></p>
<p>We refer to <span class="math inline">\(p/\left(1-p\right)\)</span> as the <em>odds</em> of the event (how likely versus not). We see that logistic regression is linear in the <em>log-odds</em> of <span class="math inline">\(Y\)</span>. When we exponentiate both sides of the above equation, we can interpret the coefficient <span class="math inline">\(\beta_{i}\)</span> as the multiplicative change in the odds of success associated with a one-unit change in <span class="math inline">\(X_{i}\)</span>.</p>
Finally, recall from <a href="common-families-of-distributions.html#exm:natural-param-binomial">5.6</a> that <span class="math inline">\(\log\left(p/\left(1-p\right)\right)\)</span> is the natural parameter of the binomial exponential family. When the natural parameter is used as the link function in a GLM, it is called the <em>canonical link</em>.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-67" class="example"><strong>Example 7.2  (Poisson regression)  </strong></span>FINISH THIS EXAMPLE</p>
In a Poisson regression, we have <span class="math inline">\(\log\left(\lambda\right)=\beta_{0}+\beta_{1}X_{1}+\ldots+\beta_{k}X_{k}\)</span>.
</div>


</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-casella2002statistical">
<p>Casella, G., and R.L. Berger. 2002. <em>Statistical Inference</em>. Duxbury Advanced Series in Statistics and Decision Sciences. Thomson Learning. <a href="https://books.google.com/books?id=0x\_vAAAAMAAJ" class="uri">https://books.google.com/books?id=0x\_vAAAAMAAJ</a>.</p>
</div>
<div id="ref-fitzmaurice2012applied">
<p>Fitzmaurice, Garrett M, Nan M Laird, and James H Ware. 2012. <em>Applied Longitudinal Analysis</em>. Vol. 998. John Wiley &amp; Sons.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="point-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="machine-representation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["course-notes.pdf", "course-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
